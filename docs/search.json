[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "My name is Ann Zi’an Zhang, currently a student at University of Pennsylvania studying city planning and urban spatial analytics. I am interested in data-driven problem-solving, especially in urban context.\nPrior to Penn, I graduated from Wesleyan University in Connecticut, U.S., majoring in Art History, Psychology, and Science in Society Program."
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium."
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "analysis/Ass-2.html",
    "href": "analysis/Ass-2.html",
    "title": "Where to spend leisure time in NYC?",
    "section": "",
    "text": "---\nformat:\n    html:\n        toc: false\n        page-layout: full\nexecute:\n        echo: false\n\n---"
  },
  {
    "objectID": "analysis/Ass-2.html#imports",
    "href": "analysis/Ass-2.html#imports",
    "title": "Where to spend leisure time in NYC?",
    "section": "Imports",
    "text": "Imports\n\nimport altair as alt\nimport geopandas as gpd\nimport hvplot.pandas\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport datetime\nimport math\n\n%matplotlib inline"
  },
  {
    "objectID": "analysis/Ass-2.html#nyc-base-maps",
    "href": "analysis/Ass-2.html#nyc-base-maps",
    "title": "Where to spend leisure time in NYC?",
    "section": "NYC Base Maps",
    "text": "NYC Base Maps\n\nTracts, Neighborhood, and Borough\n\nnyc_tracts = pd.read_csv(\"data/2022 Census Tracts.csv\")\nnyc_tracts['geometry'] = gpd.GeoSeries.from_wkt(nyc_tracts['the_geom'])\ngeo_tracts = gpd.GeoDataFrame(nyc_tracts, geometry='geometry')\ngeo_tracts = geo_tracts.set_crs(epsg=4326)\ntracts = geo_tracts[['BoroName', 'CT2020', 'BoroCT2020', 'NTAName', 'Shape_Area','geometry','GEOID']].copy()\ntracts_clean = tracts[['BoroName', 'CT2020', 'NTAName', 'geometry','GEOID']]\nBoro_NTA = tracts_clean[['BoroName', 'NTAName']].drop_duplicates(subset = \"NTAName\")"
  },
  {
    "objectID": "analysis/Ass-2.html#dataset-selection-setup",
    "href": "analysis/Ass-2.html#dataset-selection-setup",
    "title": "Where to spend leisure time in NYC?",
    "section": "Dataset Selection & Setup",
    "text": "Dataset Selection & Setup\nAfter glancing through NYC Open Data portal, I have selected art galleries, museums, libraries, theatres, and parks as common leisure spaces. In addition, I explored data of open streets, a program thatt had been implemented in many global cities including New York City to transform roads into public spaces for cultural and all kinds of events on particular days (mostly on weekends).\nThis step is to bring in all datasets, clean them up, and aggregate different types of leisure spaces. The aggregated dataframe is then spacially joined with tracts, neighborhood, and boroughs, ready for further investigation on their geospatial distributions.\nSimilar data wrangling is performed on parks. Since parks come in polygon instead of points, which may result in problems with spatial joins, in the case when one park falls in two or more tracts or neighborhood. Hence, the geometry of parks’ centroids is adopted to replace the polygon geometry for further analysis.\nThe Open Street data comes with more detailed information on the approved time for each street. The days of week (e.g. Monday) and time of the day (e.g. 7:30) that it opens and closes are rearranged by melting and pivoting.\nAdditionally, 2020 Census Data on population and median household income are brought in for analysis.\n\nIndoor Leisure Spaces: Art Galleries, Museums, Libraries, and Theatres\n\n# establish geodataframe\nart_galleries = gpd.read_file(\"data/art galleries.geojson\")\ngeo_art_galleries = gpd.GeoDataFrame(art_galleries, geometry='geometry')\ngeo_art_galleries = art_galleries.set_crs(epsg=4326)\n\n# add type\nart_galleries_clean = geo_art_galleries[['name','zip','address1','geometry']]\nart_galleries_clean.loc[:,\"Type\"]= \"Art Galleries\"\nart_galleries_clean.rename(\n    columns={\"address1\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1429971093.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  art_galleries_clean.rename(\n\n\n\n# Museums \n\n# establish geodataframe\nmuseums = gpd.read_file(\"data/museums.geojson\")\ngeo_museums= gpd.GeoDataFrame(museums, geometry='geometry')\ngeo_museums = geo_museums.set_crs(epsg=4326)\ngeo_museums\n\n# add type\nmuseums_clean = geo_museums[['name','zip','adress1','geometry']]\nmuseums_clean.loc[:,\"Type\"]= \"Museums\"\nmuseums_clean.rename(\n    columns={\"adress1\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\nmuseums_clean\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/3071166290.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  museums_clean.rename(\n\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\n\n\n\n\n0\nAlexander Hamilton U.S. Custom House\n10004.0\n1 Bowling Grn\nPOINT (-74.01376 40.70382)\nMuseums\n\n\n1\nAlice Austen House Museum\n10305.0\n2 Hylan Blvd\nPOINT (-74.06303 40.61512)\nMuseums\n\n\n2\nAmerican Academy of Arts and Letters\n10032.0\n633 W. 155th St.\nPOINT (-73.94730 40.83385)\nMuseums\n\n\n3\nAmerican Folk Art Museum\n10019.0\n45 West 53rd Street\nPOINT (-73.97810 40.76162)\nMuseums\n\n\n4\nAmerican Immigration History Center\n0.0\nEllis Island\nPOINT (-74.03968 40.69906)\nMuseums\n\n\n...\n...\n...\n...\n...\n...\n\n\n125\nAmerican Sephardi Federation / Sephardic House\n10011.0\n15 W. 16th St.\nPOINT (-73.99389 40.73808)\nMuseums\n\n\n126\nYIVO Institute for Jewish Research\n10011.0\n15 W. 16th St.\nPOINT (-73.99379 40.73796)\nMuseums\n\n\n127\nAmerican Jewish Historical Society\n10011.0\n15 W. 16th St.\nPOINT (-73.99393 40.73802)\nMuseums\n\n\n128\nYeshiva University Museum\n10011.0\n15 W. 16th St.\nPOINT (-73.99382 40.73805)\nMuseums\n\n\n129\nCenter For Jewish History\n10011.0\n15 W. 16th St.\nPOINT (-73.99387 40.73799)\nMuseums\n\n\n\n\n130 rows × 5 columns\n\n\n\n\n# Libraries\n\n# establish geodataframe\nlibrary = gpd.read_file(\"data/libraries.geojson\")\ngeo_library = gpd.GeoDataFrame(library, geometry='geometry')\ngeo_library= geo_library.set_crs(epsg=4326)\ngeo_library\n\n# add type\nlibrary_clean = geo_library[['name','zip','streetname','geometry']]\nlibrary_clean.loc[:,\"Type\"]= \"Libraries\"\nlibrary_clean.rename(\n    columns={\"streetname\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\nlibrary_clean\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/3524914390.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  library_clean.rename(\n\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\n\n\n\n\n0\n115th Street\n10026\nWest 115th Street\nPOINT (-73.95353 40.80298)\nLibraries\n\n\n1\n125th Street\n10035\nEast 125th Street\nPOINT (-73.93485 40.80302)\nLibraries\n\n\n2\n53rd Street\n10019\nWest 53rd Street\nPOINT (-73.97736 40.76081)\nLibraries\n\n\n3\n58th Street\n10022\nEast 58th Street\nPOINT (-73.96938 40.76219)\nLibraries\n\n\n4\n67th Street\n10065\nEast 67th Street\nPOINT (-73.95955 40.76492)\nLibraries\n\n\n...\n...\n...\n...\n...\n...\n\n\n211\nSunnyside\n11104\nGreenpoint Avenue\nPOINT (-73.92167 40.74085)\nLibraries\n\n\n212\nWhitestone\n11357\n14 Road\nPOINT (-73.81070 40.78854)\nLibraries\n\n\n213\nWindsor Park\n11364\nBell Boulevard\nPOINT (-73.75562 40.73450)\nLibraries\n\n\n214\nWoodhaven\n11421\nForest Parkway\nPOINT (-73.86146 40.69453)\nLibraries\n\n\n215\nWoodside\n11377\nSkillman Avenue\nPOINT (-73.90979 40.74534)\nLibraries\n\n\n\n\n216 rows × 5 columns\n\n\n\n\n# Theaters\n# establish geodataframe\ntheaters = gpd.read_file(\"data/Theaters.geojson\")\ngeo_theaters = gpd.GeoDataFrame(theaters, geometry='geometry')\ngeo_theaters= geo_theaters.set_crs(epsg=4326)\ngeo_theaters\n\n\n# add type\ntheaters_clean = geo_theaters[['name','zip','address1','geometry']]\ntheaters_clean.loc[:,\"Type\"]= \"Theatres\"\ntheaters_clean.rename(\n    columns={\"address1\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\ntheaters_clean\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1518346627.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  theaters_clean.rename(\n\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\n\n\n\n\n0\n45th Street Theater\n10036.0\n354 West 45th Street\nPOINT (-73.99062 40.75985)\nTheatres\n\n\n1\n47th Street Theater\n10036.0\n304 West 47th Street\nPOINT (-73.98811 40.76047)\nTheatres\n\n\n2\n59E59\n10022.0\n59 East 59th Street\nPOINT (-73.97038 40.76340)\nTheatres\n\n\n3\nAcorn Theater\n10036.0\n410 West 42nd Street\nPOINT (-73.99332 40.75854)\nTheatres\n\n\n4\nAl Hirschfeld Theater\n10036.0\n302 W 45th Street\nPOINT (-73.98921 40.75926)\nTheatres\n\n\n...\n...\n...\n...\n...\n...\n\n\n112\nWestside Theater\n10036.0\n407 W 43rd St\nPOINT (-73.99255 40.75953)\nTheatres\n\n\n113\nWings Theatre\n10014.0\n154 Christopher St\nPOINT (-74.00889 40.73240)\nTheatres\n\n\n114\nWinter Garden Theatre\n10019.0\n1634 Broadway\nPOINT (-73.98348 40.76152)\nTheatres\n\n\n115\nYork Theatre\n10022.0\n619 Lexington Ave\nPOINT (-73.96998 40.75836)\nTheatres\n\n\n116\nDelacorte Theater\n0.0\nCentral Park - Mid-Park at 80th Street\nPOINT (-73.96882 40.78018)\nTheatres\n\n\n\n\n117 rows × 5 columns\n\n\n\n\nAggregate all types of indoor leisure space\n\nframes = [art_galleries_clean, museums_clean, library_clean, theaters_clean]\n\ntotal_indoor = pd.concat(frames)\n\ntotal_indoor\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\n\n\n\n\n0\nO'reilly William & Co Ltd\n10021.0\n52 E 76th St\nPOINT (-73.96273 40.77380)\nArt Galleries\n\n\n1\nOrganization of Independent Artists - Gallery 402\n10013.0\n19 Hudson St.\nPOINT (-74.00939 40.71647)\nArt Galleries\n\n\n2\nOwen Gallery\n10021.0\n19 E 75th St\nPOINT (-73.96435 40.77400)\nArt Galleries\n\n\n3\nP P O W Gallerie\n10001.0\n511 W 25th St\nPOINT (-74.00389 40.74959)\nArt Galleries\n\n\n4\nP P O W Inc\n10013.0\n476 Broome St\nPOINT (-74.00176 40.72291)\nArt Galleries\n\n\n...\n...\n...\n...\n...\n...\n\n\n112\nWestside Theater\n10036.0\n407 W 43rd St\nPOINT (-73.99255 40.75953)\nTheatres\n\n\n113\nWings Theatre\n10014.0\n154 Christopher St\nPOINT (-74.00889 40.73240)\nTheatres\n\n\n114\nWinter Garden Theatre\n10019.0\n1634 Broadway\nPOINT (-73.98348 40.76152)\nTheatres\n\n\n115\nYork Theatre\n10022.0\n619 Lexington Ave\nPOINT (-73.96998 40.75836)\nTheatres\n\n\n116\nDelacorte Theater\n0.0\nCentral Park - Mid-Park at 80th Street\nPOINT (-73.96882 40.78018)\nTheatres\n\n\n\n\n1380 rows × 5 columns\n\n\n\n\n\nSpatial Join\n\ngeo_total_indoor = gpd.sjoin(\n    total_indoor,  # The point data for 311 tickets\n    tracts_clean.to_crs(total_indoor.crs),  # The neighborhoods (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\ngeo_total_indoor.head()\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\nindex_right\nBoroName\nCT2020\nNTAName\nGEOID\n\n\n\n\n0\nO'reilly William & Co Ltd\n10021.0\n52 E 76th St\nPOINT (-73.96273 40.77380)\nArt Galleries\n84.0\nManhattan\n13000.0\nUpper East Side-Carnegie Hill\n3.606101e+10\n\n\n1\nOrganization of Independent Artists - Gallery 402\n10013.0\n19 Hudson St.\nPOINT (-74.00939 40.71647)\nArt Galleries\n17.0\nManhattan\n3900.0\nTribeca-Civic Center\n3.606100e+10\n\n\n2\nOwen Gallery\n10021.0\n19 E 75th St\nPOINT (-73.96435 40.77400)\nArt Galleries\n84.0\nManhattan\n13000.0\nUpper East Side-Carnegie Hill\n3.606101e+10\n\n\n3\nP P O W Gallerie\n10001.0\n511 W 25th St\nPOINT (-74.00389 40.74959)\nArt Galleries\n1134.0\nManhattan\n9901.0\nChelsea-Hudson Yards\n3.606101e+10\n\n\n4\nP P O W Inc\n10013.0\n476 Broome St\nPOINT (-74.00176 40.72291)\nArt Galleries\n1156.0\nManhattan\n4900.0\nSoHo-Little Italy-Hudson Square\n3.606100e+10\n\n\n\n\n\n\n\n\n\n\nOutdoor Leisure Space: Parks\n\n# Parks\n# establish geodataframe\nparks = gpd.read_file(\"data/Parks Properties.geojson\")\ngeo_parks = gpd.GeoDataFrame(parks, geometry='geometry')\ngeo_parks= geo_parks.set_crs(epsg=4326)\ngeo_parks\n\n\n# add type\nparks_clean = geo_parks[['signname','geometry','acres']]\nparks_clean.loc[:,\"Type\"]= \"Parks\"\nparks_clean.rename(\n    columns={\"location\": \"Address\", \"signname\": \"Name\", \"zipcode\": \"Zip\"},\n    inplace=True,)\n\nparks_clean['new_geom'] = parks_clean['geometry']\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1177565529.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  parks_clean.rename(\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n\n\n\n\n# Since many parks are in multi-polygons, which will lead to errors when doing spatial join later, a centroid of each park is generated here for smoother spatial join.\nparks_clean['geometry'] = parks_clean['geometry'].centroid\n\ngeo_parks_clean = gpd.sjoin(\n    parks_clean,  # The point data for 311 tickets\n    tracts_clean.to_crs(parks_clean.crs),  # The neighborhoods (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\ngeo_parks_clean\n\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/3082874007.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  parks_clean['geometry'] = parks_clean['geometry'].centroid\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n\n\n\n\n\n\n\n\n\nName\ngeometry\nacres\nType\nnew_geom\nindex_right\nBoroName\nCT2020\nNTAName\nGEOID\n\n\n\n\n0\nInwood Hill Park\nPOINT (-73.92544 40.87257)\n196.398\nParks\nMULTIPOLYGON (((-73.92093 40.86999, -73.92145 ...\n2191.0\nManhattan\n29700.0\nInwood Hill Park\n3.606103e+10\n\n\n1\nChallenge Playground\nPOINT (-73.72796 40.75662)\n2.035\nParks\nMULTIPOLYGON (((-73.72738 40.75605, -73.72783 ...\n936.0\nQueens\n152902.0\nDouglaston-Little Neck\n3.608115e+10\n\n\n2\nSunset Cove Park\nPOINT (-73.82300 40.59853)\n9.375\nParks\nMULTIPOLYGON (((-73.82218 40.59892, -73.82221 ...\n1129.0\nQueens\n107201.0\nBreezy Point-Belle Harbor-Rockaway Park-Broad ...\n3.608111e+10\n\n\n3\nGrand Central Parkway Extension\nPOINT (-73.85317 40.75316)\n249.389\nParks\nMULTIPOLYGON (((-73.85875 40.76741, -73.85976 ...\n625.0\nQueens\n39902.0\nNorth Corona\n3.608104e+10\n\n\n4\nIdlewild Park\nPOINT (-73.75229 40.65043)\n180.85\nParks\nMULTIPOLYGON (((-73.75809 40.65427, -73.75845 ...\n1013.0\nQueens\n66404.0\nSpringfield Gardens (South)-Brookville\n3.608107e+10\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2040\nMaria Hernandez Park\nPOINT (-73.92386 40.70317)\n6.873\nParks\nMULTIPOLYGON (((-73.92251 40.70351, -73.92381 ...\n1729.0\nBrooklyn\n42900.0\nBushwick (West)\n3.604704e+10\n\n\n2041\nCrotona Parkway Malls\nPOINT (-73.88477 40.84405)\n8.75\nParks\nMULTIPOLYGON (((-73.88496 40.84470, -73.88496 ...\n336.0\nBronx\n36300.0\nWest Farms\n3.600504e+10\n\n\n2042\nPark\nPOINT (-73.89807 40.84408)\n0.511\nParks\nMULTIPOLYGON (((-73.89759 40.84410, -73.89773 ...\n1229.0\nBronx\n16500.0\nClaremont Village-Claremont (East)\n3.600502e+10\n\n\n2043\nCunningham Park\nPOINT (-73.76880 40.73382)\n358.0\nParks\nMULTIPOLYGON (((-73.77466 40.72442, -73.77439 ...\n2241.0\nQueens\n128300.0\nCunningham Park\n3.608113e+10\n\n\n2044\nRoberto Clemente Ballfield\nPOINT (-73.96767 40.70635)\n1.93\nParks\nMULTIPOLYGON (((-73.96761 40.70581, -73.96735 ...\n2220.0\nBrooklyn\n54500.0\nSouth Williamsburg\n3.604705e+10\n\n\n\n\n2045 rows × 10 columns\n\n\n\n\n\nOutdoor Leisure Space: Open Streets\n\nopen_streets = gpd.read_file(\"data/Open Streets Locations.geojson\")\ngeo_open_streets = gpd.GeoDataFrame(open_streets, geometry='geometry', crs=2263)\ngeo_open_streets = open_streets.to_crs(epsg=4326)\n\nopen_streets_clean = geo_open_streets[['appronstre','apprtostre','apprdayswe','boroughname','reviewstat','shape_stle','geometry']]\n\n\njoin_test = gpd.sjoin(\n    open_streets_clean,  # The point data for 311 tickets\n    tracts_clean.to_crs(open_streets_clean.crs),  # The neighborhoods (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\njoin_test.head()\n\n\n\n\n\n\n\n\nappronstre\napprtostre\napprdayswe\nboroughname\nreviewstat\nshape_stle\ngeometry\nindex_right\nBoroName\nCT2020\nNTAName\nGEOID\n\n\n\n\n0\nDEISIUS STREET\nSTECHER STREET\nmon,tue,wed,thu,fri\nStaten Island\napprovedFullSchools\n264.932398036\nMULTILINESTRING ((-74.18738 40.53028, -74.1882...\n1318.0\nStaten Island\n17600.0\nAnnadale-Huguenot-Prince's Bay-Woodrow\n3.608502e+10\n\n\n1\nSUFFOLK AVENUE\nHAROLD STREET\nfri,sat,sun\nStaten Island\napprovedFull\n313.087821487\nMULTILINESTRING ((-74.12784 40.60288, -74.1277...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n\n\n2\nSUFFOLK AVENUE\nHAROLD STREET\nfri,sat,sun\nStaten Island\napprovedFull\n142.063500219\nMULTILINESTRING ((-74.12772 40.60202, -74.1276...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n\n\n3\nVERMONT COURT\nSUFFOLK AVENUE\nfri,sat,sun\nStaten Island\napprovedFull\n421.392020366\nMULTILINESTRING ((-74.12620 40.60209, -74.1277...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n\n\n4\n9 STREET\nROSE AVENUE\nfri,sat,sun\nStaten Island\napprovedFull\n448.103939286\nMULTILINESTRING ((-74.11481 40.57316, -74.1157...\n1300.0\nStaten Island\n13400.0\nNew Dorp-Midland Beach\n3.608501e+10\n\n\n\n\n\n\n\n\njoin_test['monday'] = join_test['apprdayswe'].str.count('mon')\njoin_test['tuesday'] = join_test['apprdayswe'].str.count('tue')\njoin_test['wednesday'] = join_test['apprdayswe'].str.count('wed')\njoin_test['thursday'] = join_test['apprdayswe'].str.count('thu')\njoin_test['friday'] = join_test['apprdayswe'].str.count('fri')\njoin_test['saturday'] = join_test['apprdayswe'].str.count('sat')\njoin_test['sunday'] = join_test['apprdayswe'].str.count('sun')\njoin_test['dayscount'] = join_test[['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']].sum(axis=1)\n\njoin_test\n\n\n\n\n\n\n\n\nappronstre\napprtostre\napprdayswe\nboroughname\nreviewstat\nshape_stle\ngeometry\nindex_right\nBoroName\nCT2020\nNTAName\nGEOID\nmonday\ntuesday\nwednesday\nthursday\nfriday\nsaturday\nsunday\ndayscount\n\n\n\n\n0\nDEISIUS STREET\nSTECHER STREET\nmon,tue,wed,thu,fri\nStaten Island\napprovedFullSchools\n264.932398036\nMULTILINESTRING ((-74.18738 40.53028, -74.1882...\n1318.0\nStaten Island\n17600.0\nAnnadale-Huguenot-Prince's Bay-Woodrow\n3.608502e+10\n1\n1\n1\n1\n1\n0\n0\n5\n\n\n1\nSUFFOLK AVENUE\nHAROLD STREET\nfri,sat,sun\nStaten Island\napprovedFull\n313.087821487\nMULTILINESTRING ((-74.12784 40.60288, -74.1277...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n0\n0\n0\n0\n1\n1\n1\n3\n\n\n2\nSUFFOLK AVENUE\nHAROLD STREET\nfri,sat,sun\nStaten Island\napprovedFull\n142.063500219\nMULTILINESTRING ((-74.12772 40.60202, -74.1276...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n0\n0\n0\n0\n1\n1\n1\n3\n\n\n3\nVERMONT COURT\nSUFFOLK AVENUE\nfri,sat,sun\nStaten Island\napprovedFull\n421.392020366\nMULTILINESTRING ((-74.12620 40.60209, -74.1277...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n0\n0\n0\n0\n1\n1\n1\n3\n\n\n4\n9 STREET\nROSE AVENUE\nfri,sat,sun\nStaten Island\napprovedFull\n448.103939286\nMULTILINESTRING ((-74.11481 40.57316, -74.1157...\n1300.0\nStaten Island\n13400.0\nNew Dorp-Midland Beach\n3.608501e+10\n0\n0\n0\n0\n1\n1\n1\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\nBECK STREET\nAVENUE ST JOHN\nwed\nBronx\napprovedFull\n619.314479576\nMULTILINESTRING ((-73.90222 40.81449, -73.9014...\n200.0\nBronx\n8300.0\nLongwood\n3.600501e+10\n0\n0\n1\n0\n0\n0\n0\n1\n\n\n362\nNEWKIRK AVENUE\nEAST 17 STREET\nsun\nBrooklyn\napprovedFull\n284.786787549\nMULTILINESTRING ((-73.96422 40.63510, -73.9641...\n1807.0\nBrooklyn\n52000.0\nFlatbush (West)-Ditmas Park-Parkville\n3.604705e+10\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n363\n18 STREET\n4 AVENUE\nmon,tue,wed,thu,fri\nBrooklyn\napprovedFullSchools\n768.660222317\nMULTILINESTRING ((-73.99201 40.66323, -73.9916...\n2248.0\nBrooklyn\n14300.0\nSunset Park (West)\n3.604701e+10\n1\n1\n1\n1\n1\n0\n0\n5\n\n\n364\n34 AVENUE\nJUNCTION BOULEVARD\nmon,tue,wed,thu,fri,sat,sun\nQueens\napprovedLimited\n272.995599601\nMULTILINESTRING ((-73.89716 40.75246, -73.8970...\n2168.0\nQueens\n29100.0\nJackson Heights\n3.608103e+10\n1\n1\n1\n1\n1\n1\n1\n7\n\n\n365\nDECATUR STREET\nSARATOGA AVENUE\nsat\nBrooklyn\napprovedLimited\n769.964464111\nMULTILINESTRING ((-73.92001 40.68299, -73.9176...\n1682.0\nBrooklyn\n37700.0\nBedford-Stuyvesant (East)\n3.604704e+10\n0\n0\n0\n0\n0\n1\n0\n1\n\n\n\n\n366 rows × 20 columns\n\n\n\n\nopen_streets_days = join_test[['appronstre', 'BoroName', 'monday', 'tuesday', 'wednesday','thursday', 'friday', 'saturday', 'sunday']]\nopen_streets_days\n\n\n\n\n\n\n\n\nappronstre\nBoroName\nmonday\ntuesday\nwednesday\nthursday\nfriday\nsaturday\nsunday\n\n\n\n\n0\nDEISIUS STREET\nStaten Island\n1\n1\n1\n1\n1\n0\n0\n\n\n1\nSUFFOLK AVENUE\nStaten Island\n0\n0\n0\n0\n1\n1\n1\n\n\n2\nSUFFOLK AVENUE\nStaten Island\n0\n0\n0\n0\n1\n1\n1\n\n\n3\nVERMONT COURT\nStaten Island\n0\n0\n0\n0\n1\n1\n1\n\n\n4\n9 STREET\nStaten Island\n0\n0\n0\n0\n1\n1\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\nBECK STREET\nBronx\n0\n0\n1\n0\n0\n0\n0\n\n\n362\nNEWKIRK AVENUE\nBrooklyn\n0\n0\n0\n0\n0\n0\n1\n\n\n363\n18 STREET\nBrooklyn\n1\n1\n1\n1\n1\n0\n0\n\n\n364\n34 AVENUE\nQueens\n1\n1\n1\n1\n1\n1\n1\n\n\n365\nDECATUR STREET\nBrooklyn\n0\n0\n0\n0\n0\n1\n0\n\n\n\n\n366 rows × 9 columns\n\n\n\n\n\nCensus Data\n\nPop_2020 = gpd.read_file(\"data/NYC_tracts_2020.geojson\")\nPop_2020 = Pop_2020[['GEOID', 'estimate']]\nPop_2020['GEOID']=Pop_2020['GEOID'].astype(int)\n\ntracts_pop = tracts_clean.merge(Pop_2020, on='GEOID', how='left') \nneighbor_pop = tracts_pop.groupby(['NTAName']).sum(['estimate']).reset_index()\n\n\nincome = gpd.read_file(\"data/NYC_Income.geojson\")\ngeo_income = gpd.GeoDataFrame(income, geometry='geometry')\ngeo_income = geo_income.set_crs(epsg=4326)\ngeo_income['GEOID']=geo_income['GEOID'].astype('int')\n\ntracts_income = tracts_clean.merge(geo_income.drop(columns='geometry'), on='GEOID', how='left').dropna()\nneighbor_income = tracts_income.groupby(['NTAName', 'BoroName']).median(['estimate']).reset_index()"
  },
  {
    "objectID": "analysis/Ass-2.html#chart-i-matplotlib-parks-in-neighborhoods",
    "href": "analysis/Ass-2.html#chart-i-matplotlib-parks-in-neighborhoods",
    "title": "Where to spend leisure time in NYC?",
    "section": "Chart I: Matplotlib – Parks in Neighborhoods",
    "text": "Chart I: Matplotlib – Parks in Neighborhoods\nI first hope to investigate into the distribution of Parks in different neighborhoods and boroughs in relation to population. On the one hand, higher population means more people will have need for a bigger public green space for leisure time. On the other hand, less populated neighborhoods tend to have more spaces for parks. And since the count of parks doesn’t perfectly reflect how much space is available, I am using acrage data instead of counts for this analysis.\nTo investigate, I utilized Matplotlib, which is great for making simple scatterplot charts that speaks for simple linear relationship, if there is any.\n\ngeo_parks_clean['acres'] = geo_parks_clean['acres'].astype(float)\nparks_acres_neighborhood = geo_parks_clean.groupby('NTAName').sum().drop(columns=['index_right','CT2020']).reset_index()\nparks_acres_pop = neighbor_pop.merge(parks_acres_neighborhood, on='NTAName', how='left').dropna()\nparks_pop = parks_acres_pop.merge(Boro_NTA, on='NTAName', how='left').dropna()\nparks_pop['acres'].describe()\n\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1268464147.py:2: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  parks_acres_neighborhood = geo_parks_clean.groupby('NTAName').sum().drop(columns=['index_right','CT2020']).reset_index()\n\n\ncount     237.000000\nmean      105.945812\nstd       224.643587\nmin         0.005000\n25%         7.651000\n50%        20.653000\n75%        85.527000\nmax      1930.636136\nName: acres, dtype: float64\n\n\nAfter a quick glance of the park data, I notice some extreme outliers with extremely large parks that not only serves adjacent neighborhoods but the whole city. I excluded those outliers to have a better sense of how much acrage of parks common neighborhoods get.\n\nparks_pop_filtered = parks_pop.loc[(parks_pop['acres'] &lt; 86) & (parks_pop['acres'] &gt; 7) & (parks_pop['estimate'] &gt;0)]\nparks_pop_filtered\n\n\n\n\n\n\n\n\nNTAName\nCT2020\nGEOID_x\nestimate\nacres\nGEOID_y\nBoroName\n\n\n\n\n5\nAstoria (East)-Woodside (North)\n241600\n505134241600\n34825.0\n8.642\n2.886482e+11\nQueens\n\n\n6\nAstoria (North)-Ditmars-Steinway\n214102\n613377214102\n47134.0\n11.530\n4.690532e+11\nQueens\n\n\n10\nBarren Island-Floyd Bennett Field\n70202\n36047070202\n26.0\n64.665\n3.604707e+10\nBrooklyn\n\n\n11\nBath Beach\n235800\n396517235800\n32716.0\n21.398\n1.081411e+11\nBrooklyn\n\n\n15\nBedford Park\n448115\n396055448115\n55702.0\n23.917\n1.800252e+11\nBronx\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n225\nWest Farms\n136300\n180025136300\n18206.0\n13.540\n3.960554e+11\nBronx\n\n\n230\nWhitestone-Beechhurst\n703300\n252567703300\n28353.0\n29.086\n2.164866e+11\nQueens\n\n\n232\nWilliamsburg\n695100\n468611695100\n59410.0\n81.282\n1.045365e+12\nBrooklyn\n\n\n233\nWindsor Terrace-South Slope\n363808\n288376363808\n25442.0\n8.408\n4.325650e+11\nBrooklyn\n\n\n235\nWoodside\n406004\n505134406004\n45417.0\n10.217\n5.412154e+11\nQueens\n\n\n\n\n113 rows × 7 columns\n\n\n\n\n\ncolor_map = {\"Bronx\": \"#550527\", \"Brooklyn\": \"#688E26\", \"Manhattan\": \"#FAA613\", \"Queens\": \"#F44708\", \"Staten Island\": \"#A10702\"}\n\nfig, ax = plt.subplots(figsize=(11,6))\n\nfor BoroName, group_df in parks_pop_filtered.groupby(\"BoroName\"):\n    \n    ax.scatter(\n        group_df[\"estimate\"],\n        group_df[\"acres\"],\n        marker=\"P\",\n        label=BoroName,\n        color=color_map[BoroName],\n        alpha=0.75,\n        zorder=10\n    )\n\nax.legend(loc=\"best\")\nax.set(\n    title = \"Park Space (acres) Relative to Population in Neighborhoods, by Boroughs\",\n    xlabel = \"Population in each neighborhood (2020 Census)\",\n    ylabel = \"Parks in Acres\")\nax.grid(True)\n\nplt.show()\n\n\n\n\nThis chart does not suggest a strong linear relationship between population and park acrages. Overall, Manhattan, Queens and Brooklyn host more large parks, but some neighborhoods are particularly underserved, with very high population and low park acrage (points towards lower right of the chart)."
  },
  {
    "objectID": "analysis/Ass-2.html#chart-ii-seaborn-x2",
    "href": "analysis/Ass-2.html#chart-ii-seaborn-x2",
    "title": "Where to spend leisure time in NYC?",
    "section": "Chart II: Seaborn (x2)",
    "text": "Chart II: Seaborn (x2)\nLooking at data for indoor leisure space and open street data, I utilize seaborn to create two types of charts that suite the nature of the data sets.\nThe first is a grouped bar charts. Similar to park distribution, I hope to have a quick glimpse of number of each type of leisure spaces in each borough, and hope to identify any general spatial patterns or inequalities. A grouped chart is great a revealing such pattern.\nThe second is a heatmap for open street data. The heatmap explores the number of open streets approved in different boroughs on different days of the week.\n\nIndoor Leisure Spaces: Grouped Bar Charts\n\nindoor_clean = geo_total_indoor.groupby(['Type','BoroName']).count().reset_index().drop(columns=['Zip','Address','geometry','index_right','CT2020','NTAName']).pivot(index='Type',columns='BoroName', values='Name').fillna(0).reset_index()\nindoor_melt = indoor_clean.melt(id_vars='Type', value_vars=['Bronx','Brooklyn','Manhattan', 'Queens', 'Staten Island'])\nindoor_melt\n\n\n\n\n\n\n\n\nType\nBoroName\nvalue\n\n\n\n\n0\nArt Galleries\nBronx\n6.0\n\n\n1\nLibraries\nBronx\n35.0\n\n\n2\nMuseums\nBronx\n8.0\n\n\n3\nTheatres\nBronx\n0.0\n\n\n4\nArt Galleries\nBrooklyn\n61.0\n\n\n5\nLibraries\nBrooklyn\n59.0\n\n\n6\nMuseums\nBrooklyn\n12.0\n\n\n7\nTheatres\nBrooklyn\n0.0\n\n\n8\nArt Galleries\nManhattan\n823.0\n\n\n9\nLibraries\nManhattan\n44.0\n\n\n10\nMuseums\nManhattan\n87.0\n\n\n11\nTheatres\nManhattan\n115.0\n\n\n12\nArt Galleries\nQueens\n24.0\n\n\n13\nLibraries\nQueens\n65.0\n\n\n14\nMuseums\nQueens\n12.0\n\n\n15\nTheatres\nQueens\n2.0\n\n\n16\nArt Galleries\nStaten Island\n3.0\n\n\n17\nLibraries\nStaten Island\n13.0\n\n\n18\nMuseums\nStaten Island\n9.0\n\n\n19\nTheatres\nStaten Island\n0.0\n\n\n\n\n\n\n\n\nsns.set_theme(style=\"whitegrid\")\n\ncolor_map = [\"#550527\", \"#688E26\", \"#FAA613\", \"#F44708\", \"#A10702\"]\nsns.set_palette(color_map)\n\nsns.catplot(\n    data=indoor_melt, kind=\"bar\",\n    x=\"Type\", \n    y=\"value\", \n    hue=\"BoroName\",\n    aspect=2, \n    alpha=1\n).set_axis_labels(\n    \"Type of Leisure Space\", \"Counts\"\n).set(title=\"Distribution of 4 Types of Leisure Spaces in Each Borough\")\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\nDays and Location of Open Streets in NYC: A Heatmap\n\n#heatmap for open_streets: borough x Days of the week \nopen_streets_days_melt= open_streets_days.melt(id_vars=['appronstre','BoroName'], value_vars=['monday','tuesday','wednesday', 'thursday', 'friday','saturday','sunday'])\n\n\n\n\n\n\n\n\nappronstre\nBoroName\nvariable\nvalue\n\n\n\n\n0\nDEISIUS STREET\nStaten Island\nmonday\n1\n\n\n1\nSUFFOLK AVENUE\nStaten Island\nmonday\n0\n\n\n2\nSUFFOLK AVENUE\nStaten Island\nmonday\n0\n\n\n3\nVERMONT COURT\nStaten Island\nmonday\n0\n\n\n4\n9 STREET\nStaten Island\nmonday\n0\n\n\n...\n...\n...\n...\n...\n\n\n2557\nBECK STREET\nBronx\nsunday\n0\n\n\n2558\nNEWKIRK AVENUE\nBrooklyn\nsunday\n1\n\n\n2559\n18 STREET\nBrooklyn\nsunday\n0\n\n\n2560\n34 AVENUE\nQueens\nsunday\n1\n\n\n2561\nDECATUR STREET\nBrooklyn\nsunday\n0\n\n\n\n\n2562 rows × 4 columns\n\n\n\n\nopen_streets_seaborn = open_streets_days_melt.groupby(['variable','BoroName']).sum().reset_index()\n\n# sort the order of day from monday to sunday \n\norder=['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\nopen_streets_seaborn['variable'] = pd.Categorical(open_streets_seaborn['variable'], categories=order, ordered=True)\nopen_streets_seaborn = open_streets_seaborn.sort_values(by='variable')\n\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/3045256453.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  open_streets_seaborn = open_streets_days_melt.groupby(['variable','BoroName']).sum().reset_index()\n\n\n\nimport seaborn as sns\n\nsns.set_theme()\n\n# Load the example flights dataset and convert to long-form\n\nopen_street_heatmap = (\n    open_streets_seaborn\n    .pivot(index=\"BoroName\", columns=\"variable\", values=\"value\")\n)\n\n# Draw a heatmap with the numeric values in each cell\nf, ax = plt.subplots(figsize=(9, 6))\n\nax = sns.heatmap(open_street_heatmap, annot=True, linewidths=.5)\nax.set(xlabel=\"Days in a week\", ylabel=\"Borough\", title=\"Number of Streets Open on Specific Days\")\n\n\n[Text(0.5, 33.249999999999986, 'Days in a week'),\n Text(79.75, 0.5, 'Borough'),\n Text(0.5, 1.0, 'Number of Streets Open on Specific Days')]\n\n\n\n\n\nFrom both charts, we can see Manhattan has disproportional number of art galleries and open streets in comparison to other boroughs. Museums and theatres are predominantly located on Manhattan as well. Interestingly, the distribution of libraries seems more even. However, overall, we are seeing leisure spaces are disproportional abundant and diverse in Manhattan, then brooklyn and queens, leaving Bronx and Staten Island less resourceful."
  },
  {
    "objectID": "analysis/Ass-2.html#chart-iii-altair-charts-x3",
    "href": "analysis/Ass-2.html#chart-iii-altair-charts-x3",
    "title": "Where to spend leisure time in NYC?",
    "section": "Chart III: Altair Charts (x3)",
    "text": "Chart III: Altair Charts (x3)\nFurthering the exploration on indoor leisure spaces, I created one chart on its ditribution in relation with median household income, to see if it embeds any socio-economic inequality. I then creatd a map to visualize this relationship, which can also be helpful to locate different kinds of leisure spaces in the city.\nMy third chart is an interactive bar chart on open streets in Brookylen, where audience can choose the day of the week to see all open streets approved for that day and the time they open and close. This could potentially be developed into a tool for residents and tourists to track open streets.\n\nBrush Selection: Indoor Space and Income\n\nneighbor_indoor = geo_total_indoor.groupby(['NTAName','Type']).count().reset_index().drop(['Zip', 'Address', 'geometry','index_right','BoroName', 'CT2020'], axis=1)\nneighbor_indoor = neighbor_indoor.rename(columns={'Name': 'Count'})\nneighbor_income_indoor = neighbor_income.merge(neighbor_indoor, on='NTAName', how='left')\n\nneighbor_income_indoor_pivot=neighbor_income_indoor.pivot(index=['NTAName', 'BoroName','estimate'], columns=\"Type\", values=\"Count\").reset_index().fillna(0)\nneighbor_income_indoor_pivot_1 = neighbor_income_indoor_pivot.drop(neighbor_income_indoor_pivot.columns[[3]],axis=1)\nincome_indoor_pivot = neighbor_income_indoor_pivot_1.melt(id_vars=[\"NTAName\", \"estimate\", \"BoroName\"], value_vars=[\"Art Galleries\", \"Libraries\", \"Museums\", \"Theatres\"], var_name=\"Types\",value_name=\"Count\")\nincome_indoor_pivot_filtered = income_indoor_pivot.loc[(income_indoor_pivot['Count'] &lt;20)]\nincome_indoor_pivot_filtered = income_indoor_pivot_filtered.rename(columns={'estimate': 'Median Household Income'})\n\n\nbrush = alt.selection_interval()\n\nBrush_Chart = (\nalt.Chart(income_indoor_pivot_filtered)\n   .mark_point()\n   .encode(\n       x=alt.X(\"Median Household Income:Q\", scale=alt.Scale(zero=False)),\n       y=alt.Y(\"Count:Q\", scale=alt.Scale(zero=False)),\n       color=alt.condition(brush, \"BoroName:N\", alt.value(\"lightgray\")),\n       tooltip=[\"NTAName\",\"BoroName:N\", \"Median Household Income:Q\", \"Count:Q\"])\n   .add_params(brush)\n   .properties(width=200, height=200)\n   .facet(column=\"Types:N\")\n)\n\nBrush_Chart\n\n\n\n\n\n\n\n\n\nMap: Income and Indoor Leisure Spaces - Relationships\n\nNTA = pd.read_csv(\"data/2020 Neighborhood.csv\")\nNTA['geometry'] = gpd.GeoSeries.from_wkt(NTA['the_geom'])\nNTA_geo = gpd.GeoDataFrame(NTA, geometry='geometry', crs=4326)\nNTA_geo = NTA_geo.to_crs(epsg=2263)\ngeo_total_indoor = geo_total_indoor.to_crs(epsg=2263)\n\ngeo_NTA = NTA_geo[['NTAName', 'geometry']]\n\ntracts_income_1 = tracts_clean.merge(geo_income.drop(columns='geometry'), on='GEOID', how='left').dropna()\nneighbor_income_1 = tracts_income.groupby(['NTAName', 'BoroName']).median(['estimate']).reset_index()\n\nNTA_income = geo_NTA.merge(neighbor_income_1, on='NTAName', how='left')\n\n\ngeo_total_indoor_1 = geo_total_indoor\n\ngeo_total_indoor_1['lon'] = geo_total_indoor_1['geometry'].x\ngeo_total_indoor_1['lat'] = geo_total_indoor_1['geometry'].y\n\n\nIncome = (\n    alt.Chart(NTA_income)\n    .mark_geoshape(stroke=\"white\")\n    .encode(\n        tooltip=[\"NTAName:N\", \"estimate:Q\", \"moe:Q\"],\n        color=alt.Color(\"estimate:Q\", scale=alt.Scale(scheme=\"greys\")),\n    )\n    # Important! Otherwise altair will try to re-project your data\n    .project(type=\"identity\", reflectY=True)\n    .properties(width=1000, height=800).interactive()\n)\n\nIndoorSpaces = (\n    alt.Chart(geo_total_indoor_1)\n    .mark_circle(size=10)\n    .encode(tooltip=['Name','Type','Address'],\n           longitude=\"lon\", latitude=\"lat\",\n           color=alt.Color('Type:N', scale=alt.Scale(scheme=\"lightmulti\"))\n         ).project(type=\"identity\", reflectY=True)\n)\n\n\n\nmap_1 = Income + IndoorSpaces\nmap_1\n\n\n\n\n\n\n\n\nSimilarly, libraries seem to be the least discriminatory type of leisure space. For art galleries and museums, while there are some distributed in mid to lower income neighborhood, the higher income neighborhood has higher density of such leisure space. Theatres, at the same time, is mostly located in mid to higher income neighborhood. This trend can be clearly see on the map too. Such spatial distribution means, for residents in lower to mid income neighborhood, they might have to travel further for accessing those spaces.\n\n\nOpen Street Time\n\nopen_csv = pd.read_csv(\"data/Open Streets CSV.csv\")\nopen_csv['geometry'] = gpd.GeoSeries.from_wkt(open_csv['the Geom'])\nopen_csv_geo = gpd.GeoDataFrame(open_csv, geometry='geometry', crs=2263)\nopen_csv_geo = open_csv_geo.to_crs(epsg=4326)\n\nopen_7days_time = open_csv_geo.drop(['apprDaysWe','Object ID', 'Organization Name', 'Approved From Street', 'Approved To Street', 'apprStartD', 'apprEndDat', 'Shape_STLe', 'segmentidt', 'segmentidf', 'lionversion', 'the Geom'], axis=1)\nopen_7days_time = open_7days_time.drop_duplicates(subset = \"Approved On Street\")\nopen_7days_time_melt = open_7days_time.melt(id_vars=['Approved On Street', 'Borough Name'], value_vars=['Approved Monday Open', 'Approved Monday Close', 'Approved Tuesday Open', 'Approved Tuesday Close', 'Approved Wednesday Open', 'Approved Wednesday Close', 'Approved Thursday Open', 'Approved Thursday Close', 'Approved Friday Open', 'Approved Friday Close', 'Approved Saturday Open', 'Approved Saturday Close', 'Approved Sunday Open', 'Approved Sunday Close']).dropna()\n#open_7days_time_melt['value'] =  pd.to_datetime(open_7days_time_melt['value']).dt.time\nopen_time_Brooklyn = open_7days_time_melt[(open_7days_time_melt['Borough Name'] == 'Brooklyn')]\nopen_time_Brooklyn\n\n\n\n\n\n\n\n\nApproved On Street\nBorough Name\nvariable\nvalue\n\n\n\n\n6\nRIDGE BOULEVARD\nBrooklyn\nApproved Monday Open\n10:00\n\n\n9\n82 STREET\nBrooklyn\nApproved Monday Open\n08:30\n\n\n10\n48 STREET\nBrooklyn\nApproved Monday Open\n13:30\n\n\n11\n43 STREET\nBrooklyn\nApproved Monday Open\n09:30\n\n\n12\nALBEMARLE ROAD\nBrooklyn\nApproved Monday Open\n08:00\n\n\n...\n...\n...\n...\n...\n\n\n2282\nJEFFERSON AVENUE\nBrooklyn\nApproved Sunday Close\n21:00\n\n\n2287\nSHARON STREET\nBrooklyn\nApproved Sunday Close\n20:00\n\n\n2288\nTROUTMAN STREET\nBrooklyn\nApproved Sunday Close\n22:00\n\n\n2289\nRANDOLPH STREET\nBrooklyn\nApproved Sunday Close\n23:00\n\n\n2354\nLEXINGTON AVENUE\nBrooklyn\nApproved Sunday Close\n20:00\n\n\n\n\n378 rows × 4 columns\n\n\n\n\nopen_time_Brooklyn['time'] = open_7days_time_melt['variable'].str.extract('(Open|Close)')\nopen_time_Brooklyn['dayweek'] = open_7days_time_melt['variable'].str.extract('(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)')\nopen_time_Brooklyn= open_time_Brooklyn.pivot(index=['Approved On Street', 'dayweek'], columns=\"time\", values=\"value\").reset_index()\n\norder=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nopen_time_Brooklyn['dayweek'] = pd.Categorical(open_time_Brooklyn['dayweek'], categories=order, ordered=True)\nopen_time_Brooklyn = open_time_Brooklyn.sort_values(by='dayweek').reset_index()\n\nopen_time_Brooklyn\n\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1900940832.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  open_time_Brooklyn['time'] = open_7days_time_melt['variable'].str.extract('(Open|Close)')\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1900940832.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  open_time_Brooklyn['dayweek'] = open_7days_time_melt['variable'].str.extract('(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)')\n\n\n\n\n\n\n\n\ntime\nindex\nApproved On Street\ndayweek\nClose\nOpen\n\n\n\n\n0\n139\nSUMMIT STREET\nMonday\n14:30\n12:30\n\n\n1\n151\nUNDERHILL AVENUE\nMonday\n20:00\n08:00\n\n\n2\n28\n82 STREET\nMonday\n15:30\n08:30\n\n\n3\n143\nTHATFORD AVENUE\nMonday\n18:00\n11:00\n\n\n4\n33\nAITKEN PLACE\nMonday\n15:00\n11:00\n\n\n...\n...\n...\n...\n...\n...\n\n\n184\n147\nTOMPKINS AVENUE\nSunday\n20:00\n11:00\n\n\n185\n51\nBEVERLEY ROAD\nSunday\n18:00\n10:00\n\n\n186\n112\nRANDOLPH STREET\nSunday\n23:00\n12:00\n\n\n187\n10\n4 STREET\nSunday\n22:00\n08:00\n\n\n188\n157\nVANDERBILT AVENUE\nSunday\n23:00\n11:00\n\n\n\n\n189 rows × 5 columns\n\n\n\n\nselection = alt.selection_multi(fields=['dayweek'])\ncolor = alt.condition(selection,\n                      alt.Color('dayweek:N', legend=None, \n                      scale=alt.Scale(scheme='category10')),\n                      alt.value('lightgray'))\n\nopacity = alt.condition(selection,\n                        alt.value(1), alt.value(0))\n\n\n\nbar = alt.Chart(open_time_Brooklyn).mark_bar().encode(\n        x='Open',\n        x2='Close',\n        y='Approved On Street',\n        color=color,\n        opacity=opacity,\n        tooltip=['Open', 'Close', 'Approved On Street', 'dayweek']).properties(\n        width=500,\n        height=1000).interactive()\n\nlegend = alt.Chart(open_time_Brooklyn).mark_bar().encode(\n    y=alt.Y('dayweek:N', axis=alt.Axis(orient='right')),\n    color=color\n).add_selection(\nselection\n)\n\nA_Chart = bar | legend\nA_Chart\n\n\n\n\n\n\n\n\nThis interactive bar chart on Brooklyn Open Street serves as a pilot that can be adapted for data of all five boroughs. By comparing to open and close time, we can observe that many streets are approved to be open streets with later opening and closing time on weekends. To improve this bar chart the status of each street can be added (i.e., whether they are approved to be closed fully or partially or only on school days), which is important for visitors as well."
  },
  {
    "objectID": "analysis/Ass-2.html#dashboard",
    "href": "analysis/Ass-2.html#dashboard",
    "title": "Where to spend leisure time in NYC?",
    "section": "Dashboard",
    "text": "Dashboard\nThe dashboard below is an upgrade from the indoor space and income grouped scatter plot. This dashboard makes it easier to explore the spatial distribution of indoor spaces of certain type or in neighborhoods with certain level of income. Through selecting points with higher counts, we can see neighborhoods with higher density of identified indoor leisure spaces are mostly located in Manhattan.\n\nbrush = alt.selection_interval()\n\npoints = alt.Chart(income_indoor_pivot_filtered).mark_point().encode(\n       x=alt.X(\"Median Household Income:Q\", scale=alt.Scale(zero=False)),\n       y=alt.Y(\"Count:Q\", scale=alt.Scale(zero=False)),\n       color=alt.condition(brush, \"BoroName:N\", alt.value(\"lightgray\")),\n       tooltip=[\"NTAName\",\"BoroName:N\", \"Median Household Income:Q\", \"Count:Q\"]).add_params(brush\n).properties(width=200, height=200\n).facet(column=\"Types:N\")\n\n\nbars = alt.Chart(income_indoor_pivot_filtered).mark_bar().encode(\n    y='BoroName:N',\n    color='BoroName:N',\n    x='Count:Q'\n).transform_filter(\n    brush\n)\n\nDashboard = points & bars\nDashboard"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap"
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "In-Class Projects",
    "section": "",
    "text": "In-Class Projects\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features.\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader."
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007"
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ann Zhang Python Projects",
    "section": "",
    "text": "Here exhibits course assignments completed as a part of MUSA 550 Geospatial Data Science in Python. The website is made possible using Quarto.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Ann Zhang Python Projects",
    "section": "",
    "text": "Here exhibits course assignments completed as a part of MUSA 550 Geospatial Data Science in Python. The website is made possible using Quarto.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online."
  },
  {
    "objectID": "index.html#find-out-more",
    "href": "index.html#find-out-more",
    "title": "Ann Zhang Python Projects",
    "section": "Find out more",
    "text": "Find out more\nMy personal online website can be found here: https://annannzhang.github.io/"
  },
  {
    "objectID": "analysis/assignment-1.html",
    "href": "analysis/assignment-1.html",
    "title": "1 - Philly Donut Effect",
    "section": "",
    "text": "In this assignment, we will practice our pandas skills and explore the “Donut Effect” within Philadelphia. The “Donut Effect” describes the following phenomenon: with more flexible working options and pandemic-driven density fears, people left urban dense cores and opted for more space in city suburbs, driving home and rental prices up in the suburbs relative to city centers.\nWe will be working with Zillow data for the Zillow Home Value Index (ZHVI) for Philadelphia ZIP codes. The goal will be to calculate home price appreciation in Philadelphia, comparing those ZIP codes in Center City (the central business district) to those not in Center City."
  },
  {
    "objectID": "analysis/assignment-1.html#load-the-data",
    "href": "analysis/assignment-1.html#load-the-data",
    "title": "1 - Philly Donut Effect",
    "section": "1. Load the data",
    "text": "1. Load the data\nI’ve already downloaded the relevant data file and put in the data/ folder. Let’s load it using pandas.\nNote: Be sure to use a relative file path to make it easier to load your data when grading. See this guide for more info.\n\n\nCode\nimport pandas as pd\n\n\n\n\nCode\nimport numpy as np\n\n\n\n\nCode\nzip_df = pd.read_csv(\"data/Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")"
  },
  {
    "objectID": "analysis/assignment-1.html#trim-the-data-to-just-philadelphia",
    "href": "analysis/assignment-1.html#trim-the-data-to-just-philadelphia",
    "title": "1 - Philly Donut Effect",
    "section": "2. Trim the data to just Philadelphia",
    "text": "2. Trim the data to just Philadelphia\nSelect the subset of the dataframe for Philadelphia, PA.\n\n\nCode\n# or use isin to trim data\n\nstate_trim = zip_df[\"StateName\"].isin([\"PA\"])\nstate_trim_df= zip_df.loc[state_trim]\n\nphilly_df = state_trim_df.loc[state_trim_df[\"City\"] == \"Philadelphia\"]\nphilly_df.head()\n\n\n\n\n\n\n\n\n\nRegionID\nSizeRank\nRegionName\nRegionType\nStateName\nState\nCity\nMetro\nCountyName\n2000-01-31\n...\n2021-10-31\n2021-11-30\n2021-12-31\n2022-01-31\n2022-02-28\n2022-03-31\n2022-04-30\n2022-05-31\n2022-06-30\n2022-07-31\n\n\n\n\n125\n65810\n126\n19143\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n60701.0\n...\n173114.0\n172087.0\n171445.0\n171542.0\n171680.0\n171878.0\n171607.0\n171333.0\n171771.0\n172611.0\n\n\n247\n65779\n249\n19111\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n85062.0\n...\n257911.0\n260104.0\n262257.0\n263715.0\n264809.0\n265684.0\n267222.0\n269460.0\n272201.0\n274446.0\n\n\n338\n65791\n340\n19124\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n47155.0\n...\n156225.0\n157780.0\n159029.0\n159274.0\n159886.0\n160780.0\n161929.0\n163625.0\n165020.0\n166009.0\n\n\n423\n65787\n426\n19120\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n59285.0\n...\n161167.0\n161807.0\n162634.0\n162972.0\n163597.0\n164008.0\n164887.0\n165860.0\n167321.0\n168524.0\n\n\n509\n65772\n512\n19104\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n74255.0\n...\n220270.0\n221454.0\n222006.0\n220760.0\n217933.0\n216447.0\n216424.0\n218663.0\n220453.0\n223443.0\n\n\n\n\n5 rows × 280 columns"
  },
  {
    "objectID": "analysis/assignment-1.html#melt-the-data-into-tidy-format",
    "href": "analysis/assignment-1.html#melt-the-data-into-tidy-format",
    "title": "1 - Philly Donut Effect",
    "section": "3. Melt the data into tidy format",
    "text": "3. Melt the data into tidy format\nLet’s transform the data from wide to tidy using the pd.melt() function. Create a new column in your data called “ZHVI” that holds the ZHVI values.\n\n\nCode\ndef looks_like_a_date(col):\n    \"\"\"A function that tests if a string starts with '20'\"\"\"\n    \n    return col.startswith(\"20\")\n\n\n\n\nCode\nphilly_tidy = philly_df.melt(\n    id_vars = [\"RegionName\",\"RegionID\"],\n    value_vars = list(filter(looks_like_a_date, philly_df.columns)),\n    var_name = \"Date\",\n    value_name = \"ZHVI\",\n)\n    \n\n\n\n\nCode\nphilly_tidy\n\n\n\n\n\n\n\n\n\nRegionName\nRegionID\nDate\nZHVI\n\n\n\n\n0\n19143\n65810\n2000-01-31\n60701.0\n\n\n1\n19111\n65779\n2000-01-31\n85062.0\n\n\n2\n19124\n65791\n2000-01-31\n47155.0\n\n\n3\n19120\n65787\n2000-01-31\n59285.0\n\n\n4\n19104\n65772\n2000-01-31\n74255.0\n\n\n...\n...\n...\n...\n...\n\n\n12461\n19153\n65820\n2022-07-31\n247560.0\n\n\n12462\n19118\n65785\n2022-07-31\n746009.0\n\n\n12463\n19102\n65770\n2022-07-31\n347614.0\n\n\n12464\n19127\n65794\n2022-07-31\n318732.0\n\n\n12465\n19137\n65804\n2022-07-31\n222107.0\n\n\n\n\n12466 rows × 4 columns"
  },
  {
    "objectID": "analysis/assignment-1.html#split-the-data-for-zip-codes-inoutside-center-city",
    "href": "analysis/assignment-1.html#split-the-data-for-zip-codes-inoutside-center-city",
    "title": "1 - Philly Donut Effect",
    "section": "4. Split the data for ZIP codes in/outside Center City",
    "text": "4. Split the data for ZIP codes in/outside Center City\nTo compare home appreciation in Center City vs. outside Center City, we’ll need to split the data into two dataframes, one that holds the Center City ZIP codes and one that holds the data for the rest of the ZIP codes in Philadelphia.\nTo help with this process, I’ve included a list of ZIP codes that make up the “greater Center City” region of Philadelphia. Use this list to split the melted data into two dataframes.\n\n\nCode\ngreater_center_city_zip_codes = [\n    19123,\n    19102,\n    19103,\n    19106,\n    19107,\n    19109,\n    19130,\n    19146,\n    19147,\n]\n\n\n\n\nCode\n# CENTER CITY\ncenter_city_zip = philly_tidy[\"RegionName\"].isin(greater_center_city_zip_codes)\ncenter_city_zip\ncenter_city = philly_tidy.loc[center_city_zip].copy()\ncenter_city\n\n\n\n\n\n\n\n\n\nRegionName\nRegionID\nDate\nZHVI\n\n\n\n\n9\n19146\n65813\n2000-01-31\n97460.0\n\n\n12\n19147\n65814\n2000-01-31\n119919.0\n\n\n14\n19103\n65771\n2000-01-31\n183436.0\n\n\n18\n19130\n65797\n2000-01-31\n128477.0\n\n\n33\n19107\n65775\n2000-01-31\n128049.0\n\n\n...\n...\n...\n...\n...\n\n\n12438\n19130\n65797\n2022-07-31\n431501.0\n\n\n12453\n19107\n65775\n2022-07-31\n330958.0\n\n\n12457\n19123\n65790\n2022-07-31\n443152.0\n\n\n12458\n19106\n65774\n2022-07-31\n407423.0\n\n\n12463\n19102\n65770\n2022-07-31\n347614.0\n\n\n\n\n2168 rows × 4 columns\n\n\n\n\n\nCode\n# Out_of_Center_City\nout_of_center_city_zip = ~philly_tidy[\"RegionName\"].isin(greater_center_city_zip_codes)\nout_of_center_city_zip\nout_of_center_city = philly_tidy.loc[out_of_center_city_zip].copy()\nout_of_center_city\n\n\n\n\n\n\n\n\n\nRegionName\nRegionID\nDate\nZHVI\n\n\n\n\n0\n19143\n65810\n2000-01-31\n60701.0\n\n\n1\n19111\n65779\n2000-01-31\n85062.0\n\n\n2\n19124\n65791\n2000-01-31\n47155.0\n\n\n3\n19120\n65787\n2000-01-31\n59285.0\n\n\n4\n19104\n65772\n2000-01-31\n74255.0\n\n\n...\n...\n...\n...\n...\n\n\n12460\n19129\n65796\n2022-07-31\n302177.0\n\n\n12461\n19153\n65820\n2022-07-31\n247560.0\n\n\n12462\n19118\n65785\n2022-07-31\n746009.0\n\n\n12464\n19127\n65794\n2022-07-31\n318732.0\n\n\n12465\n19137\n65804\n2022-07-31\n222107.0\n\n\n\n\n10298 rows × 4 columns"
  },
  {
    "objectID": "analysis/assignment-1.html#compare-home-value-appreciation-in-philadelpia",
    "href": "analysis/assignment-1.html#compare-home-value-appreciation-in-philadelpia",
    "title": "1 - Philly Donut Effect",
    "section": "5. Compare home value appreciation in Philadelpia",
    "text": "5. Compare home value appreciation in Philadelpia\nIn this step, we’ll calculate the average percent increase in ZHVI from March 2020 to March 2022 for ZIP codes in/out of Center City. We’ll do this by:\n\nWriting a function (see the template below) that will calculate the percent increase in ZHVI from March 31, 2020 to March 31, 2022\nGroup your data and apply this function to calculate the ZHVI percent change for each ZIP code in Philadelphia. Do this for both of your dataframes from the previous step.\nCalculate the average value across ZIP codes for both sets of ZIP codes and then compare\n\nYou should see much larger growth for ZIP codes outside of Center City…the Donut Effect!\n\n\nCode\ndef calculate_percent_increase(group_df):\n    \n    march_20sel = group_df[\"Date\"] == \"2020-03-31\"\n    march_22sel = group_df[\"Date\"] == \"2022-03-31\"\n    \n    march_20 = group_df.loc[march_20sel].squeeze()\n    march_22 = group_df.loc[march_22sel].squeeze()\n    \n    columns = [\"ZHVI\"]\n    \n    return (march_22[columns] / march_20[columns] - 1) * 100\n\n\n\n\nCode\n# Center City Grouped\ngrouped_center_city = center_city.groupby(\"RegionName\")\nresult_center_city = grouped_center_city.apply(calculate_percent_increase)\nresult_center_city\n\n\n\n\n\n\n\n\n\nZHVI\n\n\nRegionName\n\n\n\n\n\n19102\n-1.716711\n\n\n19103\n-1.696176\n\n\n19106\n2.520802\n\n\n19107\n2.883181\n\n\n19123\n5.212747\n\n\n19130\n6.673031\n\n\n19146\n6.480788\n\n\n19147\n6.139806\n\n\n\n\n\n\n\n\n\nCode\n# Outside Center City Grouped\ngrouped_outside = out_of_center_city.groupby(\"RegionName\")\nresult_outside = grouped_outside.apply(calculate_percent_increase)\nresult_outside\n\n\n\n\n\n\n\n\n\nZHVI\n\n\nRegionName\n\n\n\n\n\n19104\n14.539797\n\n\n19111\n28.690446\n\n\n19114\n21.074312\n\n\n19115\n22.455454\n\n\n19116\n23.079842\n\n\n19118\n17.585001\n\n\n19119\n17.478667\n\n\n19120\n26.927423\n\n\n19121\n26.228643\n\n\n19122\n10.275804\n\n\n19124\n28.743474\n\n\n19125\n11.007135\n\n\n19126\n20.819254\n\n\n19127\n20.023926\n\n\n19128\n21.887555\n\n\n19129\n15.598565\n\n\n19131\n23.363129\n\n\n19132\n72.218386\n\n\n19133\n36.143992\n\n\n19134\n23.936841\n\n\n19135\n28.115259\n\n\n19136\n26.487833\n\n\n19137\n23.248505\n\n\n19138\n24.662626\n\n\n19139\n37.008969\n\n\n19140\n57.150847\n\n\n19141\n26.441684\n\n\n19142\n44.564396\n\n\n19143\n23.951077\n\n\n19144\n21.094020\n\n\n19145\n7.634693\n\n\n19148\n6.963237\n\n\n19149\n24.916458\n\n\n19150\n18.735248\n\n\n19151\n19.651429\n\n\n19152\n21.993528\n\n\n19153\n38.240461\n\n\n19154\n17.930932\n\n\n\n\n\n\n\n\n\nCode\n# Center City Growth\nresult_cc_mean = result_center_city.mean().squeeze()\n\n\n\n\nCode\n# Outside of Center City Growth\nresult_ot_mean = result_outside.mean().squeeze()\n\n\n\n\nCode\nHV_growth = pd.Series([result_cc_mean,result_ot_mean])\nLocation = pd.Series([\"Center City\", \"Outside Center City\"])\nfinal_result_df = pd.DataFrame({\"Location\": Location, \"Home Value growth (%)\": HV_growth})\nfinal_result_df\n\n\n\n\n\n\n\n\n\nLocation\nHome Value growth (%)\n\n\n\n\n0\nCenter City\n3.312183\n\n\n1\nOutside Center City\n25.022864\n\n\n\n\n\n\n\nDonut Effect it is!"
  },
  {
    "objectID": "analysis/assignment-2.html",
    "href": "analysis/assignment-2.html",
    "title": "2 - Where to spend leisure time in NYC?",
    "section": "",
    "text": "This project aims to explore the spatial distribution of some leisure spaces in New York City, including indoor spaces like museums, art galleries, and theatres, and outddoor spaces like parks or open streets. The project first aims to idenfity some patterns of spatial distribution of such spaces in each borough and neighborhood, and in relation to population and median household income. Then some interactive visualization is created for both local NYC residents and tourists to access to information about some leisure spaces more easily."
  },
  {
    "objectID": "analysis/assignment-2.html#imports",
    "href": "analysis/assignment-2.html#imports",
    "title": "2 - Where to spend leisure time in NYC?",
    "section": "Imports",
    "text": "Imports\n\n\nCode\nimport altair as alt\nimport geopandas as gpd\nimport hvplot.pandas\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport datetime\nimport math\n\n%matplotlib inline"
  },
  {
    "objectID": "analysis/assignment-2.html#nyc-base-maps",
    "href": "analysis/assignment-2.html#nyc-base-maps",
    "title": "2 - Where to spend leisure time in NYC?",
    "section": "NYC Base Maps",
    "text": "NYC Base Maps\n\nTracts, Neighborhood, and Borough\n\n\nCode\nnyc_tracts = pd.read_csv(\"data/2022 Census Tracts.csv\")\nnyc_tracts['geometry'] = gpd.GeoSeries.from_wkt(nyc_tracts['the_geom'])\ngeo_tracts = gpd.GeoDataFrame(nyc_tracts, geometry='geometry')\ngeo_tracts = geo_tracts.set_crs(epsg=4326)\ntracts = geo_tracts[['BoroName', 'CT2020', 'BoroCT2020', 'NTAName', 'Shape_Area','geometry','GEOID']].copy()\ntracts_clean = tracts[['BoroName', 'CT2020', 'NTAName', 'geometry','GEOID']]\nBoro_NTA = tracts_clean[['BoroName', 'NTAName']].drop_duplicates(subset = \"NTAName\")"
  },
  {
    "objectID": "analysis/assignment-2.html#dataset-selection-setup",
    "href": "analysis/assignment-2.html#dataset-selection-setup",
    "title": "2 - Where to spend leisure time in NYC?",
    "section": "Dataset Selection & Setup",
    "text": "Dataset Selection & Setup\nAfter glancing through NYC Open Data portal, I have selected art galleries, museums, libraries, theatres, and parks as common leisure spaces. In addition, I explored data of open streets, a program thatt had been implemented in many global cities including New York City to transform roads into public spaces for cultural and all kinds of events on particular days (mostly on weekends).\nThis step is to bring in all datasets, clean them up, and aggregate different types of leisure spaces. The aggregated dataframe is then spacially joined with tracts, neighborhood, and boroughs, ready for further investigation on their geospatial distributions.\nSimilar data wrangling is performed on parks. Since parks come in polygon instead of points, which may result in problems with spatial joins, in the case when one park falls in two or more tracts or neighborhood. Hence, the geometry of parks’ centroids is adopted to replace the polygon geometry for further analysis.\nThe Open Street data comes with more detailed information on the approved time for each street. The days of week (e.g. Monday) and time of the day (e.g. 7:30) that it opens and closes are rearranged by melting and pivoting.\nAdditionally, 2020 Census Data on population and median household income are brought in for analysis.\n\nIndoor Leisure Spaces: Art Galleries, Museums, Libraries, and Theatres\n\n\nCode\n# establish geodataframe\nart_galleries = gpd.read_file(\"data/art galleries.geojson\")\ngeo_art_galleries = gpd.GeoDataFrame(art_galleries, geometry='geometry')\ngeo_art_galleries = art_galleries.set_crs(epsg=4326)\n\n# add type\nart_galleries_clean = geo_art_galleries[['name','zip','address1','geometry']]\nart_galleries_clean.loc[:,\"Type\"]= \"Art Galleries\"\nart_galleries_clean.rename(\n    columns={\"address1\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\n\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1429971093.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  art_galleries_clean.rename(\n\n\n\n\nCode\n# Museums \n\n# establish geodataframe\nmuseums = gpd.read_file(\"data/museums.geojson\")\ngeo_museums= gpd.GeoDataFrame(museums, geometry='geometry')\ngeo_museums = geo_museums.set_crs(epsg=4326)\ngeo_museums\n\n# add type\nmuseums_clean = geo_museums[['name','zip','adress1','geometry']]\nmuseums_clean.loc[:,\"Type\"]= \"Museums\"\nmuseums_clean.rename(\n    columns={\"adress1\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\nmuseums_clean\n\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/3071166290.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  museums_clean.rename(\n\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\n\n\n\n\n0\nAlexander Hamilton U.S. Custom House\n10004.0\n1 Bowling Grn\nPOINT (-74.01376 40.70382)\nMuseums\n\n\n1\nAlice Austen House Museum\n10305.0\n2 Hylan Blvd\nPOINT (-74.06303 40.61512)\nMuseums\n\n\n2\nAmerican Academy of Arts and Letters\n10032.0\n633 W. 155th St.\nPOINT (-73.94730 40.83385)\nMuseums\n\n\n3\nAmerican Folk Art Museum\n10019.0\n45 West 53rd Street\nPOINT (-73.97810 40.76162)\nMuseums\n\n\n4\nAmerican Immigration History Center\n0.0\nEllis Island\nPOINT (-74.03968 40.69906)\nMuseums\n\n\n...\n...\n...\n...\n...\n...\n\n\n125\nAmerican Sephardi Federation / Sephardic House\n10011.0\n15 W. 16th St.\nPOINT (-73.99389 40.73808)\nMuseums\n\n\n126\nYIVO Institute for Jewish Research\n10011.0\n15 W. 16th St.\nPOINT (-73.99379 40.73796)\nMuseums\n\n\n127\nAmerican Jewish Historical Society\n10011.0\n15 W. 16th St.\nPOINT (-73.99393 40.73802)\nMuseums\n\n\n128\nYeshiva University Museum\n10011.0\n15 W. 16th St.\nPOINT (-73.99382 40.73805)\nMuseums\n\n\n129\nCenter For Jewish History\n10011.0\n15 W. 16th St.\nPOINT (-73.99387 40.73799)\nMuseums\n\n\n\n\n130 rows × 5 columns\n\n\n\n\n\nCode\n# Libraries\n\n# establish geodataframe\nlibrary = gpd.read_file(\"data/libraries.geojson\")\ngeo_library = gpd.GeoDataFrame(library, geometry='geometry')\ngeo_library= geo_library.set_crs(epsg=4326)\ngeo_library\n\n# add type\nlibrary_clean = geo_library[['name','zip','streetname','geometry']]\nlibrary_clean.loc[:,\"Type\"]= \"Libraries\"\nlibrary_clean.rename(\n    columns={\"streetname\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\nlibrary_clean\n\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/3524914390.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  library_clean.rename(\n\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\n\n\n\n\n0\n115th Street\n10026\nWest 115th Street\nPOINT (-73.95353 40.80298)\nLibraries\n\n\n1\n125th Street\n10035\nEast 125th Street\nPOINT (-73.93485 40.80302)\nLibraries\n\n\n2\n53rd Street\n10019\nWest 53rd Street\nPOINT (-73.97736 40.76081)\nLibraries\n\n\n3\n58th Street\n10022\nEast 58th Street\nPOINT (-73.96938 40.76219)\nLibraries\n\n\n4\n67th Street\n10065\nEast 67th Street\nPOINT (-73.95955 40.76492)\nLibraries\n\n\n...\n...\n...\n...\n...\n...\n\n\n211\nSunnyside\n11104\nGreenpoint Avenue\nPOINT (-73.92167 40.74085)\nLibraries\n\n\n212\nWhitestone\n11357\n14 Road\nPOINT (-73.81070 40.78854)\nLibraries\n\n\n213\nWindsor Park\n11364\nBell Boulevard\nPOINT (-73.75562 40.73450)\nLibraries\n\n\n214\nWoodhaven\n11421\nForest Parkway\nPOINT (-73.86146 40.69453)\nLibraries\n\n\n215\nWoodside\n11377\nSkillman Avenue\nPOINT (-73.90979 40.74534)\nLibraries\n\n\n\n\n216 rows × 5 columns\n\n\n\n\n\nCode\n# Theaters\n# establish geodataframe\ntheaters = gpd.read_file(\"data/Theaters.geojson\")\ngeo_theaters = gpd.GeoDataFrame(theaters, geometry='geometry')\ngeo_theaters= geo_theaters.set_crs(epsg=4326)\ngeo_theaters\n\n\n# add type\ntheaters_clean = geo_theaters[['name','zip','address1','geometry']]\ntheaters_clean.loc[:,\"Type\"]= \"Theatres\"\ntheaters_clean.rename(\n    columns={\"address1\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\ntheaters_clean\n\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1518346627.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  theaters_clean.rename(\n\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\n\n\n\n\n0\n45th Street Theater\n10036.0\n354 West 45th Street\nPOINT (-73.99062 40.75985)\nTheatres\n\n\n1\n47th Street Theater\n10036.0\n304 West 47th Street\nPOINT (-73.98811 40.76047)\nTheatres\n\n\n2\n59E59\n10022.0\n59 East 59th Street\nPOINT (-73.97038 40.76340)\nTheatres\n\n\n3\nAcorn Theater\n10036.0\n410 West 42nd Street\nPOINT (-73.99332 40.75854)\nTheatres\n\n\n4\nAl Hirschfeld Theater\n10036.0\n302 W 45th Street\nPOINT (-73.98921 40.75926)\nTheatres\n\n\n...\n...\n...\n...\n...\n...\n\n\n112\nWestside Theater\n10036.0\n407 W 43rd St\nPOINT (-73.99255 40.75953)\nTheatres\n\n\n113\nWings Theatre\n10014.0\n154 Christopher St\nPOINT (-74.00889 40.73240)\nTheatres\n\n\n114\nWinter Garden Theatre\n10019.0\n1634 Broadway\nPOINT (-73.98348 40.76152)\nTheatres\n\n\n115\nYork Theatre\n10022.0\n619 Lexington Ave\nPOINT (-73.96998 40.75836)\nTheatres\n\n\n116\nDelacorte Theater\n0.0\nCentral Park - Mid-Park at 80th Street\nPOINT (-73.96882 40.78018)\nTheatres\n\n\n\n\n117 rows × 5 columns\n\n\n\n\nAggregate all types of indoor leisure space\n\n\nCode\nframes = [art_galleries_clean, museums_clean, library_clean, theaters_clean]\n\ntotal_indoor = pd.concat(frames)\n\ntotal_indoor\n\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\n\n\n\n\n0\nO'reilly William & Co Ltd\n10021.0\n52 E 76th St\nPOINT (-73.96273 40.77380)\nArt Galleries\n\n\n1\nOrganization of Independent Artists - Gallery 402\n10013.0\n19 Hudson St.\nPOINT (-74.00939 40.71647)\nArt Galleries\n\n\n2\nOwen Gallery\n10021.0\n19 E 75th St\nPOINT (-73.96435 40.77400)\nArt Galleries\n\n\n3\nP P O W Gallerie\n10001.0\n511 W 25th St\nPOINT (-74.00389 40.74959)\nArt Galleries\n\n\n4\nP P O W Inc\n10013.0\n476 Broome St\nPOINT (-74.00176 40.72291)\nArt Galleries\n\n\n...\n...\n...\n...\n...\n...\n\n\n112\nWestside Theater\n10036.0\n407 W 43rd St\nPOINT (-73.99255 40.75953)\nTheatres\n\n\n113\nWings Theatre\n10014.0\n154 Christopher St\nPOINT (-74.00889 40.73240)\nTheatres\n\n\n114\nWinter Garden Theatre\n10019.0\n1634 Broadway\nPOINT (-73.98348 40.76152)\nTheatres\n\n\n115\nYork Theatre\n10022.0\n619 Lexington Ave\nPOINT (-73.96998 40.75836)\nTheatres\n\n\n116\nDelacorte Theater\n0.0\nCentral Park - Mid-Park at 80th Street\nPOINT (-73.96882 40.78018)\nTheatres\n\n\n\n\n1380 rows × 5 columns\n\n\n\n\n\nSpatial Join\n\n\nCode\ngeo_total_indoor = gpd.sjoin(\n    total_indoor,  # The point data for 311 tickets\n    tracts_clean.to_crs(total_indoor.crs),  # The neighborhoods (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\ngeo_total_indoor.head()\n\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\nindex_right\nBoroName\nCT2020\nNTAName\nGEOID\n\n\n\n\n0\nO'reilly William & Co Ltd\n10021.0\n52 E 76th St\nPOINT (-73.96273 40.77380)\nArt Galleries\n84.0\nManhattan\n13000.0\nUpper East Side-Carnegie Hill\n3.606101e+10\n\n\n1\nOrganization of Independent Artists - Gallery 402\n10013.0\n19 Hudson St.\nPOINT (-74.00939 40.71647)\nArt Galleries\n17.0\nManhattan\n3900.0\nTribeca-Civic Center\n3.606100e+10\n\n\n2\nOwen Gallery\n10021.0\n19 E 75th St\nPOINT (-73.96435 40.77400)\nArt Galleries\n84.0\nManhattan\n13000.0\nUpper East Side-Carnegie Hill\n3.606101e+10\n\n\n3\nP P O W Gallerie\n10001.0\n511 W 25th St\nPOINT (-74.00389 40.74959)\nArt Galleries\n1134.0\nManhattan\n9901.0\nChelsea-Hudson Yards\n3.606101e+10\n\n\n4\nP P O W Inc\n10013.0\n476 Broome St\nPOINT (-74.00176 40.72291)\nArt Galleries\n1156.0\nManhattan\n4900.0\nSoHo-Little Italy-Hudson Square\n3.606100e+10\n\n\n\n\n\n\n\n\n\n\nOutdoor Leisure Space: Parks\n\n\nCode\n# Parks\n# establish geodataframe\nparks = gpd.read_file(\"data/Parks Properties.geojson\")\ngeo_parks = gpd.GeoDataFrame(parks, geometry='geometry')\ngeo_parks= geo_parks.set_crs(epsg=4326)\ngeo_parks\n\n\n# add type\nparks_clean = geo_parks[['signname','geometry','acres']]\nparks_clean.loc[:,\"Type\"]= \"Parks\"\nparks_clean.rename(\n    columns={\"location\": \"Address\", \"signname\": \"Name\", \"zipcode\": \"Zip\"},\n    inplace=True,)\n\nparks_clean['new_geom'] = parks_clean['geometry']\n\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1177565529.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  parks_clean.rename(\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n\n\n\n\nCode\n\n# Since many parks are in multi-polygons, which will lead to errors when doing spatial join later, a centroid of each park is generated here for smoother spatial join.\nparks_clean['geometry'] = parks_clean['geometry'].centroid\n\ngeo_parks_clean = gpd.sjoin(\n    parks_clean,  # The point data for 311 tickets\n    tracts_clean.to_crs(parks_clean.crs),  # The neighborhoods (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\ngeo_parks_clean\n\n\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/3082874007.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  parks_clean['geometry'] = parks_clean['geometry'].centroid\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n\n\n\n\n\n\n\n\n\nName\ngeometry\nacres\nType\nnew_geom\nindex_right\nBoroName\nCT2020\nNTAName\nGEOID\n\n\n\n\n0\nInwood Hill Park\nPOINT (-73.92544 40.87257)\n196.398\nParks\nMULTIPOLYGON (((-73.92093 40.86999, -73.92145 ...\n2191.0\nManhattan\n29700.0\nInwood Hill Park\n3.606103e+10\n\n\n1\nChallenge Playground\nPOINT (-73.72796 40.75662)\n2.035\nParks\nMULTIPOLYGON (((-73.72738 40.75605, -73.72783 ...\n936.0\nQueens\n152902.0\nDouglaston-Little Neck\n3.608115e+10\n\n\n2\nSunset Cove Park\nPOINT (-73.82300 40.59853)\n9.375\nParks\nMULTIPOLYGON (((-73.82218 40.59892, -73.82221 ...\n1129.0\nQueens\n107201.0\nBreezy Point-Belle Harbor-Rockaway Park-Broad ...\n3.608111e+10\n\n\n3\nGrand Central Parkway Extension\nPOINT (-73.85317 40.75316)\n249.389\nParks\nMULTIPOLYGON (((-73.85875 40.76741, -73.85976 ...\n625.0\nQueens\n39902.0\nNorth Corona\n3.608104e+10\n\n\n4\nIdlewild Park\nPOINT (-73.75229 40.65043)\n180.85\nParks\nMULTIPOLYGON (((-73.75809 40.65427, -73.75845 ...\n1013.0\nQueens\n66404.0\nSpringfield Gardens (South)-Brookville\n3.608107e+10\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2040\nMaria Hernandez Park\nPOINT (-73.92386 40.70317)\n6.873\nParks\nMULTIPOLYGON (((-73.92251 40.70351, -73.92381 ...\n1729.0\nBrooklyn\n42900.0\nBushwick (West)\n3.604704e+10\n\n\n2041\nCrotona Parkway Malls\nPOINT (-73.88477 40.84405)\n8.75\nParks\nMULTIPOLYGON (((-73.88496 40.84470, -73.88496 ...\n336.0\nBronx\n36300.0\nWest Farms\n3.600504e+10\n\n\n2042\nPark\nPOINT (-73.89807 40.84408)\n0.511\nParks\nMULTIPOLYGON (((-73.89759 40.84410, -73.89773 ...\n1229.0\nBronx\n16500.0\nClaremont Village-Claremont (East)\n3.600502e+10\n\n\n2043\nCunningham Park\nPOINT (-73.76880 40.73382)\n358.0\nParks\nMULTIPOLYGON (((-73.77466 40.72442, -73.77439 ...\n2241.0\nQueens\n128300.0\nCunningham Park\n3.608113e+10\n\n\n2044\nRoberto Clemente Ballfield\nPOINT (-73.96767 40.70635)\n1.93\nParks\nMULTIPOLYGON (((-73.96761 40.70581, -73.96735 ...\n2220.0\nBrooklyn\n54500.0\nSouth Williamsburg\n3.604705e+10\n\n\n\n\n2045 rows × 10 columns\n\n\n\n\n\nOutdoor Leisure Space: Open Streets\n\n\nCode\nopen_streets = gpd.read_file(\"data/Open Streets Locations.geojson\")\ngeo_open_streets = gpd.GeoDataFrame(open_streets, geometry='geometry', crs=2263)\ngeo_open_streets = open_streets.to_crs(epsg=4326)\n\nopen_streets_clean = geo_open_streets[['appronstre','apprtostre','apprdayswe','boroughname','reviewstat','shape_stle','geometry']]\n\n\n\n\nCode\njoin_test = gpd.sjoin(\n    open_streets_clean,  # The point data for 311 tickets\n    tracts_clean.to_crs(open_streets_clean.crs),  # The neighborhoods (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\njoin_test.head()\n\n\n\n\n\n\n\n\n\nappronstre\napprtostre\napprdayswe\nboroughname\nreviewstat\nshape_stle\ngeometry\nindex_right\nBoroName\nCT2020\nNTAName\nGEOID\n\n\n\n\n0\nDEISIUS STREET\nSTECHER STREET\nmon,tue,wed,thu,fri\nStaten Island\napprovedFullSchools\n264.932398036\nMULTILINESTRING ((-74.18738 40.53028, -74.1882...\n1318.0\nStaten Island\n17600.0\nAnnadale-Huguenot-Prince's Bay-Woodrow\n3.608502e+10\n\n\n1\nSUFFOLK AVENUE\nHAROLD STREET\nfri,sat,sun\nStaten Island\napprovedFull\n313.087821487\nMULTILINESTRING ((-74.12784 40.60288, -74.1277...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n\n\n2\nSUFFOLK AVENUE\nHAROLD STREET\nfri,sat,sun\nStaten Island\napprovedFull\n142.063500219\nMULTILINESTRING ((-74.12772 40.60202, -74.1276...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n\n\n3\nVERMONT COURT\nSUFFOLK AVENUE\nfri,sat,sun\nStaten Island\napprovedFull\n421.392020366\nMULTILINESTRING ((-74.12620 40.60209, -74.1277...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n\n\n4\n9 STREET\nROSE AVENUE\nfri,sat,sun\nStaten Island\napprovedFull\n448.103939286\nMULTILINESTRING ((-74.11481 40.57316, -74.1157...\n1300.0\nStaten Island\n13400.0\nNew Dorp-Midland Beach\n3.608501e+10\n\n\n\n\n\n\n\n\n\nCode\njoin_test['monday'] = join_test['apprdayswe'].str.count('mon')\njoin_test['tuesday'] = join_test['apprdayswe'].str.count('tue')\njoin_test['wednesday'] = join_test['apprdayswe'].str.count('wed')\njoin_test['thursday'] = join_test['apprdayswe'].str.count('thu')\njoin_test['friday'] = join_test['apprdayswe'].str.count('fri')\njoin_test['saturday'] = join_test['apprdayswe'].str.count('sat')\njoin_test['sunday'] = join_test['apprdayswe'].str.count('sun')\njoin_test['dayscount'] = join_test[['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']].sum(axis=1)\n\njoin_test\n\n\n\n\n\n\n\n\n\nappronstre\napprtostre\napprdayswe\nboroughname\nreviewstat\nshape_stle\ngeometry\nindex_right\nBoroName\nCT2020\nNTAName\nGEOID\nmonday\ntuesday\nwednesday\nthursday\nfriday\nsaturday\nsunday\ndayscount\n\n\n\n\n0\nDEISIUS STREET\nSTECHER STREET\nmon,tue,wed,thu,fri\nStaten Island\napprovedFullSchools\n264.932398036\nMULTILINESTRING ((-74.18738 40.53028, -74.1882...\n1318.0\nStaten Island\n17600.0\nAnnadale-Huguenot-Prince's Bay-Woodrow\n3.608502e+10\n1\n1\n1\n1\n1\n0\n0\n5\n\n\n1\nSUFFOLK AVENUE\nHAROLD STREET\nfri,sat,sun\nStaten Island\napprovedFull\n313.087821487\nMULTILINESTRING ((-74.12784 40.60288, -74.1277...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n0\n0\n0\n0\n1\n1\n1\n3\n\n\n2\nSUFFOLK AVENUE\nHAROLD STREET\nfri,sat,sun\nStaten Island\napprovedFull\n142.063500219\nMULTILINESTRING ((-74.12772 40.60202, -74.1276...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n0\n0\n0\n0\n1\n1\n1\n3\n\n\n3\nVERMONT COURT\nSUFFOLK AVENUE\nfri,sat,sun\nStaten Island\napprovedFull\n421.392020366\nMULTILINESTRING ((-74.12620 40.60209, -74.1277...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n0\n0\n0\n0\n1\n1\n1\n3\n\n\n4\n9 STREET\nROSE AVENUE\nfri,sat,sun\nStaten Island\napprovedFull\n448.103939286\nMULTILINESTRING ((-74.11481 40.57316, -74.1157...\n1300.0\nStaten Island\n13400.0\nNew Dorp-Midland Beach\n3.608501e+10\n0\n0\n0\n0\n1\n1\n1\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\nBECK STREET\nAVENUE ST JOHN\nwed\nBronx\napprovedFull\n619.314479576\nMULTILINESTRING ((-73.90222 40.81449, -73.9014...\n200.0\nBronx\n8300.0\nLongwood\n3.600501e+10\n0\n0\n1\n0\n0\n0\n0\n1\n\n\n362\nNEWKIRK AVENUE\nEAST 17 STREET\nsun\nBrooklyn\napprovedFull\n284.786787549\nMULTILINESTRING ((-73.96422 40.63510, -73.9641...\n1807.0\nBrooklyn\n52000.0\nFlatbush (West)-Ditmas Park-Parkville\n3.604705e+10\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n363\n18 STREET\n4 AVENUE\nmon,tue,wed,thu,fri\nBrooklyn\napprovedFullSchools\n768.660222317\nMULTILINESTRING ((-73.99201 40.66323, -73.9916...\n2248.0\nBrooklyn\n14300.0\nSunset Park (West)\n3.604701e+10\n1\n1\n1\n1\n1\n0\n0\n5\n\n\n364\n34 AVENUE\nJUNCTION BOULEVARD\nmon,tue,wed,thu,fri,sat,sun\nQueens\napprovedLimited\n272.995599601\nMULTILINESTRING ((-73.89716 40.75246, -73.8970...\n2168.0\nQueens\n29100.0\nJackson Heights\n3.608103e+10\n1\n1\n1\n1\n1\n1\n1\n7\n\n\n365\nDECATUR STREET\nSARATOGA AVENUE\nsat\nBrooklyn\napprovedLimited\n769.964464111\nMULTILINESTRING ((-73.92001 40.68299, -73.9176...\n1682.0\nBrooklyn\n37700.0\nBedford-Stuyvesant (East)\n3.604704e+10\n0\n0\n0\n0\n0\n1\n0\n1\n\n\n\n\n366 rows × 20 columns\n\n\n\n\n\nCode\nopen_streets_days = join_test[['appronstre', 'BoroName', 'monday', 'tuesday', 'wednesday','thursday', 'friday', 'saturday', 'sunday']]\nopen_streets_days\n\n\n\n\n\n\n\n\n\nappronstre\nBoroName\nmonday\ntuesday\nwednesday\nthursday\nfriday\nsaturday\nsunday\n\n\n\n\n0\nDEISIUS STREET\nStaten Island\n1\n1\n1\n1\n1\n0\n0\n\n\n1\nSUFFOLK AVENUE\nStaten Island\n0\n0\n0\n0\n1\n1\n1\n\n\n2\nSUFFOLK AVENUE\nStaten Island\n0\n0\n0\n0\n1\n1\n1\n\n\n3\nVERMONT COURT\nStaten Island\n0\n0\n0\n0\n1\n1\n1\n\n\n4\n9 STREET\nStaten Island\n0\n0\n0\n0\n1\n1\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\nBECK STREET\nBronx\n0\n0\n1\n0\n0\n0\n0\n\n\n362\nNEWKIRK AVENUE\nBrooklyn\n0\n0\n0\n0\n0\n0\n1\n\n\n363\n18 STREET\nBrooklyn\n1\n1\n1\n1\n1\n0\n0\n\n\n364\n34 AVENUE\nQueens\n1\n1\n1\n1\n1\n1\n1\n\n\n365\nDECATUR STREET\nBrooklyn\n0\n0\n0\n0\n0\n1\n0\n\n\n\n\n366 rows × 9 columns\n\n\n\n\n\nCensus Data\n\n\nCode\nPop_2020 = gpd.read_file(\"data/NYC_tracts_2020.geojson\")\nPop_2020 = Pop_2020[['GEOID', 'estimate']]\nPop_2020['GEOID']=Pop_2020['GEOID'].astype(int)\n\ntracts_pop = tracts_clean.merge(Pop_2020, on='GEOID', how='left') \nneighbor_pop = tracts_pop.groupby(['NTAName']).sum(['estimate']).reset_index()\n\n\n\n\nCode\nincome = gpd.read_file(\"data/NYC_Income.geojson\")\ngeo_income = gpd.GeoDataFrame(income, geometry='geometry')\ngeo_income = geo_income.set_crs(epsg=4326)\ngeo_income['GEOID']=geo_income['GEOID'].astype('int')\n\ntracts_income = tracts_clean.merge(geo_income.drop(columns='geometry'), on='GEOID', how='left').dropna()\nneighbor_income = tracts_income.groupby(['NTAName', 'BoroName']).median(['estimate']).reset_index()"
  },
  {
    "objectID": "analysis/assignment-2.html#chart-i-matplotlib-parks-in-neighborhoods",
    "href": "analysis/assignment-2.html#chart-i-matplotlib-parks-in-neighborhoods",
    "title": "2 - Where to spend leisure time in NYC?",
    "section": "Chart I: Matplotlib – Parks in Neighborhoods",
    "text": "Chart I: Matplotlib – Parks in Neighborhoods\nI first hope to investigate into the distribution of Parks in different neighborhoods and boroughs in relation to population. On the one hand, higher population means more people will have need for a bigger public green space for leisure time. On the other hand, less populated neighborhoods tend to have more spaces for parks. And since the count of parks doesn’t perfectly reflect how much space is available, I am using acrage data instead of counts for this analysis.\nTo investigate, I utilized Matplotlib, which is great for making simple scatterplot charts that speaks for simple linear relationship, if there is any.\n\n\nCode\ngeo_parks_clean['acres'] = geo_parks_clean['acres'].astype(float)\nparks_acres_neighborhood = geo_parks_clean.groupby('NTAName').sum().drop(columns=['index_right','CT2020']).reset_index()\nparks_acres_pop = neighbor_pop.merge(parks_acres_neighborhood, on='NTAName', how='left').dropna()\nparks_pop = parks_acres_pop.merge(Boro_NTA, on='NTAName', how='left').dropna()\nparks_pop['acres'].describe()\n\n\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1268464147.py:2: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  parks_acres_neighborhood = geo_parks_clean.groupby('NTAName').sum().drop(columns=['index_right','CT2020']).reset_index()\n\n\ncount     237.000000\nmean      105.945812\nstd       224.643587\nmin         0.005000\n25%         7.651000\n50%        20.653000\n75%        85.527000\nmax      1930.636136\nName: acres, dtype: float64\n\n\nAfter a quick glance of the park data, I notice some extreme outliers with extremely large parks that not only serves adjacent neighborhoods but the whole city. I excluded those outliers to have a better sense of how much acrage of parks common neighborhoods get.\n\n\nCode\nparks_pop_filtered = parks_pop.loc[(parks_pop['acres'] &lt; 86) & (parks_pop['acres'] &gt; 7) & (parks_pop['estimate'] &gt;0)]\nparks_pop_filtered\n\n\n\n\n\n\n\n\n\nNTAName\nCT2020\nGEOID_x\nestimate\nacres\nGEOID_y\nBoroName\n\n\n\n\n5\nAstoria (East)-Woodside (North)\n241600\n505134241600\n34825.0\n8.642\n2.886482e+11\nQueens\n\n\n6\nAstoria (North)-Ditmars-Steinway\n214102\n613377214102\n47134.0\n11.530\n4.690532e+11\nQueens\n\n\n10\nBarren Island-Floyd Bennett Field\n70202\n36047070202\n26.0\n64.665\n3.604707e+10\nBrooklyn\n\n\n11\nBath Beach\n235800\n396517235800\n32716.0\n21.398\n1.081411e+11\nBrooklyn\n\n\n15\nBedford Park\n448115\n396055448115\n55702.0\n23.917\n1.800252e+11\nBronx\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n225\nWest Farms\n136300\n180025136300\n18206.0\n13.540\n3.960554e+11\nBronx\n\n\n230\nWhitestone-Beechhurst\n703300\n252567703300\n28353.0\n29.086\n2.164866e+11\nQueens\n\n\n232\nWilliamsburg\n695100\n468611695100\n59410.0\n81.282\n1.045365e+12\nBrooklyn\n\n\n233\nWindsor Terrace-South Slope\n363808\n288376363808\n25442.0\n8.408\n4.325650e+11\nBrooklyn\n\n\n235\nWoodside\n406004\n505134406004\n45417.0\n10.217\n5.412154e+11\nQueens\n\n\n\n\n113 rows × 7 columns\n\n\n\n\n\nCode\n\ncolor_map = {\"Bronx\": \"#550527\", \"Brooklyn\": \"#688E26\", \"Manhattan\": \"#FAA613\", \"Queens\": \"#F44708\", \"Staten Island\": \"#A10702\"}\n\nfig, ax = plt.subplots(figsize=(11,6))\n\nfor BoroName, group_df in parks_pop_filtered.groupby(\"BoroName\"):\n    \n    ax.scatter(\n        group_df[\"estimate\"],\n        group_df[\"acres\"],\n        marker=\"P\",\n        label=BoroName,\n        color=color_map[BoroName],\n        alpha=0.75,\n        zorder=10\n    )\n\nax.legend(loc=\"best\")\nax.set(\n    title = \"Park Space (acres) Relative to Population in Neighborhoods, by Boroughs\",\n    xlabel = \"Population in each neighborhood (2020 Census)\",\n    ylabel = \"Parks in Acres\")\nax.grid(True)\n\nplt.show()\n\n\n\n\n\nThis chart does not suggest a strong linear relationship between population and park acrages. Overall, Manhattan, Queens and Brooklyn host more large parks, but some neighborhoods are particularly underserved, with very high population and low park acrage (points towards lower right of the chart)."
  },
  {
    "objectID": "analysis/assignment-2.html#chart-ii-seaborn-x2",
    "href": "analysis/assignment-2.html#chart-ii-seaborn-x2",
    "title": "2 - Where to spend leisure time in NYC?",
    "section": "Chart II: Seaborn (x2)",
    "text": "Chart II: Seaborn (x2)\nLooking at data for indoor leisure space and open street data, I utilize seaborn to create two types of charts that suite the nature of the data sets.\nThe first is a grouped bar charts. Similar to park distribution, I hope to have a quick glimpse of number of each type of leisure spaces in each borough, and hope to identify any general spatial patterns or inequalities. A grouped chart is great a revealing such pattern.\nThe second is a heatmap for open street data. The heatmap explores the number of open streets approved in different boroughs on different days of the week.\n\nIndoor Leisure Spaces: Grouped Bar Charts\n\n\nCode\nindoor_clean = geo_total_indoor.groupby(['Type','BoroName']).count().reset_index().drop(columns=['Zip','Address','geometry','index_right','CT2020','NTAName']).pivot(index='Type',columns='BoroName', values='Name').fillna(0).reset_index()\nindoor_melt = indoor_clean.melt(id_vars='Type', value_vars=['Bronx','Brooklyn','Manhattan', 'Queens', 'Staten Island'])\nindoor_melt\n\n\n\n\n\n\n\n\n\nType\nBoroName\nvalue\n\n\n\n\n0\nArt Galleries\nBronx\n6.0\n\n\n1\nLibraries\nBronx\n35.0\n\n\n2\nMuseums\nBronx\n8.0\n\n\n3\nTheatres\nBronx\n0.0\n\n\n4\nArt Galleries\nBrooklyn\n61.0\n\n\n5\nLibraries\nBrooklyn\n59.0\n\n\n6\nMuseums\nBrooklyn\n12.0\n\n\n7\nTheatres\nBrooklyn\n0.0\n\n\n8\nArt Galleries\nManhattan\n823.0\n\n\n9\nLibraries\nManhattan\n44.0\n\n\n10\nMuseums\nManhattan\n87.0\n\n\n11\nTheatres\nManhattan\n115.0\n\n\n12\nArt Galleries\nQueens\n24.0\n\n\n13\nLibraries\nQueens\n65.0\n\n\n14\nMuseums\nQueens\n12.0\n\n\n15\nTheatres\nQueens\n2.0\n\n\n16\nArt Galleries\nStaten Island\n3.0\n\n\n17\nLibraries\nStaten Island\n13.0\n\n\n18\nMuseums\nStaten Island\n9.0\n\n\n19\nTheatres\nStaten Island\n0.0\n\n\n\n\n\n\n\n\n\nCode\nsns.set_theme(style=\"whitegrid\")\n\ncolor_map = [\"#550527\", \"#688E26\", \"#FAA613\", \"#F44708\", \"#A10702\"]\nsns.set_palette(color_map)\n\nsns.catplot(\n    data=indoor_melt, kind=\"bar\",\n    x=\"Type\", \n    y=\"value\", \n    hue=\"BoroName\",\n    aspect=2, \n    alpha=1\n).set_axis_labels(\n    \"Type of Leisure Space\", \"Counts\"\n).set(title=\"Distribution of 4 Types of Leisure Spaces in Each Borough\")\n\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\nDays and Location of Open Streets in NYC: A Heatmap\n\n\nCode\n#heatmap for open_streets: borough x Days of the week \nopen_streets_days_melt= open_streets_days.melt(id_vars=['appronstre','BoroName'], value_vars=['monday','tuesday','wednesday', 'thursday', 'friday','saturday','sunday'])\n\n\n\n\n\n\n\n\n\nappronstre\nBoroName\nvariable\nvalue\n\n\n\n\n0\nDEISIUS STREET\nStaten Island\nmonday\n1\n\n\n1\nSUFFOLK AVENUE\nStaten Island\nmonday\n0\n\n\n2\nSUFFOLK AVENUE\nStaten Island\nmonday\n0\n\n\n3\nVERMONT COURT\nStaten Island\nmonday\n0\n\n\n4\n9 STREET\nStaten Island\nmonday\n0\n\n\n...\n...\n...\n...\n...\n\n\n2557\nBECK STREET\nBronx\nsunday\n0\n\n\n2558\nNEWKIRK AVENUE\nBrooklyn\nsunday\n1\n\n\n2559\n18 STREET\nBrooklyn\nsunday\n0\n\n\n2560\n34 AVENUE\nQueens\nsunday\n1\n\n\n2561\nDECATUR STREET\nBrooklyn\nsunday\n0\n\n\n\n\n2562 rows × 4 columns\n\n\n\n\n\nCode\nopen_streets_seaborn = open_streets_days_melt.groupby(['variable','BoroName']).sum().reset_index()\n\n# sort the order of day from monday to sunday \n\norder=['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\nopen_streets_seaborn['variable'] = pd.Categorical(open_streets_seaborn['variable'], categories=order, ordered=True)\nopen_streets_seaborn = open_streets_seaborn.sort_values(by='variable')\n\n\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/3045256453.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  open_streets_seaborn = open_streets_days_melt.groupby(['variable','BoroName']).sum().reset_index()\n\n\n\n\nCode\nimport seaborn as sns\n\nsns.set_theme()\n\n# Load the example flights dataset and convert to long-form\n\nopen_street_heatmap = (\n    open_streets_seaborn\n    .pivot(index=\"BoroName\", columns=\"variable\", values=\"value\")\n)\n\n# Draw a heatmap with the numeric values in each cell\nf, ax = plt.subplots(figsize=(9, 6))\n\nax = sns.heatmap(open_street_heatmap, annot=True, linewidths=.5)\nax.set(xlabel=\"Days in a week\", ylabel=\"Borough\", title=\"Number of Streets Open on Specific Days\")\n\n\n\n[Text(0.5, 33.249999999999986, 'Days in a week'),\n Text(79.75, 0.5, 'Borough'),\n Text(0.5, 1.0, 'Number of Streets Open on Specific Days')]\n\n\n\n\n\nFrom both charts, we can see Manhattan has disproportional number of art galleries and open streets in comparison to other boroughs. Museums and theatres are predominantly located on Manhattan as well. Interestingly, the distribution of libraries seems more even. However, overall, we are seeing leisure spaces are disproportional abundant and diverse in Manhattan, then brooklyn and queens, leaving Bronx and Staten Island less resourceful."
  },
  {
    "objectID": "analysis/assignment-2.html#chart-iii-altair-charts-x3",
    "href": "analysis/assignment-2.html#chart-iii-altair-charts-x3",
    "title": "2 - Where to spend leisure time in NYC?",
    "section": "Chart III: Altair Charts (x3)",
    "text": "Chart III: Altair Charts (x3)\nFurthering the exploration on indoor leisure spaces, I created one chart on its ditribution in relation with median household income, to see if it embeds any socio-economic inequality. I then creatd a map to visualize this relationship, which can also be helpful to locate different kinds of leisure spaces in the city.\nMy third chart is an interactive bar chart on open streets in Brookylen, where audience can choose the day of the week to see all open streets approved for that day and the time they open and close. This could potentially be developed into a tool for residents and tourists to track open streets.\n\nBrush Selection: Indoor Space and Income\n\n\nCode\nneighbor_indoor = geo_total_indoor.groupby(['NTAName','Type']).count().reset_index().drop(['Zip', 'Address', 'geometry','index_right','BoroName', 'CT2020'], axis=1)\nneighbor_indoor = neighbor_indoor.rename(columns={'Name': 'Count'})\nneighbor_income_indoor = neighbor_income.merge(neighbor_indoor, on='NTAName', how='left')\n\nneighbor_income_indoor_pivot=neighbor_income_indoor.pivot(index=['NTAName', 'BoroName','estimate'], columns=\"Type\", values=\"Count\").reset_index().fillna(0)\nneighbor_income_indoor_pivot_1 = neighbor_income_indoor_pivot.drop(neighbor_income_indoor_pivot.columns[[3]],axis=1)\nincome_indoor_pivot = neighbor_income_indoor_pivot_1.melt(id_vars=[\"NTAName\", \"estimate\", \"BoroName\"], value_vars=[\"Art Galleries\", \"Libraries\", \"Museums\", \"Theatres\"], var_name=\"Types\",value_name=\"Count\")\nincome_indoor_pivot_filtered = income_indoor_pivot.loc[(income_indoor_pivot['Count'] &lt;20)]\nincome_indoor_pivot_filtered = income_indoor_pivot_filtered.rename(columns={'estimate': 'Median Household Income'})\n\n\n\n\nCode\nbrush = alt.selection_interval()\n\nBrush_Chart = (\nalt.Chart(income_indoor_pivot_filtered)\n   .mark_point()\n   .encode(\n       x=alt.X(\"Median Household Income:Q\", scale=alt.Scale(zero=False)),\n       y=alt.Y(\"Count:Q\", scale=alt.Scale(zero=False)),\n       color=alt.condition(brush, \"BoroName:N\", alt.value(\"lightgray\")),\n       tooltip=[\"NTAName\",\"BoroName:N\", \"Median Household Income:Q\", \"Count:Q\"])\n   .add_params(brush)\n   .properties(width=200, height=200)\n   .facet(column=\"Types:N\")\n)\n\nBrush_Chart\n\n\n\n\n\n\n\n\n\n\nMap: Income and Indoor Leisure Spaces - Relationships\n\n\nCode\nNTA = pd.read_csv(\"data/2020 Neighborhood.csv\")\nNTA['geometry'] = gpd.GeoSeries.from_wkt(NTA['the_geom'])\nNTA_geo = gpd.GeoDataFrame(NTA, geometry='geometry', crs=4326)\nNTA_geo = NTA_geo.to_crs(epsg=2263)\ngeo_total_indoor = geo_total_indoor.to_crs(epsg=2263)\n\ngeo_NTA = NTA_geo[['NTAName', 'geometry']]\n\ntracts_income_1 = tracts_clean.merge(geo_income.drop(columns='geometry'), on='GEOID', how='left').dropna()\nneighbor_income_1 = tracts_income.groupby(['NTAName', 'BoroName']).median(['estimate']).reset_index()\n\nNTA_income = geo_NTA.merge(neighbor_income_1, on='NTAName', how='left')\n\n\n\n\nCode\ngeo_total_indoor_1 = geo_total_indoor\n\ngeo_total_indoor_1['lon'] = geo_total_indoor_1['geometry'].x\ngeo_total_indoor_1['lat'] = geo_total_indoor_1['geometry'].y\n\n\n\n\nCode\nIncome = (\n    alt.Chart(NTA_income)\n    .mark_geoshape(stroke=\"white\")\n    .encode(\n        tooltip=[\"NTAName:N\", \"estimate:Q\", \"moe:Q\"],\n        color=alt.Color(\"estimate:Q\", scale=alt.Scale(scheme=\"greys\")),\n    )\n    # Important! Otherwise altair will try to re-project your data\n    .project(type=\"identity\", reflectY=True)\n    .properties(width=1000, height=800).interactive()\n)\n\nIndoorSpaces = (\n    alt.Chart(geo_total_indoor_1)\n    .mark_circle(size=10)\n    .encode(tooltip=['Name','Type','Address'],\n           longitude=\"lon\", latitude=\"lat\",\n           color=alt.Color('Type:N', scale=alt.Scale(scheme=\"lightmulti\"))\n         ).project(type=\"identity\", reflectY=True)\n)\n\n\n\nmap_1 = Income + IndoorSpaces\nmap_1\n\n\n\n\n\n\n\n\n\nSimilarly, libraries seem to be the least discriminatory type of leisure space. For art galleries and museums, while there are some distributed in mid to lower income neighborhood, the higher income neighborhood has higher density of such leisure space. Theatres, at the same time, is mostly located in mid to higher income neighborhood. This trend can be clearly see on the map too. Such spatial distribution means, for residents in lower to mid income neighborhood, they might have to travel further for accessing those spaces.\n\n\nOpen Street Time\n\n\nCode\nopen_csv = pd.read_csv(\"data/Open Streets CSV.csv\")\nopen_csv['geometry'] = gpd.GeoSeries.from_wkt(open_csv['the Geom'])\nopen_csv_geo = gpd.GeoDataFrame(open_csv, geometry='geometry', crs=2263)\nopen_csv_geo = open_csv_geo.to_crs(epsg=4326)\n\nopen_7days_time = open_csv_geo.drop(['apprDaysWe','Object ID', 'Organization Name', 'Approved From Street', 'Approved To Street', 'apprStartD', 'apprEndDat', 'Shape_STLe', 'segmentidt', 'segmentidf', 'lionversion', 'the Geom'], axis=1)\nopen_7days_time = open_7days_time.drop_duplicates(subset = \"Approved On Street\")\nopen_7days_time_melt = open_7days_time.melt(id_vars=['Approved On Street', 'Borough Name'], value_vars=['Approved Monday Open', 'Approved Monday Close', 'Approved Tuesday Open', 'Approved Tuesday Close', 'Approved Wednesday Open', 'Approved Wednesday Close', 'Approved Thursday Open', 'Approved Thursday Close', 'Approved Friday Open', 'Approved Friday Close', 'Approved Saturday Open', 'Approved Saturday Close', 'Approved Sunday Open', 'Approved Sunday Close']).dropna()\n#open_7days_time_melt['value'] =  pd.to_datetime(open_7days_time_melt['value']).dt.time\nopen_time_Brooklyn = open_7days_time_melt[(open_7days_time_melt['Borough Name'] == 'Brooklyn')]\nopen_time_Brooklyn\n\n\n\n\n\n\n\n\n\nApproved On Street\nBorough Name\nvariable\nvalue\n\n\n\n\n6\nRIDGE BOULEVARD\nBrooklyn\nApproved Monday Open\n10:00\n\n\n9\n82 STREET\nBrooklyn\nApproved Monday Open\n08:30\n\n\n10\n48 STREET\nBrooklyn\nApproved Monday Open\n13:30\n\n\n11\n43 STREET\nBrooklyn\nApproved Monday Open\n09:30\n\n\n12\nALBEMARLE ROAD\nBrooklyn\nApproved Monday Open\n08:00\n\n\n...\n...\n...\n...\n...\n\n\n2282\nJEFFERSON AVENUE\nBrooklyn\nApproved Sunday Close\n21:00\n\n\n2287\nSHARON STREET\nBrooklyn\nApproved Sunday Close\n20:00\n\n\n2288\nTROUTMAN STREET\nBrooklyn\nApproved Sunday Close\n22:00\n\n\n2289\nRANDOLPH STREET\nBrooklyn\nApproved Sunday Close\n23:00\n\n\n2354\nLEXINGTON AVENUE\nBrooklyn\nApproved Sunday Close\n20:00\n\n\n\n\n378 rows × 4 columns\n\n\n\n\n\nCode\nopen_time_Brooklyn['time'] = open_7days_time_melt['variable'].str.extract('(Open|Close)')\nopen_time_Brooklyn['dayweek'] = open_7days_time_melt['variable'].str.extract('(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)')\nopen_time_Brooklyn= open_time_Brooklyn.pivot(index=['Approved On Street', 'dayweek'], columns=\"time\", values=\"value\").reset_index()\n\norder=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nopen_time_Brooklyn['dayweek'] = pd.Categorical(open_time_Brooklyn['dayweek'], categories=order, ordered=True)\nopen_time_Brooklyn = open_time_Brooklyn.sort_values(by='dayweek').reset_index()\n\nopen_time_Brooklyn\n\n\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1900940832.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  open_time_Brooklyn['time'] = open_7days_time_melt['variable'].str.extract('(Open|Close)')\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1900940832.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  open_time_Brooklyn['dayweek'] = open_7days_time_melt['variable'].str.extract('(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)')\n\n\n\n\n\n\n\n\ntime\nindex\nApproved On Street\ndayweek\nClose\nOpen\n\n\n\n\n0\n139\nSUMMIT STREET\nMonday\n14:30\n12:30\n\n\n1\n151\nUNDERHILL AVENUE\nMonday\n20:00\n08:00\n\n\n2\n28\n82 STREET\nMonday\n15:30\n08:30\n\n\n3\n143\nTHATFORD AVENUE\nMonday\n18:00\n11:00\n\n\n4\n33\nAITKEN PLACE\nMonday\n15:00\n11:00\n\n\n...\n...\n...\n...\n...\n...\n\n\n184\n147\nTOMPKINS AVENUE\nSunday\n20:00\n11:00\n\n\n185\n51\nBEVERLEY ROAD\nSunday\n18:00\n10:00\n\n\n186\n112\nRANDOLPH STREET\nSunday\n23:00\n12:00\n\n\n187\n10\n4 STREET\nSunday\n22:00\n08:00\n\n\n188\n157\nVANDERBILT AVENUE\nSunday\n23:00\n11:00\n\n\n\n\n189 rows × 5 columns\n\n\n\n\n\nCode\nselection = alt.selection_multi(fields=['dayweek'])\ncolor = alt.condition(selection,\n                      alt.Color('dayweek:N', legend=None, \n                      scale=alt.Scale(scheme='category10')),\n                      alt.value('lightgray'))\n\nopacity = alt.condition(selection,\n                        alt.value(1), alt.value(0))\n\n\n\nbar = alt.Chart(open_time_Brooklyn).mark_bar().encode(\n        x='Open',\n        x2='Close',\n        y='Approved On Street',\n        color=color,\n        opacity=opacity,\n        tooltip=['Open', 'Close', 'Approved On Street', 'dayweek']).properties(\n        width=500,\n        height=1000).interactive()\n\nlegend = alt.Chart(open_time_Brooklyn).mark_bar().encode(\n    y=alt.Y('dayweek:N', axis=alt.Axis(orient='right')),\n    color=color\n).add_selection(\nselection\n)\n\nA_Chart = bar | legend\nA_Chart\n\n\n\n\n\n\n\n\n\nThis interactive bar chart on Brooklyn Open Street serves as a pilot that can be adapted for data of all five boroughs. By comparing to open and close time, we can observe that many streets are approved to be open streets with later opening and closing time on weekends. To improve this bar chart the status of each street can be added (i.e., whether they are approved to be closed fully or partially or only on school days), which is important for visitors as well."
  },
  {
    "objectID": "analysis/assignment-2.html#dashboard",
    "href": "analysis/assignment-2.html#dashboard",
    "title": "2 - Where to spend leisure time in NYC?",
    "section": "Dashboard",
    "text": "Dashboard\nThe dashboard below is an upgrade from the indoor space and income grouped scatter plot. This dashboard makes it easier to explore the spatial distribution of indoor spaces of certain type or in neighborhoods with certain level of income. Through selecting points with higher counts, we can see neighborhoods with higher density of identified indoor leisure spaces are mostly located in Manhattan.\n\n\nCode\nbrush = alt.selection_interval()\n\npoints = alt.Chart(income_indoor_pivot_filtered).mark_point().encode(\n       x=alt.X(\"Median Household Income:Q\", scale=alt.Scale(zero=False)),\n       y=alt.Y(\"Count:Q\", scale=alt.Scale(zero=False)),\n       color=alt.condition(brush, \"BoroName:N\", alt.value(\"lightgray\")),\n       tooltip=[\"NTAName\",\"BoroName:N\", \"Median Household Income:Q\", \"Count:Q\"]).add_params(brush\n).properties(width=200, height=200\n).facet(column=\"Types:N\")\n\n\nbars = alt.Chart(income_indoor_pivot_filtered).mark_bar().encode(\n    y='BoroName:N',\n    color='BoroName:N',\n    x='Count:Q'\n).transform_filter(\n    brush\n)\n\nDashboard = points & bars\nDashboard"
  },
  {
    "objectID": "analysis/assignment-4.html",
    "href": "analysis/assignment-4.html",
    "title": "4 - Street Networks & Web Scraping",
    "section": "",
    "text": "Part 1: Visualizing crash data in Philadelphia\nIn this section, you will use osmnx to analyze the crash incidence in Center City.\nPart 2: Scraping Craigslist\nIn this section, you will use Selenium and BeautifulSoup to scrape data for hundreds of apartments from Philadelphia’s Craigslist portal.\nCode\nimport pandas as pd\nimport geopandas as gpd\nimport altair as alt\nimport numpy as np\nfrom matplotlib import pyplot as plt\nnp.seterr(invalid=\"ignore\");\nfrom shapely import geometry"
  },
  {
    "objectID": "analysis/assignment-4.html#part-1-visualizing-crash-data-in-philadelphia",
    "href": "analysis/assignment-4.html#part-1-visualizing-crash-data-in-philadelphia",
    "title": "4 - Street Networks & Web Scraping",
    "section": "Part 1: Visualizing crash data in Philadelphia",
    "text": "Part 1: Visualizing crash data in Philadelphia\n\n1.1 Load the geometry for the region being analyzed\nWe’ll analyze crashes in the “Central” planning district in Philadelphia, a rough approximation for Center City. Planning districts can be loaded from Open Data Philly. Read the data into a GeoDataFrame using the following link:\nhttp://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson\nSelect the “Central” district and extract the geometry polygon for only this district. After this part, you should have a polygon variable of type shapely.geometry.polygon.Polygon.\n\n\nCode\nurl = \"http://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson\"\nphilly_districts = gpd.read_file(url).to_crs('EPSG:4326')\ncentral = philly_districts.loc[philly_districts['DIST_NAME'] == 'Central']\n\npolygon = central['geometry'].iloc[0]\nprint(type(polygon))\n\n\n&lt;class 'shapely.geometry.polygon.Polygon'&gt;\n\n\n\n\n1.2 Get the street network graph\nUse OSMnx to create a network graph (of type ‘drive’) from your polygon boundary in 1.1.\n\n\nCode\nimport osmnx as ox\ngraph = ox.graph_from_polygon(polygon, network_type='drive')\nphilly_graph = ox.project_graph(graph)\nox.plot_graph(philly_graph)\n\n\n\n\n\n(&lt;Figure size 800x800 with 1 Axes&gt;, &lt;Axes: &gt;)\n\n\n\n\n1.3 Convert your network graph edges to a GeoDataFrame\nUse OSMnx to create a GeoDataFrame of the network edges in the graph object from part 1.2. The GeoDataFrame should contain the edges but not the nodes from the network.\n\n\nCode\nedges = ox.graph_to_gdfs(philly_graph, edges=True, nodes=False).to_crs('EPSG:4326')\nedges.head()\n\n\n\n\n\n\n\n\n\n\n\nosmid\noneway\nname\nhighway\nreversed\nlength\ngeometry\nmaxspeed\nlanes\ntunnel\nbridge\nref\nwidth\nservice\naccess\njunction\n\n\nu\nv\nkey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n109727439\n109911666\n0\n132508434\nTrue\nBainbridge Street\nresidential\nFalse\n44.137\nLINESTRING (-75.17104 39.94345, -75.17053 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n109911666\n109911655\n0\n132508434\nTrue\nBainbridge Street\nresidential\nFalse\n45.459\nLINESTRING (-75.17053 39.94339, -75.17001 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n110024009\n0\n12139665\nTrue\nSouth 17th Street\ntertiary\nFalse\n109.692\nLINESTRING (-75.17053 39.94339, -75.17060 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n109727448\n109727439\n0\n12109011\nTrue\nSouth Colorado Street\nresidential\nFalse\n109.484\nLINESTRING (-75.17125 39.94248, -75.17120 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n110034229\n0\n12159387\nTrue\nFitzwater Street\nresidential\nFalse\n91.353\nLINESTRING (-75.17125 39.94248, -75.17137 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nCode\nedges.explore()\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n1.4 Load PennDOT crash data\nData for crashes (of all types) for 2020, 2021, and 2022 in Philadelphia County is available at the following path:\n./data/CRASH_PHILADELPHIA_XXXX.csv\nYou should see three separate files in the data/ folder. Use pandas to read each of the CSV files, and combine them into a single dataframe using pd.concat().\nThe data was downloaded for Philadelphia County from here.\n\n\nCode\ncrash_2020 = pd.read_csv(\"./data/CRASH_PHILADELPHIA_2020.csv\")\ncrash_2021 = pd.read_csv(\"./data/CRASH_PHILADELPHIA_2021.csv\")\ncrash_2022 = pd.read_csv(\"./data/CRASH_PHILADELPHIA_2022.csv\")\n\n\n\n\nCode\nphilly_crash = pd.concat([crash_2020, crash_2021, crash_2020])\nphilly_crash\n\n\n\n\n\n\n\n\n\nCRN\nARRIVAL_TM\nAUTOMOBILE_COUNT\nBELTED_DEATH_COUNT\nBELTED_SUSP_SERIOUS_INJ_COUNT\nBICYCLE_COUNT\nBICYCLE_DEATH_COUNT\nBICYCLE_SUSP_SERIOUS_INJ_COUNT\nBUS_COUNT\nCHLDPAS_DEATH_COUNT\n...\nWORK_ZONE_TYPE\nWORKERS_PRES\nWZ_CLOSE_DETOUR\nWZ_FLAGGER\nWZ_LAW_OFFCR_IND\nWZ_LN_CLOSURE\nWZ_MOVING\nWZ_OTHER\nWZ_SHLDER_MDN\nWZ_WORKERS_INJ_KILLED\n\n\n\n\n0\n2020036588\n1349.0\n1\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n2020036617\n1842.0\n1\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n2020035717\n2000.0\n1\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n2020034378\n1139.0\n2\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n2020025511\n345.0\n1\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10160\n2020107585\n2159.0\n1\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n10161\n2020109081\n1945.0\n2\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n10162\n2020108478\n537.0\n1\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n10163\n2020108480\n2036.0\n2\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n10164\n2020110361\n1454.0\n0\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n30877 rows × 100 columns\n\n\n\n\n\n1.5 Convert the crash data to a GeoDataFrame\nYou will need to use the DEC_LAT and DEC_LONG columns for latitude and longitude.\nThe full data dictionary for the data is available here\n\n\nCode\ncrash_geo = gpd.GeoDataFrame(philly_crash, geometry=gpd.points_from_xy(philly_crash.DEC_LONG, philly_crash.DEC_LAT), crs=\"EPSG:4326\")\n\n\n\n\n1.6 Trim the crash data to Center City\n\nGet the boundary of the edges data frame (from part 1.3). Accessing the .geometry.unary_union.convex_hull property will give you a nice outer boundary region.\nTrim the crashes using the within() function of the crash GeoDataFrame to find which crashes are within the boundary.\n\nThere should be about 3,750 crashes within the Central district.\n\n\nCode\nboundary = edges.geometry.unary_union.convex_hull\n\ncentral_crash = crash_geo[crash_geo.within(boundary)]\ncentral_crash\n\n\n\n\n\n\n\n\n\nCRN\nARRIVAL_TM\nAUTOMOBILE_COUNT\nBELTED_DEATH_COUNT\nBELTED_SUSP_SERIOUS_INJ_COUNT\nBICYCLE_COUNT\nBICYCLE_DEATH_COUNT\nBICYCLE_SUSP_SERIOUS_INJ_COUNT\nBUS_COUNT\nCHLDPAS_DEATH_COUNT\n...\nWORKERS_PRES\nWZ_CLOSE_DETOUR\nWZ_FLAGGER\nWZ_LAW_OFFCR_IND\nWZ_LN_CLOSURE\nWZ_MOVING\nWZ_OTHER\nWZ_SHLDER_MDN\nWZ_WORKERS_INJ_KILLED\ngeometry\n\n\n\n\n0\n2020036588\n1349.0\n1\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.17940 39.96010)\n\n\n7\n2020035021\n1255.0\n1\n0\n0\n0\n0\n0\n1\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.16310 39.97000)\n\n\n11\n2020021944\n805.0\n2\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.18780 39.95230)\n\n\n12\n2020024963\n1024.0\n1\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.14840 39.95580)\n\n\n18\n2020000481\n1737.0\n1\n0\n0\n0\n0\n0\n1\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.15470 39.95340)\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10132\n2020105944\n1841.0\n0\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.14090 39.95380)\n\n\n10133\n2020112963\n2052.0\n2\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.17520 39.95310)\n\n\n10148\n2020111219\n1324.0\n0\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.16110 39.96370)\n\n\n10150\n2020110378\n2025.0\n6\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.17770 39.94430)\n\n\n10161\n2020109081\n1945.0\n2\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.18580 39.94150)\n\n\n\n\n3926 rows × 101 columns\n\n\n\n\n\n1.7 Re-project our data into an approriate CRS\nWe’ll need to find the nearest edge (street) in our graph for each crash. To do this, osmnx will calculate the distance from each crash to the graph edges. For this calculation to be accurate, we need to convert from latitude/longitude\nWe’ll convert the local state plane CRS for Philadelphia, EPSG=2272\n\nTwo steps:\n\nProject the graph object (G) using the ox.project_graph. Run ox.project_graph? to see the documentation for how to convert to a specific CRS.\nProject the crash data using the .to_crs() function.\n\n\n\nCode\nphilly_G = ox.project_graph(philly_graph, to_crs='EPSG:2272')\n\n\n\n\nCode\ncentral_crash = central_crash.to_crs('EPSG:2272')\n\n\n\n\n\n1.8 Find the nearest edge for each crash\nSee: ox.distance.nearest_edges(). It takes three arguments:\n\nthe network graph\nthe longitude of your crash data (the x attribute of the geometry column)\nthe latitude of your crash data (the y attribute of the geometry column)\n\nYou will get a numpy array with 3 columns that represent (u, v, key) where each u and v are the node IDs that the edge links together. We will ignore the key value for our analysis.\n\n\nCode\nnearest_crash = ox.distance.nearest_edges(philly_G, central_crash['geometry'].x, central_crash['geometry'].y)\n\n\n\n\n1.9 Calculate the total number of crashes per street\n\nMake a DataFrame from your data from part 1.7 with three columns, u, v, and key (we will only use the u and v columns)\nGroup by u and v and calculate the size\nReset the index and name your size() column as crash_count\n\nAfter this step you should have a DataFrame with three columns: u, v, and crash_count.\n\n\nCode\nnearest_Crash = pd.DataFrame(nearest_crash, columns=['u', 'v', 'key'])\n\n\n\n\nCode\ncrash_counts = nearest_Crash.groupby(['u', 'v']).count().reset_index()\ncrash_counts = crash_counts.rename(columns={'key': 'crash_count'})\ncrash_counts\n\n\n\n\n\n\n\n\n\nu\nv\ncrash_count\n\n\n\n\n0\n109729474\n3425014859\n2\n\n\n1\n109729486\n110342146\n4\n\n\n2\n109729699\n109811674\n10\n\n\n3\n109729709\n109729731\n3\n\n\n4\n109729731\n109729739\n6\n\n\n...\n...\n...\n...\n\n\n729\n10270051289\n5519334546\n1\n\n\n730\n10660521823\n10660521817\n2\n\n\n731\n10674041689\n10674041689\n14\n\n\n732\n11144117753\n109729699\n4\n\n\n733\n11162290432\n110329835\n1\n\n\n\n\n734 rows × 3 columns\n\n\n\n\n\n1.10 Merge your edges GeoDataFrame and crash count DataFrame\nYou can use pandas to merge them on the u and v columns. This will associate the total crash count with each edge in the street network.\nTips: - Use a left merge where the first argument of the merge is the edges GeoDataFrame. This ensures no edges are removed during the merge. - Use the fillna(0) function to fill in missing crash count values with zero.\n\n\nCode\nmerged = pd.merge(edges, crash_counts, on=['u','v'], how='left').fillna(0)\nmerged.head()\n\n\n\n\n\n\n\n\n\nu\nv\nosmid\noneway\nname\nhighway\nreversed\nlength\ngeometry\nmaxspeed\nlanes\ntunnel\nbridge\nref\nwidth\nservice\naccess\njunction\ncrash_count\n\n\n\n\n0\n109727439\n109911666\n132508434\nTrue\nBainbridge Street\nresidential\nFalse\n44.137\nLINESTRING (-75.17104 39.94345, -75.17053 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n1\n109911666\n109911655\n132508434\nTrue\nBainbridge Street\nresidential\nFalse\n45.459\nLINESTRING (-75.17053 39.94339, -75.17001 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n2\n109911666\n110024009\n12139665\nTrue\nSouth 17th Street\ntertiary\nFalse\n109.692\nLINESTRING (-75.17053 39.94339, -75.17060 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n3\n109727448\n109727439\n12109011\nTrue\nSouth Colorado Street\nresidential\nFalse\n109.484\nLINESTRING (-75.17125 39.94248, -75.17120 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n4\n109727448\n110034229\n12159387\nTrue\nFitzwater Street\nresidential\nFalse\n91.353\nLINESTRING (-75.17125 39.94248, -75.17137 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n\n\n\n\n\n\n\n1.11 Calculate a “Crash Index”\nLet’s calculate a “crash index” that provides a normalized measure of the crash frequency per street. To do this, we’ll need to:\n\nCalculate the total crash count divided by the street length, using the length column\nPerform a log transformation of the crash/length variable — use numpy’s log10() function\nNormalize the index from 0 to 1 (see the lecture notes for an example of this transformation)\n\nNote: since the crash index involves a log transformation, you should only calculate the index for streets where the crash count is greater than zero.\nAfter this step, you should have a new column in the data frame from 1.9 that includes a column called part 1.9.\n\n\nCode\nmerge_dropna = merged[merged['crash_count'] != 0]\n\n\n\n\nCode\ncalculation = np.log10(merge_dropna['crash_count']/merge_dropna['length'])\n#(np.log10(merged['crash_count']/merged['length']) - np.log10(merged['crash_count']/merged['length']).min())/ (np.log10(merged['crash_count']/merged['length']).max() - np.log10(merged['crash_count']/merged['length']).min())\nmerge_dropna['crash index'] = (calculation - calculation.min())/(calculation.max() - calculation.min())\nmerge_dropna.head(5)\n\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n\n\n\n\n\n\n\n\n\nu\nv\nosmid\noneway\nname\nhighway\nreversed\nlength\ngeometry\nmaxspeed\nlanes\ntunnel\nbridge\nref\nwidth\nservice\naccess\njunction\ncrash_count\ncrash index\n\n\n\n\n9\n110024052\n110024066\n12139665\nTrue\nSouth 17th Street\ntertiary\nFalse\n135.106\nLINESTRING (-75.17134 39.93966, -75.17152 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n9.0\n0.431858\n\n\n30\n109729474\n3425014859\n62154356\nTrue\nArch Street\nsecondary\nFalse\n126.087\nLINESTRING (-75.14847 39.95259, -75.14859 39.9...\n25 mph\n2\n0\n0\n0\n0\n0\n0\n0\n2.0\n0.258123\n\n\n31\n109729486\n110342146\n[12169305, 1052694387]\nTrue\nNorth Independence Mall East\nsecondary\nFalse\n123.116\nLINESTRING (-75.14832 39.95333, -75.14813 39.9...\n0\n[3, 2]\n0\n0\n0\n0\n0\n0\n0\n4.0\n0.344930\n\n\n33\n3425014859\n5372059859\n[12197696, 1003976882, 424804083]\nTrue\nNorth Independence Mall West\nsecondary\nFalse\n229.386\nLINESTRING (-75.14993 39.95277, -75.14995 39.9...\n0\n3\n0\n0\n0\n0\n0\n0\n0\n2.0\n0.185670\n\n\n35\n110342146\n315655546\n[424807270, 12109183]\nTrue\n0\nmotorway_link\nFalse\n153.933\nLINESTRING (-75.14809 39.95442, -75.14808 39.9...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n3.0\n0.283054\n\n\n\n\n\n\n\n\n\n1.12 Plot a histogram of the crash index values\nUse matplotlib’s hist() function to plot the crash index values from the previous step.\nYou should see that the index values are Gaussian-distributed, providing justification for why we log-transformed!\n\n\nCode\nplt.hist(merge_dropna['crash index'], color='pink', bins=20)\n\nplt.xlabel('Crash Index')\nplt.ylabel('Frequency')\nplt.title('Crash Index Values Histogram')\nplt.show()\n\n\n\n\n\n\n\n1.13 Plot an interactive map of the street networks, colored by the crash index\nYou can use GeoPandas to make an interactive Folium map, coloring the streets by the crash index column.\nTip: if you use the viridis color map, try using a “dark” tile set for better constrast of the colors.\n\n\nCode\ncrash_index = merge_dropna[['u', 'v', 'crash index']]\nmerged_index = pd.merge(merged, crash_index, on=['u','v'], how='left').fillna(0)\n\n\n\n\nCode\nmerged_index.explore(\n    column=\"crash index\",\n    cmap=\"YlGnBu\",\n    tiles=\"cartodbpositron\",)\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/assignment-4.html#part-2-scraping-craigslist",
    "href": "analysis/assignment-4.html#part-2-scraping-craigslist",
    "title": "4 - Street Networks & Web Scraping",
    "section": "Part 2: Scraping Craigslist",
    "text": "Part 2: Scraping Craigslist\nIn this part, we’ll be extracting information on apartments from Craigslist search results. You’ll be using Selenium and BeautifulSoup to extract the relevant information from the HTML text.\nFor reference on CSS selectors, please see the notes from Week 6.\n\nPrimer: the Craigslist website URL\nWe’ll start with the Philadelphia region. First we need to figure out how to submit a query to Craigslist. As with many websites, one way you can do this is simply by constructing the proper URL and sending it to Craigslist.\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1gallery0~0\nThere are three components to this URL.\n\nThe base URL: http://philadelphia.craigslist.org/search/apa\nThe user’s search parameters: ?min_price=1&min_bedrooms=1&minSqft=1\n\n\nWe will send nonzero defaults for some parameters (bedrooms, size, price) in order to exclude results that have empty values for these parameters.\n\n\nThe URL hash: #search=1~gallery~0~0\n\n\nAs we will see later, this part will be important because it contains the search page result number.\n\nThe Craigslist website requires Javascript, so we’ll need to use Selenium to load the page, and then use BeautifulSoup to extract the information we want.\n\n\n2.1 Initialize a selenium driver and open Craigslist\nAs discussed in lecture, you can use Chrome, Firefox, or Edge as your selenium driver. In this part, you should do two things:\n\nInitialize the selenium driver\nUse the driver.get() function to open the following URL:\n\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1gallery0~0\nThis will give you the search results for 1-bedroom apartments in Philadelphia.\n\n\nCode\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nimport requests\n\n\n\n\nCode\ndriver = webdriver.Chrome()\nurl = \"https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1~gallery~0~0\"\ndriver.get(url)\n\n\n\n\n2.2 Initialize your “soup”\nOnce selenium has the page open, we can get the page source from the driver and use BeautifulSoup to parse it. In this part, initialize a BeautifulSoup object with the driver’s page source\n\n\nCode\nsoup = BeautifulSoup(driver.page_source, \"html.parser\")\n\n\n\n\n2.3 Parsing the HTML\nNow that we have our “soup” object, we can use BeautifulSoup to extract out the elements we need:\n\nUse the Web Inspector to identify the HTML element that holds the information on each apartment listing.\nUse BeautifulSoup to extract these elements from the HTML.\n\nAt the end of this part, you should have a list of 120 elements, where each element is the listing for a specific apartment on the search page.\n\n\nCode\nselector = \".cl-search-result.cl-search-view-mode-gallery\"\n\ntables = soup.select(selector)\n\n\n\n\nCode\nlen(tables)\n\n\n120\n\n\n\n\n2.4 Find the relevant pieces of information\nWe will now focus on the first element in the list of 120 apartments. Use the prettify() function to print out the HTML for this first element.\nFrom this HTML, identify the HTML elements that hold:\n\nThe apartment price\nThe number of bedrooms\nThe square footage\nThe apartment title\n\nFor the first apartment, print out each of these pieces of information, using BeautifulSoup to select the proper elements.\n\n\nCode\nprint(tables[1].prettify())\n\n\n&lt;li class=\"cl-search-result cl-search-view-mode-gallery\" data-pid=\"7681605868\" title=\"Apartment by the arts!\"&gt;\n &lt;div class=\"gallery-card\"&gt;\n  &lt;div class=\"cl-gallery\"&gt;\n   &lt;div class=\"gallery-inner\"&gt;\n    &lt;a class=\"main\" href=\"https://philadelphia.craigslist.org/apa/d/philadelphia-apartment-by-the-arts/7681605868.html\"&gt;\n     &lt;div class=\"swipe\" style=\"visibility: visible;\"&gt;\n      &lt;div class=\"swipe-wrap\" style=\"width: 4032px;\"&gt;\n       &lt;div data-index=\"0\" style=\"width: 336px; left: 0px; transition-duration: 0ms; transform: translateX(0px);\"&gt;\n        &lt;span class=\"loading icom-\"&gt;\n        &lt;/span&gt;\n        &lt;img alt=\"Apartment by the arts! 1\" src=\"https://images.craigslist.org/00j0j_bWLeLp3Yzpi_0x20m2_300x300.jpg\"/&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"1\" style=\"width: 336px; left: -336px; transition-duration: 0ms; transform: translateX(336px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"2\" style=\"width: 336px; left: -672px; transition-duration: 0ms; transform: translateX(336px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"3\" style=\"width: 336px; left: -1008px; transition-duration: 0ms; transform: translateX(336px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"4\" style=\"width: 336px; left: -1344px; transition-duration: 0ms; transform: translateX(336px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"5\" style=\"width: 336px; left: -1680px; transition-duration: 0ms; transform: translateX(-336px);\"&gt;\n       &lt;/div&gt;\n      &lt;/div&gt;\n     &lt;/div&gt;\n     &lt;div class=\"slider-back-arrow icom-\"&gt;\n     &lt;/div&gt;\n     &lt;div class=\"slider-forward-arrow icom-\"&gt;\n     &lt;/div&gt;\n    &lt;/a&gt;\n   &lt;/div&gt;\n   &lt;div class=\"dots\"&gt;\n    &lt;span class=\"dot selected\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n   &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;a class=\"cl-app-anchor text-only posting-title\" href=\"https://philadelphia.craigslist.org/apa/d/philadelphia-apartment-by-the-arts/7681605868.html\" tabindex=\"0\"&gt;\n   &lt;span class=\"label\"&gt;\n    Apartment by the arts!\n   &lt;/span&gt;\n  &lt;/a&gt;\n  &lt;div class=\"meta\"&gt;\n   14 mins ago\n   &lt;span class=\"separator\"&gt;\n    ·\n   &lt;/span&gt;\n   &lt;span class=\"housing-meta\"&gt;\n    &lt;span class=\"post-bedrooms\"&gt;\n     2br\n    &lt;/span&gt;\n    &lt;span class=\"post-sqft\"&gt;\n     1390ft\n     &lt;span class=\"exponent\"&gt;\n      2\n     &lt;/span&gt;\n    &lt;/span&gt;\n   &lt;/span&gt;\n   &lt;span class=\"separator\"&gt;\n    ·\n   &lt;/span&gt;\n   Philadelphia\n  &lt;/div&gt;\n  &lt;span class=\"priceinfo\"&gt;\n   $2,850\n  &lt;/span&gt;\n  &lt;button class=\"bd-button cl-favorite-button icon-only\" tabindex=\"0\" title=\"add to favorites list\" type=\"button\"&gt;\n   &lt;span class=\"icon icom-\"&gt;\n   &lt;/span&gt;\n   &lt;span class=\"label\"&gt;\n   &lt;/span&gt;\n  &lt;/button&gt;\n  &lt;button class=\"bd-button cl-banish-button icon-only\" tabindex=\"0\" title=\"hide posting\" type=\"button\"&gt;\n   &lt;span class=\"icon icom-\"&gt;\n   &lt;/span&gt;\n   &lt;span class=\"label\"&gt;\n    hide\n   &lt;/span&gt;\n  &lt;/button&gt;\n &lt;/div&gt;\n&lt;/li&gt;\n\n\n\n\n\nCode\ntable = tables[1]\n\n\n\n\nCode\ndata = []\n\n#price\nprice = table.select_one(\".priceinfo\").text\n\n#no. of bedrooms\nbedrooms = table.select_one(\".post-bedrooms\").text\n    \n#square footage\nsize = table.select_one(\".post-sqft\").text\n    \n#apt name\nname = table.select_one(\".label\").text\n\ndata.append({\"Price\": price, \"Bedrooms\": bedrooms, \"Size\": size, \"Name\": name})\n\ndata = pd.DataFrame(data)\n\ndata\n\n\n\n\n\n\n\n\n\nPrice\nBedrooms\nSize\nName\n\n\n\n\n0\n$2,850\n2br\n1390ft2\nApartment by the arts!\n\n\n\n\n\n\n\n\n\n2.5 Functions to format the results\nIn this section, you’ll create functions that take in the raw string elements for price, size, and number of bedrooms and returns them formatted as numbers.\nI’ve started the functions to format the values. You should finish theses functions in this section.\nHints - You can use string formatting functions like string.replace() and string.strip() - The int() and float() functions can convert strings to numbers\n\n\nCode\ndef format_bedrooms(bedrooms_string):\n    \n    bedrooms = int(bedrooms_string.replace(\"br\", \"\"))\n    \n    return bedrooms\n\n\n\n\nCode\ndef format_size(size_string):\n    \n    size = int(size_string.replace(\"ft\", \"\"))\n    \n    return size \n\n\n\n\nCode\ndef format_price(price_string):\n    \n    price = int(price_string.replace(\"$\", \"\").replace(\",\" , \"\"))\n    \n    return price\n\n\n\n\n2.6 Putting it all together\nIn this part, you’ll complete the code block below using results from previous parts. The code will loop over 5 pages of search results and scrape data for 600 apartments.\nWe can get a specific page by changing the search=PAGE part of the URL hash. For example, to get page 2 instead of page 1, we will navigate to:\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=2gallery0~0\nIn the code below, the outer for loop will loop over 5 pages of search results. The inner for loop will loop over the 120 apartments listed on each search page.\nFill in the missing pieces of the inner loop using the code from the previous section. We will be able to extract out the relevant pieces of info for each apartment.\nAfter filling in the missing pieces and executing the code cell, you should have a Data Frame called results that holds the data for 600 apartment listings.\n\nNotes\nBe careful if you try to scrape more listings. Craigslist will temporarily ban your IP address (for a very short time) if you scrape too much at once. I’ve added a sleep() function to the for loop to wait 30 seconds between scraping requests.\nIf the for loop gets stuck at the “Processing page X…” step for more than a minute or so, your IP address is probably banned temporarily, and you’ll have to wait a few minutes before trying again.\n\n\nCode\nfrom time import sleep\n\n\n\n\nCode\nresults = []\n\n# search in batches of 120 for 5 pages\n# NOTE: you will get temporarily banned if running more than ~5 pages or so\n# the API limits are more leninient during off-peak times, and you can try\n# experimenting with more pages\nmax_pages = 5\n\n# The base URL we will be using\nbase_url = \"https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1\"\n\n# loop over each page of search results\nfor page_num in range(1, max_pages + 1):\n    print(f\"Processing page {page_num}...\")\n\n    # Update the URL hash for this page number and make the combined URL\n    url_hash = f\"#search={page_num}~gallery~0~0\"\n    url = base_url + url_hash\n\n    # Go to the driver and wait for 5 seconds\n    driver.get(url)\n    sleep(5)\n\n    # YOUR CODE: get the list of all apartments\n    # This is the same code from Part 1.2 and 1.3\n    # It should be a list of 120 apartments\n    soup = soup\n    apts = tables\n    print(\"Number of apartments = \", len(apts))\n\n    # loop over each apartment in the list\n    page_results = []\n    for apt in apts:\n\n        # YOUR CODE: the bedrooms string\n        bedrooms = apt.select_one(\".post-bedrooms\").text\n\n        # YOUR CODE: the size string\n        size = apt.select_one(\".post-sqft\").text\n\n        # YOUR : the title string\n        title = apt.select_one(\".label\").text\n\n        # YOUR CODE: the price string\n        price = apt.select_one(\".priceinfo\").text\n\n\n        # Format using functions from Part 1.5\n        bedrooms = format_bedrooms(bedrooms)\n        size = format_size(size)\n        price = format_price(price)\n\n        # Save the result\n        page_results.append([price, size, bedrooms, title])\n\n    # Create a dataframe and save\n    col_names = [\"price\", \"size\", \"bedrooms\", \"title\"]\n    df = pd.DataFrame(page_results, columns=col_names)\n    results.append(df)\n\n    print(\"sleeping for 10 seconds between calls\")\n    sleep(10)\n\n# Finally, concatenate all the results\nresults = pd.concat(results, axis=0).reset_index(drop=True)\n\n\nProcessing page 1...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 2...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 3...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 4...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 5...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\n\n\n\n\nCode\nresults\n\n\n\n\n\n\n\n\n\nprice\nsize\nbedrooms\ntitle\npricepersq\n\n\n\n\n0\n2474\n13942\n3\n3bd 2ba, Serene Wooded View, West Chester PA\n0.177449\n\n\n1\n2850\n13902\n2\nApartment by the arts!\n0.205006\n\n\n2\n1800\n7592\n1\nA Cozy Living Space In Rittenhouse Square.\n0.237092\n\n\n3\n1750\n5002\n1\nPets welcome in this Beautifully renovated apa...\n0.349860\n\n\n4\n1950\n8502\n2\nEnjoy a fantastic living space with tons of na...\n0.229358\n\n\n...\n...\n...\n...\n...\n...\n\n\n595\n2075\n12712\n2\nResort-Style Swimming Pool, Extra Storage, 2/BD\n0.163232\n\n\n596\n1876\n5072\n1\nOn-demand car wash/detailing, Handyman and mai...\n0.369874\n\n\n597\n1680\n5092\n1\nPenthouse Hideaway, LVT Flooring, Bike Storage\n0.329929\n\n\n598\n2199\n6922\n1\nFire Pit, On-site Management/Maintenance, Flex...\n0.317683\n\n\n599\n1544\n5182\n1\n1 BD, Bike Storage, Washer/Dryer\n0.297954\n\n\n\n\n600 rows × 5 columns\n\n\n\n\n\n\n2.7 Plotting the distribution of prices\nUse matplotlib’s hist() function to make two histograms for:\n\nApartment prices\nApartment prices per square foot (price / size)\n\nMake sure to add labels to the respective axes and a title describing the plot.\n\n\nCode\nplt.title('Apartment Price Distributions')\nplt.hist(results['price'], color='pink', edgecolor='grey')\nplt.xlabel('Price ($)')\nplt.ylabel('Counts')\nplt.show()\n\n\n\n\n\n\n\nCode\nresults['pricepersq'] = results['price'] / results['size']\n\nplt.title('Apartment Price Per Square foot')\nplt.hist(results['pricepersq'], color='lightgrey', edgecolor='grey')\nplt.xlabel('Price per sqft($)')\nplt.ylabel('Counts')\nplt.show()\n\n\n\n\n\n\nSide note: rental prices per sq. ft. from Craigslist\nThe histogram of price per sq ft should be centered around ~1.5. Here is a plot of how Philadelphia’s rents compare to the other most populous cities:\n\nSource\n\n\n\n2.8 Comparing prices for different sizes\nUse altair to explore the relationship between price, size, and number of bedrooms. Make an interactive scatter plot of price (x-axis) vs. size (y-axis), with the points colored by the number of bedrooms.\nMake sure the plot is interactive (zoom-able and pan-able) and add a tooltip with all of the columns in our scraped data frame.\nWith this sort of plot, you can quickly see the outlier apartments in terms of size and price.\n\n\nCode\nchart = alt.Chart(results)\nchart = chart.mark_circle(size=40)\nchart = chart.encode(\n    x = \"price:Q\",\n    y = \"size:Q\",\n    color = \"bedrooms:N\",\n    tooltip = [\"title\", \"price\", \"size\", \"bedrooms\"],\n)\n\nchart.interactive()"
  },
  {
    "objectID": "analysis/assignment-5.html",
    "href": "analysis/assignment-5.html",
    "title": "5 - Exploring Yelp Reviews in Philly",
    "section": "",
    "text": "In this assignment, we’ll explore restaurant review data available through the Yelp Dataset Challenge. The dataset includes Yelp data for user reviews and business information for many metropolitan areas. I’ve already downloaded this dataset (8 GB total!) and extracted out the data files for reviews and restaurants in Philadelphia. I’ve placed these data files into the data directory in this repository.\nThis assignment is broken into two parts:\nPart 1: Analyzing correlations between restaurant reviews and census data\nWe’ll explore the relationship between restaurant reviews and the income levels of the restaurant’s surrounding area.\nPart 2: Exploring the impact of fast food restaurants\nWe’ll run a sentiment analysis on reviews of fast food restaurants and estimate income levels in neighborhoods with fast food restaurants. We’ll test how well our sentiment analysis works by comparing the number of stars to the sentiment of reviews.\nBackground readings - Does sentiment analysis work? - The Geography of Taste: Using Yelp to Study Urban Culture\nCode\nimport pandas as pd\nimport geopandas as gpd\nimport altair as alt\nimport numpy as np"
  },
  {
    "objectID": "analysis/assignment-5.html#correlating-restaurant-ratings-and-income-levels",
    "href": "analysis/assignment-5.html#correlating-restaurant-ratings-and-income-levels",
    "title": "5 - Exploring Yelp Reviews in Philly",
    "section": "1. Correlating restaurant ratings and income levels",
    "text": "1. Correlating restaurant ratings and income levels\nIn this part, we’ll use the census API to download household income data and explore how it correlates with restaurant review data.\n\n1.1 Query the Census API\nUse the cenpy package to download median household income in the past 12 months by census tract from the 2021 ACS 5-year data set for your county of interest.\nYou have two options to find the correct variable names: - Search through: https://api.census.gov/data/2021/acs/acs5/variables.html - Initialize an API connection and use the .varslike() function to search for the proper keywords\nAt the end of this step, you should have a pandas DataFrame holding the income data for all census tracts within the county being analyzed. Feel free to rename your variable from the ACS so it has a more meaningful name!\n\n\n\n\n\n\nCaution\n\n\n\nSome census tracts won’t have any value because there are not enough households in that tract. The census will use a negative number as a default value for those tracts. You can safely remove those tracts from the analysis!\n\n\n\n\nCode\nimport cenpy\n\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/libpysal/cg/alpha_shapes.py:39: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def nb_dist(x, y):\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/libpysal/cg/alpha_shapes.py:165: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def get_faces(triangle):\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/libpysal/cg/alpha_shapes.py:199: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def build_faces(faces, triangles_is, num_triangles, num_faces_single):\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/libpysal/cg/alpha_shapes.py:261: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def nb_mask_faces(mask, faces):\n\n\n\n\nCode\nacs = cenpy.remote.APIConnection(\"ACSDT5Y2021\")\n\n\n\n\nCode\nmatches = acs.varslike(\n    pattern=\"household income\",\n    by=\"label\",\n).sort_index()\n\n\n\n\nCode\npd.set_option('display.max_colwidth', None)\nmatches.head(10)\n\n\n\n\n\n\n\n\n\nlabel\nconcept\npredicateType\ngroup\nlimit\npredicateOnly\nhasGeoCollectionSupport\nattributes\nrequired\n\n\n\n\nB19013A_001E\nEstimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS) (WHITE ALONE HOUSEHOLDER)\nint\nB19013A\n0\nNaN\nNaN\nB19013A_001EA,B19013A_001M,B19013A_001MA\nNaN\n\n\nB19013B_001E\nEstimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS) (BLACK OR AFRICAN AMERICAN ALONE HOUSEHOLDER)\nint\nB19013B\n0\nNaN\nNaN\nB19013B_001EA,B19013B_001M,B19013B_001MA\nNaN\n\n\nB19013C_001E\nEstimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS) (AMERICAN INDIAN AND ALASKA NATIVE ALONE HOUSEHOLDER)\nint\nB19013C\n0\nNaN\nNaN\nB19013C_001EA,B19013C_001M,B19013C_001MA\nNaN\n\n\nB19013D_001E\nEstimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS) (ASIAN ALONE HOUSEHOLDER)\nint\nB19013D\n0\nNaN\nNaN\nB19013D_001EA,B19013D_001M,B19013D_001MA\nNaN\n\n\nB19013E_001E\nEstimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS) (NATIVE HAWAIIAN AND OTHER PACIFIC ISLANDER ALONE HOUSEHOLDER)\nint\nB19013E\n0\nNaN\nNaN\nB19013E_001EA,B19013E_001M,B19013E_001MA\nNaN\n\n\nB19013F_001E\nEstimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS) (SOME OTHER RACE ALONE HOUSEHOLDER)\nint\nB19013F\n0\nNaN\nNaN\nB19013F_001EA,B19013F_001M,B19013F_001MA\nNaN\n\n\nB19013G_001E\nEstimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS) (TWO OR MORE RACES HOUSEHOLDER)\nint\nB19013G\n0\nNaN\nNaN\nB19013G_001EA,B19013G_001M,B19013G_001MA\nNaN\n\n\nB19013H_001E\nEstimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS) (WHITE ALONE, NOT HISPANIC OR LATINO HOUSEHOLDER)\nint\nB19013H\n0\nNaN\nNaN\nB19013H_001EA,B19013H_001M,B19013H_001MA\nNaN\n\n\nB19013I_001E\nEstimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS) (HISPANIC OR LATINO HOUSEHOLDER)\nint\nB19013I\n0\nNaN\nNaN\nB19013I_001EA,B19013I_001M,B19013I_001MA\nNaN\n\n\nB19013_001E\nEstimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS)\nint\nB19013\n0\nNaN\nNaN\nB19013_001EA,B19013_001M,B19013_001MA\nNaN\n\n\n\n\n\n\n\n\n\nCode\nvariables = [\n    \"NAME\",\n    \"B19013_001E\" #median income in the past 12 month\n]\n\n\n\n\nCode\nphilly_county_code = \"101\"\npa_state_code = \"42\"\n\nphilly_income = acs.query(cols=variables, geo_unit=\"block group:*\", geo_filter={\"state\": pa_state_code, \"county\": philly_county_code, \"tract\": \"*\"}, apikey='b3abcecc231fa30ccaa18cb5e854c30f1982fe3f')\nphilly_income.head(10)\n\nphilly_income['B19013_001E'] = philly_income['B19013_001E'].replace('-666666666', '0').astype('int32')\n\n\n\n\nCode\nphilly_income.head()\n\n\n\n\n\n\n\n\n\nNAME\nB19013_001E\nstate\ncounty\ntract\nblock group\n\n\n\n\n0\nBlock Group 1, Census Tract 1.01, Philadelphia County, Pennsylvania\n0\n42\n101\n000101\n1\n\n\n1\nBlock Group 2, Census Tract 1.01, Philadelphia County, Pennsylvania\n0\n42\n101\n000101\n2\n\n\n2\nBlock Group 3, Census Tract 1.01, Philadelphia County, Pennsylvania\n0\n42\n101\n000101\n3\n\n\n3\nBlock Group 4, Census Tract 1.01, Philadelphia County, Pennsylvania\n97210\n42\n101\n000101\n4\n\n\n4\nBlock Group 5, Census Tract 1.01, Philadelphia County, Pennsylvania\n109269\n42\n101\n000101\n5\n\n\n\n\n\n\n\n\n\n1.2 Download census tracts from the Census and merge the data from part 1.1\n\nDownload census tracts for the desired geography using the pygris package\nMerge the downloaded census tracts with the household income DataFrame\n\n\n\nCode\nimport pygris\nphilly_blocks = pygris.block_groups(state=pa_state_code, county=philly_county_code, year=2021)\n\n\n\n\nCode\nphilly_merged = philly_blocks.merge(\n    philly_income,\n    left_on=[\"STATEFP\", \"COUNTYFP\", \"TRACTCE\", \"BLKGRPCE\"],\n    right_on=[\"state\", \"county\", \"tract\", \"block group\"],)\n\n\n\n\nCode\nphilly_merged.head(1)\n\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nTRACTCE\nBLKGRPCE\nGEOID\nNAMELSAD\nMTFCC\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\ngeometry\nNAME\nB19013_001E\nstate\ncounty\ntract\nblock group\n\n\n\n\n0\n42\n101\n989100\n2\n421019891002\nBlock Group 2\nG5030\nS\n373653\n7060\n+40.0373207\n-075.0177378\nPOLYGON ((-75.02195 40.03435, -75.02191 40.03451, -75.02173 40.03482, -75.02151 40.03515, -75.02135 40.03544, -75.02129 40.03562, -75.02130 40.03582, -75.02134 40.03607, -75.02150 40.03638, -75.02163 40.03663, -75.02181 40.03702, -75.02168 40.03709, -75.01967 40.03813, -75.01902 40.03847, -75.01843 40.03877, -75.01783 40.03908, -75.01703 40.03950, -75.01621 40.03991, -75.01535 40.03894, -75.01500 40.03910, -75.01432 40.03945, -75.01254 40.03803, -75.01216 40.03772, -75.01195 40.03755, -75.01131 40.03704, -75.01225 40.03654, -75.01530 40.03485, -75.01790 40.03340, -75.01797 40.03336, -75.01817 40.03350, -75.01862 40.03366, -75.01959 40.03374, -75.01986 40.03376, -75.02000 40.03377, -75.02011 40.03377, -75.02022 40.03378, -75.02043 40.03379, -75.02083 40.03378, -75.02125 40.03374, -75.02142 40.03375, -75.02157 40.03381, -75.02173 40.03392, -75.02180 40.03401, -75.02189 40.03411, -75.02195 40.03422, -75.02195 40.03435))\nBlock Group 2, Census Tract 9891, Philadelphia County, Pennsylvania\n0\n42\n101\n989100\n2\n\n\n\n\n\n\n\n\n\n1.3 Load the restaurants data\nThe Yelp dataset includes data for 7,350 restaurants across the city. Load the data from the data/ folder and use the latitude and longitude columns to create a GeoDataFrame after loading the JSON data. Be sure to set the right CRS on when initializing the GeoDataFrame!\nNotes\nThe JSON data is in a “records” format. To load it, you’ll need to pass the following keywords:\n\norient='records'\nlines=True\n\n\n\nCode\ndata = pd.read_json(\"data/restaurants_philly.json.gz\", orient='records', lines=True)\ngpd_Data = gpd.GeoDataFrame(data, geometry=gpd.points_from_xy(data.longitude, data.latitude), crs=\"EPSG:4326\")\n\n\n\n\nCode\ngpd_Data.head()\n\n\n\n\n\n\n\n\n\nbusiness_id\nlatitude\nlongitude\nname\nreview_count\nstars\ncategories\ngeometry\n\n\n\n\n0\nMTSW4McQd7CbVtyjqoe9mw\n39.955505\n-75.155564\nSt Honore Pastries\n80\n4.0\nRestaurants, Food, Bubble Tea, Coffee & Tea, Bakeries\nPOINT (-75.15556 39.95551)\n\n\n1\nMUTTqe8uqyMdBl186RmNeA\n39.953949\n-75.143226\nTuna Bar\n245\n4.0\nSushi Bars, Restaurants, Japanese\nPOINT (-75.14323 39.95395)\n\n\n2\nROeacJQwBeh05Rqg7F6TCg\n39.943223\n-75.162568\nBAP\n205\n4.5\nKorean, Restaurants\nPOINT (-75.16257 39.94322)\n\n\n3\nQdN72BWoyFypdGJhhI5r7g\n39.939825\n-75.157447\nBar One\n65\n4.0\nCocktail Bars, Bars, Italian, Nightlife, Restaurants\nPOINT (-75.15745 39.93982)\n\n\n4\nMjboz24M9NlBeiOJKLEd_Q\n40.022466\n-75.218314\nDeSandro on Main\n41\n3.0\nPizza, Restaurants, Salad, Soup\nPOINT (-75.21831 40.02247)\n\n\n\n\n\n\n\n\n\n1.4 Add tract info for each restaurant\nDo a spatial join to identify which census tract each restaurant is within. Make sure each dataframe has the same CRS!\nAt the end of this step, you should have a new dataframe with a column identifying the tract number for each restaurant.\n\n\nCode\ntract_restaurant = gpd.sjoin(gpd_Data, philly_merged.loc[:,[\"geometry\", \"tract\", \"B19013_001E\"]], how=\"left\", op=\"within\")\ntract_restaurant = tract_restaurant.rename(columns={'B19013_001E': 'Median Household Income'})\ntract_restaurant.head()\n\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_14881/1177895681.py:1: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\nUse `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n\nLeft CRS: EPSG:4326\nRight CRS: EPSG:4269\n\n  tract_restaurant = gpd.sjoin(gpd_Data, philly_merged.loc[:,[\"geometry\", \"tract\", \"B19013_001E\"]], how=\"left\", op=\"within\")\n\n\n\n\n\n\n\n\n\nbusiness_id\nlatitude\nlongitude\nname\nreview_count\nstars\ncategories\ngeometry\nindex_right\ntract\nMedian Household Income\n\n\n\n\n0\nMTSW4McQd7CbVtyjqoe9mw\n39.955505\n-75.155564\nSt Honore Pastries\n80\n4.0\nRestaurants, Food, Bubble Tea, Coffee & Tea, Bakeries\nPOINT (-75.15556 39.95551)\n988.0\n000200\n42308.0\n\n\n1\nMUTTqe8uqyMdBl186RmNeA\n39.953949\n-75.143226\nTuna Bar\n245\n4.0\nSushi Bars, Restaurants, Japanese\nPOINT (-75.14323 39.95395)\n658.0\n000102\n198125.0\n\n\n2\nROeacJQwBeh05Rqg7F6TCg\n39.943223\n-75.162568\nBAP\n205\n4.5\nKorean, Restaurants\nPOINT (-75.16257 39.94322)\n689.0\n001500\n107292.0\n\n\n3\nQdN72BWoyFypdGJhhI5r7g\n39.939825\n-75.157447\nBar One\n65\n4.0\nCocktail Bars, Bars, Italian, Nightlife, Restaurants\nPOINT (-75.15745 39.93982)\n774.0\n001800\n103125.0\n\n\n4\nMjboz24M9NlBeiOJKLEd_Q\n40.022466\n-75.218314\nDeSandro on Main\n41\n3.0\nPizza, Restaurants, Salad, Soup\nPOINT (-75.21831 40.02247)\n715.0\n021000\n86146.0\n\n\n\n\n\n\n\n\n\n1.5 Add income data to your restaurant data\nAdd the income data to your dataframe from the previous step, merging the census data based on the tract that each restaurant is within.\n\n\nCode\n# this step is completed in 1.4\n\n\n\n\n1.6 Make a plot of median household income vs. Yelp stars\nOur dataset has the number of stars for each restaurant, rounded to the nearest 0.5 star. In this step, create a line plot that shows the average income value for each stars category (e.g., all restaurants with 1 star, 1.5 stars, 2 stars, etc.)\nWhile their are multiple ways to do this, the seaborn.lineplot() is a great option. This can show the average value in each category as well as 95% uncertainty intervals. Use this function to plot the stars (“x”) vs. average income (“y”) for all of our restaurants, using the dataframe from last step. Be sure to format your figure to make it look nice!\nQuestion: Is there a correlation between a restaurant’s ratings and the income levels of its surrounding neighborhood?\n\n\nCode\nstars = tract_restaurant.loc[:, ['stars','Median Household Income']].groupby(['stars']).median(['Median Household Income']).reset_index()\n\n\n\n\nCode\nimport seaborn as sns\n\n\n\n\nCode\nsns.set_theme()\nsns.lineplot(data=stars, x=\"stars\", y=\"Median Household Income\")\n\n\n&lt;Axes: xlabel='stars', ylabel='Median Household Income'&gt;\n\n\n\n\n\nAs suggested by the graph above, there is an overall trend of higher ratings of restaurants associated with higeher median household income in the tracts that restaurants are located in."
  },
  {
    "objectID": "analysis/assignment-5.html#fast-food-trends-in-philadelphia",
    "href": "analysis/assignment-5.html#fast-food-trends-in-philadelphia",
    "title": "5 - Exploring Yelp Reviews in Philly",
    "section": "2. Fast food trends in Philadelphia",
    "text": "2. Fast food trends in Philadelphia\nAt the end of part 1, you should have seen a strong trend where higher income tracts generally had restaurants with better reviews. In this section, we’ll explore the impact of fast food restaurants and how they might be impacting this trend.\nHypothesis\n\nFast food restaurants are predominantly located in areas with lower median income levels.\nFast food restaurants have worse reviews compared to typical restaurants.\n\nIf true, these two hypotheses could help to explain the trend we found in part 1. Let’s dive in and test our hypotheses!\n\n2.1 Identify fast food restaurants\nThe “categories” column in our dataset contains multiple classifications for each restaurant. One such category is “Fast Food”. In this step, add a new column called “is_fast_food” that is True if the “categories” column contains the term “Fast Food” and False otherwise\n\n\nCode\ntract_restaurant['is_fast_food'] = tract_restaurant['categories'].str.contains('Fast Food')\n\n\n\n\n2.2 Calculate the median income for fast food and otherwise\nGroup by the “is_fast_food” column and calculate the median income for restaurants that are and are not fast food. You should find that income levels are lower in tracts with fast food.\nNote: this is just an estimate, since we are calculating a median of median income values.\n\n\nCode\nfastfood_income = tract_restaurant.loc[:, ['is_fast_food','Median Household Income']].groupby(['is_fast_food']).median(['Median Household Income']).reset_index()\nfastfood_income\n\n\n\n\n\n\n\n\n\nis_fast_food\nMedian Household Income\n\n\n\n\n0\nFalse\n66290.0\n\n\n1\nTrue\n46668.0\n\n\n\n\n\n\n\n\n\n2.3 Load fast food review data\nIn the rest of part 2, we’re going to run a sentiment analysis on the reviews for fast food restaurants. The review data for all fast food restaurants identified in part 2.1 is already stored in the data/ folder. The data is stored as a JSON file and you can use pandas.read_json to load it.\nNotes\nThe JSON data is in a “records” format. To load it, you’ll need to pass the following keywords:\n\norient='records'\nlines=True\n\n\n\nCode\nreviews = pd.read_json(\"data/reviews_philly_fast_food.json.gz\", orient='records',lines=True)\nreviews.head()\n\n\n\n\n\n\n\n\n\nbusiness_id\nreview_id\nstars\ntext\n\n\n\n\n0\nkgMEBZG6rjkGeFzPaIM4MQ\nE-yGr1OhsUBxNeUVLDVouA\n1\nI know I shouldn't expect much but everything I asked for that was on the drive thru menu was not available. I was actually afraid of what I was going to get once I did get it. I saw the movie \"waiting\". Word of advice stay clear of this arch. Just so you know I was only trying to order a beverage how pathetic is that.\n\n\n1\nFKrP06TDAKtxNG1vrRQcQQ\n0IpFZoaY_RRNjha8Q_Wz6w\n2\nPerfect place to go if you like waiting 20 minutes at the counter and getting dirty looks from the waiters while you're waiting for service. My friend and I made the mistake of coming here after being famished after a long day. Hey Johnny Rockets, I understand that you guys can get busy, but a quick acknowledgment that we existed would have sufficed. After finally waving down a waitress, we finally got a menu with a heavy sigh on the side as if we were troubling the staff. I urged my friend that we should probably leave, but we had already waited long enough, so I reluctantly ordered.\\nI got the original burger and fries as did my friend as per usual. Don't get me wrong, the burgers and fries are decent but definitely not worth the wait or hassle. Even the chocolate shake that I ordered was pretty good with a good consistency, but the taste of bad service still lingered and eventually overpowered the chocolate flavored concoction. Needless to say, I will definitely not be coming back to this location if it continues to be managed so rudely.\n\n\n2\nw9hS5x1F52Id-G1KTrAOZg\n0KlwfaHZyvao41_3S47dyg\n2\nWas not a fan of their cheesesteak. Their wiz sauce was mustard based and it was not terrible as a sandwich itself but mustard is not the flavor one expects or wants in a cheesesteak. It was overwhelming and not good.\n\n\n3\nfr2qDm_mY1afIGMvqsKUCg\noKSUOq7pCQzyypFDSa1HoA\n3\nOk this is an aberration from my city foodie reviews but I figured I'll take the time and review some of the local establishments that I order from on a consistent base while working in Bensalem, P.A. Without further adieu...\\nGeorge's Chix is good. I'm always satisfied with the food and value each and every time. I haven't branched out seeking much more than a good deal each time and trust me I'm not looking to find the healthiest option on the menu (usually I get the nugget/fries/soda deal, the chix fingers/fries/soda deal, or Ceasar wrap/fries/soda deal- $8 or $9 after a nice tip) but the food is alright.\\nGive George's a try if you're in town and looking to get down on some fried grub!\n\n\n4\nfr2qDm_mY1afIGMvqsKUCg\n6SMUmb7Npwnq6AusxqOXzQ\n5\nMy family has been customers of George's for years. You pay for what you get. The quality of the food is premier and they don't hold back on portion size. Their cheesesteaks are amazing (chopped steak) and their chicken cheesesteaks are made from the same chicken as their Char platters, which is legit. Their Char platters come with some of the best and juciest grilled chicken I've ever had. They make their own sauce, so just that alone puts their wings above the other restaurants in the neighborhood that just use Franks Red Hot. I have yet to have something on their menu that I didn't like.\n\n\n\n\n\n\n\n\n\n2.4 Trim to the most popular fast food restaurants\nThere’s too many reviews to run a sentiment analysis on all of them in a reasonable time. Let’s trim our reviews dataset to the most popular fast food restaurants, using the list provided below.\nYou will need to get the “business_id” values for each of these restaurants from the restaurants data loaded in part 1.3. Then, trim the reviews data to include reviews only for those business IDs.\n\n\nCode\npopular_fast_food = [\n    \"McDonald's\",\n    \"Wendy's\",\n    \"Subway\",\n    \"Popeyes Louisiana Kitchen\",\n    \"Taco Bell\",\n    \"KFC\",\n    \"Burger King\",\n]\n\n\n\n\nCode\npop_fast_food = tract_restaurant[tract_restaurant['name'].isin(popular_fast_food)]\nselected_reviews = reviews[reviews['business_id'].isin(pop_fast_food['business_id'])]\nselected_reviews.head()\n\n\n\n\n\n\n\n\n\nbusiness_id\nreview_id\nstars\ntext\n\n\n\n\n0\nkgMEBZG6rjkGeFzPaIM4MQ\nE-yGr1OhsUBxNeUVLDVouA\n1\nI know I shouldn't expect much but everything I asked for that was on the drive thru menu was not available. I was actually afraid of what I was going to get once I did get it. I saw the movie \"waiting\". Word of advice stay clear of this arch. Just so you know I was only trying to order a beverage how pathetic is that.\n\n\n8\nPjknD8uD_0tisZQbomiYoQ\n6TqKBa-HDiq2_W_ip2AItA\n5\nI am only giving 5 stars because the Shamrock Shake is back and delicious!! Too bad it's around only once a year ;(\n\n\n13\nkgMEBZG6rjkGeFzPaIM4MQ\nNGaXI03qbtBLshjfJV4pbQ\n3\nDirty bathrooms and very slow service, but I was pleased because they had a TV on with subtitles and the volume on, and it was turned to the news! A good place to pass some time with a tasty Mc-snack and a hot coffee while in Philly during a chilly day! We stopped here on the way to a football game and found it a very pleasant and relaxing place to hang out for a while.\n\n\n17\nLACylKxImNI29DKUQpWuHw\nHHy9yIjW07VHUE6nXVbsVA\n3\nBurger King is an okay alternative to Mcdonalds. 6/10 Would recommend but idk about coming again.\n\n\n21\ngq4zw-ru_rkZ2UBIanaZFQ\nyMZTK5B_0SAdUXSrIkXrmA\n1\nive tried going here four times with no success because the drive tru takes so long that i have pulled away every single time. great job idiots!\n\n\n\n\n\n\n\n\n\n2.5 Run the emotions classifier on fast food reviews\nRun a sentiment analysis on the reviews data from the previous step. Use the DistilBERT model that can predict emotion labels (anger, fear, sadness, joy, love, and surprise). Transform the result from the classifier into a DataFrame so that you have a column for each of the emotion labels.\n\n\nCode\nfrom transformers import pipeline\n\n\n2023-11-11 21:15:57.312117: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\n\nCode\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\nmodel = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n\ninputs = tokenizer(\"Hello, my daog is cute\", return_tensors=\"pt\")\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npredicted_class_id = logits.argmax().item()\nmodel.config.id2label[predicted_class_id]\n\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n'LABEL_0'\n\n\n\n\nCode\n# The model\nmodel = \"bhadresh-savani/distilbert-base-uncased-emotion\"\n\n# Initialize our sentiment analyzer\nemotion_classifier = pipeline(\n    task=\"text-classification\",  # The task we are doing\n    model=model,  # The specific model name\n    top_k=None,  # Predict all labels, not just top ones\n    tokenizer=model,  # Tokenize inputs using model tokenizer\n    truncation=True,  # Truncate text if we need to\n)\n\n\n\n\n\n\n\nCode\nreview = selected_reviews['text'].str.strip().tolist()\n\n\n\n\nCode\n%%time\nscores = emotion_classifier(review)\n\n\nCPU times: user 11min 18s, sys: 19.7 s, total: 11min 38s\nWall time: 2min 56s\n\n\n\n\nCode\nscores[0]\n\n\n[{'label': 'sadness', 'score': 0.7338692545890808},\n {'label': 'fear', 'score': 0.2506760060787201},\n {'label': 'anger', 'score': 0.011038999073207378},\n {'label': 'joy', 'score': 0.0027580487076193094},\n {'label': 'surprise', 'score': 0.0010148979490622878},\n {'label': 'love', 'score': 0.0006427845801226795}]\n\n\n\n\nCode\nemo = pd.DataFrame([{d[\"label\"]: d[\"score\"] for d in dd} for dd in scores]).assign(\n    text=review\n)\nemo.head()\n\n\n\n\n\n\n\n\n\nsadness\nfear\nanger\njoy\nsurprise\nlove\ntext\n\n\n\n\n0\n0.733869\n0.250676\n0.011039\n0.002758\n0.001015\n0.000643\nI know I shouldn't expect much but everything I asked for that was on the drive thru menu was not available. I was actually afraid of what I was going to get once I did get it. I saw the movie \"waiting\". Word of advice stay clear of this arch. Just so you know I was only trying to order a beverage how pathetic is that.\n\n\n1\n0.000230\n0.000126\n0.000165\n0.998759\n0.000246\n0.000475\nI am only giving 5 stars because the Shamrock Shake is back and delicious!! Too bad it's around only once a year ;(\n\n\n2\n0.000216\n0.000088\n0.000153\n0.998563\n0.000161\n0.000819\nDirty bathrooms and very slow service, but I was pleased because they had a TV on with subtitles and the volume on, and it was turned to the news! A good place to pass some time with a tasty Mc-snack and a hot coffee while in Philly during a chilly day! We stopped here on the way to a football game and found it a very pleasant and relaxing place to hang out for a while.\n\n\n3\n0.000838\n0.000403\n0.000811\n0.996928\n0.000140\n0.000880\nBurger King is an okay alternative to Mcdonalds. 6/10 Would recommend but idk about coming again.\n\n\n4\n0.005284\n0.000753\n0.006195\n0.985421\n0.001620\n0.000726\nive tried going here four times with no success because the drive tru takes so long that i have pulled away every single time. great job idiots!\n\n\n\n\n\n\n\n\n\n2.6 Identify the predicted emotion for each text\nUse the pandas idxmax() to identify the predicted emotion for each review, and add this value to a new column called “prediction”\nThe predicted emotion has the highest confidence score across all emotion labels for a particular label.\n\n\nCode\nemo_labels = [\"anger\", \"fear\", \"sadness\", \"joy\", \"surprise\", \"love\"]\nemo['prediction'] = emo[emo_labels].idxmax(axis=1)\nemo.head()\n\n\n\n\n\n\n\n\n\nsadness\nfear\nanger\njoy\nsurprise\nlove\ntext\nprediction\n\n\n\n\n0\n0.733869\n0.250676\n0.011039\n0.002758\n0.001015\n0.000643\nI know I shouldn't expect much but everything I asked for that was on the drive thru menu was not available. I was actually afraid of what I was going to get once I did get it. I saw the movie \"waiting\". Word of advice stay clear of this arch. Just so you know I was only trying to order a beverage how pathetic is that.\nsadness\n\n\n1\n0.000230\n0.000126\n0.000165\n0.998759\n0.000246\n0.000475\nI am only giving 5 stars because the Shamrock Shake is back and delicious!! Too bad it's around only once a year ;(\njoy\n\n\n2\n0.000216\n0.000088\n0.000153\n0.998563\n0.000161\n0.000819\nDirty bathrooms and very slow service, but I was pleased because they had a TV on with subtitles and the volume on, and it was turned to the news! A good place to pass some time with a tasty Mc-snack and a hot coffee while in Philly during a chilly day! We stopped here on the way to a football game and found it a very pleasant and relaxing place to hang out for a while.\njoy\n\n\n3\n0.000838\n0.000403\n0.000811\n0.996928\n0.000140\n0.000880\nBurger King is an okay alternative to Mcdonalds. 6/10 Would recommend but idk about coming again.\njoy\n\n\n4\n0.005284\n0.000753\n0.006195\n0.985421\n0.001620\n0.000726\nive tried going here four times with no success because the drive tru takes so long that i have pulled away every single time. great job idiots!\njoy\n\n\n\n\n\n\n\n\n\n2.7 Combine the ratings and sentiment data\nCombine the data from part 2.4 (reviews data) and part 2.6 (emotion data). Use the pd.concat() function and combine along the column axis.\nNote: You’ll need to reset the index of your reviews data frame so it matches the emotion data index (it should run from 0 to the length of the data - 1).\n\n\nCode\ncombined = pd.concat([selected_reviews.reset_index(), emo], axis=1)\ncombined.head()\n\n\n\n\n\n\n\n\n\nindex\nbusiness_id\nreview_id\nstars\ntext\nsadness\nfear\nanger\njoy\nsurprise\nlove\ntext\nprediction\n\n\n\n\n0\n0\nkgMEBZG6rjkGeFzPaIM4MQ\nE-yGr1OhsUBxNeUVLDVouA\n1\nI know I shouldn't expect much but everything I asked for that was on the drive thru menu was not available. I was actually afraid of what I was going to get once I did get it. I saw the movie \"waiting\". Word of advice stay clear of this arch. Just so you know I was only trying to order a beverage how pathetic is that.\n0.733869\n0.250676\n0.011039\n0.002758\n0.001015\n0.000643\nI know I shouldn't expect much but everything I asked for that was on the drive thru menu was not available. I was actually afraid of what I was going to get once I did get it. I saw the movie \"waiting\". Word of advice stay clear of this arch. Just so you know I was only trying to order a beverage how pathetic is that.\nsadness\n\n\n1\n8\nPjknD8uD_0tisZQbomiYoQ\n6TqKBa-HDiq2_W_ip2AItA\n5\nI am only giving 5 stars because the Shamrock Shake is back and delicious!! Too bad it's around only once a year ;(\n0.000230\n0.000126\n0.000165\n0.998759\n0.000246\n0.000475\nI am only giving 5 stars because the Shamrock Shake is back and delicious!! Too bad it's around only once a year ;(\njoy\n\n\n2\n13\nkgMEBZG6rjkGeFzPaIM4MQ\nNGaXI03qbtBLshjfJV4pbQ\n3\nDirty bathrooms and very slow service, but I was pleased because they had a TV on with subtitles and the volume on, and it was turned to the news! A good place to pass some time with a tasty Mc-snack and a hot coffee while in Philly during a chilly day! We stopped here on the way to a football game and found it a very pleasant and relaxing place to hang out for a while.\n0.000216\n0.000088\n0.000153\n0.998563\n0.000161\n0.000819\nDirty bathrooms and very slow service, but I was pleased because they had a TV on with subtitles and the volume on, and it was turned to the news! A good place to pass some time with a tasty Mc-snack and a hot coffee while in Philly during a chilly day! We stopped here on the way to a football game and found it a very pleasant and relaxing place to hang out for a while.\njoy\n\n\n3\n17\nLACylKxImNI29DKUQpWuHw\nHHy9yIjW07VHUE6nXVbsVA\n3\nBurger King is an okay alternative to Mcdonalds. 6/10 Would recommend but idk about coming again.\n0.000838\n0.000403\n0.000811\n0.996928\n0.000140\n0.000880\nBurger King is an okay alternative to Mcdonalds. 6/10 Would recommend but idk about coming again.\njoy\n\n\n4\n21\ngq4zw-ru_rkZ2UBIanaZFQ\nyMZTK5B_0SAdUXSrIkXrmA\n1\nive tried going here four times with no success because the drive tru takes so long that i have pulled away every single time. great job idiots!\n0.005284\n0.000753\n0.006195\n0.985421\n0.001620\n0.000726\nive tried going here four times with no success because the drive tru takes so long that i have pulled away every single time. great job idiots!\njoy\n\n\n\n\n\n\n\n\n\n2.8 Plot sentiment vs. stars\nWe now have a dataframe with the predicted primary emotion for each review and the associated number of stars for each review. Let’s explore two questions:\n\nDoes sentiment analysis work? Do reviews with fewer stars have negative emotions?\nFor our fast food restaurants, are reviews generally positive or negative?\n\nUse seaborn’s histplot() to make a stacked bar chart showing the breakdown of each emotion for each stars category (1 star, 2 stars, etc.). A few notes:\n\nTo stack multiple emotion labels in one bar, use the multiple=\"stack\" keyword\nThe discrete=True can be helpful to tell seaborn our stars values are discrete categories\n\n\n\nCode\nsns.set_theme()\nsns.histplot(data=combined, x=\"stars\", hue=\"prediction\", multiple=\"stack\", discrete=True)\n\n\n&lt;Axes: xlabel='stars', ylabel='Count'&gt;\n\n\n\n\n\nQuestion: What does your chart indicate for the effectiveness of our sentiment analysis? Does our original hypothesis about fast food restaurants seem plausible?\nThe chart indicates two important pieces of information – 1) disproportionally large number of reviews on fast food restaurants are 1-star; 2) based on results from sentiment analysis, highly rated reviews usually reflects joy, while 1-star reviews largely reflects sadness and anger, despite a small portion reflecting joy."
  },
  {
    "objectID": "analysis/assignment-6.html",
    "href": "analysis/assignment-6.html",
    "title": "6 - Predictive Modeling of Housing Prices in Philadelphia",
    "section": "",
    "text": "Due date: Wednesday, 12/6 by the end of the day\nLectures 12B and 13A will cover predictive modeling of housing prices in Philadelphia. We’ll extend that analysis in this section by:"
  },
  {
    "objectID": "analysis/assignment-6.html#part-2-modeling-philadelphias-housing-prices-and-algorithmic-fairness",
    "href": "analysis/assignment-6.html#part-2-modeling-philadelphias-housing-prices-and-algorithmic-fairness",
    "title": "6 - Predictive Modeling of Housing Prices in Philadelphia",
    "section": "Part 2: Modeling Philadelphia’s Housing Prices and Algorithmic Fairness",
    "text": "Part 2: Modeling Philadelphia’s Housing Prices and Algorithmic Fairness\n\n2.1 Load data from the Office of Property Assessment\nUse the requests package to query the CARTO API for single-family property assessment data in Philadelphia for properties that had their last sale during 2022.\nSources: - OpenDataPhilly - Metadata\n\n\nCode\nimport requests\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\n\n\n\n\nCode\ncarto_url = \"https://phl.carto.com/api/v2/sql\"\nwhere = \"sale_date &gt;= '2022-01-01' and sale_date &lt;= '2022-12-31'\"\nwhere = where + \" and category_code_description IN ('SINGLE FAMILY', 'Single Family')\"\n\n\n\n\nCode\n# Create the query\nquery = f\"SELECT * FROM opa_properties_public WHERE {where}\"\n\n# Make the request\nparams = {\"q\": query, \"format\": \"geojson\", \"where\": where}\nresponse = requests.get(carto_url, params=params)\n\n# Make the GeoDataFrame\nsales2022 = gpd.GeoDataFrame.from_features(response.json(), crs=\"EPSG:4326\")\n\n\n\n\n2.2 Load data for census tracts and neighborhoods\nLoad various Philadelphia-based regions that we will use in our analysis.\n\nCensus tracts can be downloaded from: https://opendata.arcgis.com/datasets/8bc0786524a4486bb3cf0f9862ad0fbf_0.geojson\nNeighborhoods can be downloaded from: https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson\n\n\n\nCode\ntracts = gpd.read_file('2010_Tracts.geojson')\n\n\n\n\nCode\nurl = 'https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson'\nneighbor = gpd.read_file(url).to_crs('EPSG:4326')\n\n\n\n\n2.3 Spatially join the sales data and neighborhoods/census tracts.\nPerform a spatial join, such that each sale has an associated neighborhood and census tract.\nNote: After performing the first spatial join, you will need to use the drop() function to remove the index_right column; otherwise an error will be raised on the second spatial join about duplicate columns.\n\n\nCode\njoined = gpd.sjoin(sales2022, tracts, predicate=\"within\", how=\"left\",)\njoined = gpd.sjoin(joined.drop('index_right', axis=1), neighbor, predicate = \"within\", how = \"left\")\n\n\n\n\n2.4 Train a Random Forest on the sales data\nIn this step, you should follow the steps outlined in lecture to preprocess and train your model. We’ll extend our analysis to do a hyperparameter grid search to find the best model configuration. As you train your model, follow the following steps:\nPreprocessing Requirements - Trim the sales data to those sales with prices between $3,000 and $1 million - Set up a pipeline that includes both numerical columns and categorical columns - Include one-hot encoded variable for the neighborhood of the sale, instead of ZIP code. We don’t want to include multiple location based categories, since they encode much of the same information.\nTraining requirements - Use a 70/30% training/test split and predict the log of the sales price. - Use GridSearchCV to perform a k-fold cross validation that optimize at least 2 hyperparameters of the RandomForestRegressor - After fitting your model and finding the optimal hyperparameters, you should evaluate the score (R-squared) on the test set (the original 30% sample withheld)\nNote: You don’t need to include additional features (such as spatial distance features) or perform any extra feature engineering beyond what is required above to receive full credit. Of course, you are always welcome to experiment!\n\n\nCode\n# model\nfrom sklearn.ensemble import RandomForestRegressor\n\n# pipeline\nfrom sklearn.pipeline import make_pipeline\n\n# model selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\n\n\n\n\nCode\n# trim data to between $3000 and $1 million\nvalid = (joined['sale_price'] &gt; 3000) & (joined['sale_price'] &lt; 1e6)\njoined = joined.loc[valid]\n\n\n\n\nCode\npd.set_option('display.max_columns', None)\njoined.head()\n\n\n\n\n\n\n\n\n\ngeometry\ncartodb_id_left\nassessment_date\nbasements\nbeginning_point\nbook_and_page\nbuilding_code\nbuilding_code_description\ncategory_code\ncategory_code_description\ncensus_tract\ncentral_air\ncross_reference\ndate_exterior_condition\ndepth\nexempt_building\nexempt_land\nexterior_condition\nfireplaces\nfrontage\nfuel\ngarage_spaces\ngarage_type\ngeneral_construction\ngeographic_ward\nhomestead_exemption\nhouse_extension\nhouse_number\ninterior_condition\nlocation\nmailing_address_1\nmailing_address_2\nmailing_care_of\nmailing_city_state\nmailing_street\nmailing_zip\nmarket_value\nmarket_value_date\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_of_rooms\nnumber_stories\noff_street_open\nother_building\nowner_1\nowner_2\nparcel_number\nparcel_shape\nquality_grade\nrecording_date\nregistry_number\nsale_date\nsale_price\nseparate_utilities\nsewer\nsite_type\nstate_code\nstreet_code\nstreet_designation\nstreet_direction\nstreet_name\nsuffix\ntaxable_building\ntaxable_land\ntopography\ntotal_area\ntotal_livable_area\ntype_heater\nunfinished\nunit\nutility\nview_type\nyear_built\nyear_built_estimate\nzip_code\nzoning\npin\nbuilding_code_new\nbuilding_code_description_new\nobjectid\nOBJECTID\nSTATEFP10\nCOUNTYFP10\nTRACTCE10\nGEOID10\nNAME10\nNAMELSAD10\nMTFCC10\nFUNCSTAT10\nALAND10\nAWATER10\nINTPTLAT10\nINTPTLON10\nLOGRECNO\nindex_right\nname\nlistname\nmapname\nshape_leng\nshape_area\ncartodb_id_right\ncreated_at\nupdated_at\n\n\n\n\n0\nPOINT (-75.14337 40.00957)\n1136\n2022-05-24T00:00:00Z\nD\n415' N OF ERIE AVE\n54230032\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n198\nN\nNone\nNone\n45.0\n0.0\n0.0\n4\n0.0\n16.0\nNone\n0.0\nNone\nA\n43\n0\nNone\n3753\n4\n3753 N DELHI ST\nNone\nNone\nNone\nDELRAY BEACH FL\n4899 NW 6TH STREET\n33445\n73800\nNone\n1.0\n3.0\nNaN\n2.0\n1683.0\nNone\nRJ SIMPLE SOLUTION LLC\nNone\n432345900\nE\nC\n2023-10-04T00:00:00Z\n100N040379\n2022-06-13T00:00:00Z\n35000\nNone\nNone\nNone\nFL\n28040\nST\nN\nDELHI\nNone\n59040.0\n14760.0\nF\n720.0\n960.0\nH\nNone\nNone\nNone\nI\n1942\nY\n19140\nRM1\n1001175031\n24\nROW PORCH FRONT\n397011561\n23.0\n42\n101\n019800\n42101019800\n198\nCensus Tract 198\nG5020\nS\n541006.0\n0.0\n+40.0107245\n-075.1421472\n10523\n98.0\nHUNTING_PARK\nHunting Park\nHunting Park\n32920.799360\n3.902450e+07\n73.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n1\nPOINT (-75.13389 40.03928)\n1153\n2022-05-24T00:00:00Z\nH\n241' N OF CHEW ST\n54230133\nR30\nROW B/GAR 2 STY MASONRY\n1\nSINGLE FAMILY\n275\nN\nNone\nNone\n95.0\n0.0\n0.0\n7\n0.0\n15.0\nNone\n1.0\nNone\nB\n61\n0\nNone\n5732\n4\n5732 N 7TH ST\nWALKER MICHAEL\nNone\nNone\nSICKLERVILLE NJ\n44 FARMHOUSE RD\n08081\n133400\nNone\n1.0\n3.0\nNaN\n2.0\n1920.0\nNone\nWALKER MICHAEL\nNone\n612234600\nE\nC\n2023-10-04T00:00:00Z\n135N7 61\n2022-08-21T00:00:00Z\n21000\nNone\nNone\nNone\nNJ\n87930\nST\nN\n7TH\nNone\n106720.0\n26680.0\nF\n1425.0\n1164.0\nH\nNone\nNone\nNone\nI\n1925\nY\n19120\nRSA5\n1001602509\n24\nROW PORCH FRONT\n397011578\n350.0\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825.0\n0.0\n+40.0400497\n-075.1322707\n10588\n42.0\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n5\nPOINT (-75.16524 40.00137)\n2832\n2022-05-24T00:00:00Z\nD\n120' W 21ST ST\n54222844\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n173\nN\nNone\nNone\n79.0\n0.0\n0.0\n4\n0.0\n20.0\nNone\n0.0\nNone\nF\n11\n0\nNone\n2116\n3\n2116 W CLEARFIELD ST\nNone\nNone\nNone\nPHILADELPHIA PA\n2116 W CLEARFIELD ST\n19132-1517\n139700\nNone\n1.0\n4.0\nNaN\n2.0\n929.0\nNone\nMILLIONIQUE LLC\nNone\n111112200\nE\nC\n2023-09-13T00:00:00Z\n38N5 289\n2022-03-03T00:00:00Z\n10000\nNone\nNone\nNone\nPA\n23640\nST\nW\nCLEARFIELD\nNone\n111760.0\n27940.0\nF\n1583.0\n1960.0\nH\nNone\nNone\nNone\nI\n1925\nY\n19132\nRSA5\n1001149040\n22\nROW TYPICAL\n397012834\n241.0\n42\n101\n017300\n42101017300\n173\nCensus Tract 173\nG5020\nS\n874586.0\n0.0\n+39.9985707\n-075.1609337\n10503\n12.0\nGLENWOOD\nGlenwood\nGlenwood\n12350.882116\n9.652808e+06\n65.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n6\nPOINT (-75.19413 40.07053)\n2895\n2022-05-24T00:00:00Z\nNone\n22'3\" S MERMAID LANE\n54222305\nH30\nSEMI/DET 2 STY MASONRY\n1\nSINGLE FAMILY\n257\nNone\nNone\nNone\n74.0\n0.0\n0.0\n4\n0.0\n35.0\nNone\n0.0\nNone\nA\n9\n0\n50\n7648\n4\n7648-50 ARDLEIGH ST\n7648-50 ARDLEIGH ST LLC\nNone\nNone\nPHILADELPHIA PA\n7715 CRITTENDEN ST#301\n19118\n560400\nNone\n1.0\n4.0\nNaN\n2.0\n6392.0\nNone\n7648-50 ARDLEIGH ST LLC\nNone\n091196200\nB\nC+\n2023-09-12T00:00:00Z\n099N020059\n2022-11-21T00:00:00Z\n350000\nNone\nNone\nNone\nPA\n13080\nST\nNone\nARDLEIGH\nNone\n448320.0\n112080.0\nF\n2457.0\n2110.0\nNone\nNone\nNone\nNone\nI\n1925\nNone\n19118\nRSA3\n1001071159\n34\nTWIN COLONIAL\n397012855\n261.0\n42\n101\n025700\n42101025700\n257\nCensus Tract 257\nG5020\nS\n748205.0\n0.0\n+40.0724586\n-075.1962708\n10568\n41.0\nCHESTNUT_HILL\nChestnut Hill\nChestnut Hill\n56394.297195\n7.966498e+07\n4.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n10\nPOINT (-75.23000 39.98765)\n3997\n2022-05-24T00:00:00Z\nH\n16'4\"W 53RD\n54207394\nR30\nROW B/GAR 2 STY MASONRY\n1\nSINGLE FAMILY\n119\nN\nNone\nNone\n115.0\n0.0\n0.0\n4\n0.0\n16.0\nNone\n1.0\nNone\nA\n52\n0\nNone\n5302\n5\n5302 LEBANON AVE\nDEE & J HOLDINGS LLC\nNone\nNone\nLANSDOWNE PA\n440 CHESTER AVE APT B\n19050\n105700\nNone\n1.0\n4.0\nNaN\n2.0\n2894.0\nNone\nDEE & J HOLDINGS LLC\nNone\n521134700\nE\nC\n2023-07-31T00:00:00Z\n156N200076\n2022-08-31T00:00:00Z\n165000\nNone\nNone\nNone\nPA\n49800\nAVE\nNone\nLEBANON\nNone\n84560.0\n21140.0\nF\n1840.0\n1736.0\nB\nNone\nNone\nNone\nA\n1925\nY\n19131\nRSA5\n1001319486\n24\nROW PORCH FRONT\n397015123\n230.0\n42\n101\n011900\n42101011900\n119\nCensus Tract 119\nG5020\nS\n691111.0\n0.0\n+39.9861142\n-075.2288429\n10452\n147.0\nWYNNEFIELD\nWynnefield\nWynnefield\n29540.613118\n3.169078e+07\n145.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n\n\n\n\n\n\n\nCode\n#categorical \ncat_cols = ['exterior_condition', 'interior_condition', 'quality_grade', 'parcel_shape',  'view_type', 'year_built', 'listname']\n \n# numeric\nnum_cols = ['fireplaces', 'garage_spaces', 'number_of_bathrooms', 'number_of_bedrooms', 'number_stories', 'total_area', 'total_livable_area']\n\nfeatures = cat_cols + num_cols + ['sale_price']\n\n\n\n\nCode\njoined_trimmed = joined[features].dropna()\njoined_trimmed\n\n\n\n\n\n\n\n\n\nexterior_condition\ninterior_condition\nquality_grade\nparcel_shape\nview_type\nyear_built\nlistname\nfireplaces\ngarage_spaces\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_stories\ntotal_area\ntotal_livable_area\nsale_price\n\n\n\n\n0\n4\n4\nC\nE\nI\n1942\nHunting Park\n0.0\n0.0\n1.0\n3.0\n2.0\n720.0\n960.0\n35000\n\n\n1\n7\n4\nC\nE\nI\n1925\nOlney\n0.0\n1.0\n1.0\n3.0\n2.0\n1425.0\n1164.0\n21000\n\n\n5\n4\n3\nC\nE\nI\n1925\nGlenwood\n0.0\n0.0\n1.0\n4.0\n2.0\n1583.0\n1960.0\n10000\n\n\n6\n4\n4\nC+\nB\nI\n1925\nChestnut Hill\n0.0\n0.0\n1.0\n4.0\n2.0\n2457.0\n2110.0\n350000\n\n\n10\n4\n5\nC\nE\nA\n1925\nWynnefield\n0.0\n1.0\n1.0\n4.0\n2.0\n1840.0\n1736.0\n165000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n24440\n4\n4\nC\nE\nI\n1910\nRichmond\n0.0\n0.0\n1.0\n3.0\n2.0\n741.0\n1020.0\n46000\n\n\n24444\n4\n3\nC\nE\nA\n1950\nMayfair\n0.0\n1.0\n2.0\n3.0\n1.0\n1424.0\n1292.0\n280000\n\n\n24448\n4\n4\nC+\nE\nI\n1930\nFox Chase\n0.0\n1.0\n2.0\n4.0\n2.0\n2850.0\n1572.0\n335000\n\n\n24449\n4\n4\nC\nE\nI\n1955\nHolmesburg\n0.0\n1.0\n0.0\n0.0\n1.0\n2567.0\n1121.0\n285000\n\n\n24450\n4\n4\nC+\nE\nI\n1965\nBustleton\n0.0\n1.0\n2.0\n3.0\n1.0\n7700.0\n2131.0\n438000\n\n\n\n\n17434 rows × 15 columns\n\n\n\n\n\nCode\njoined_trimmed['sale_price'] = pd.Series(joined_trimmed['sale_price'], dtype='int32')\n\n\n\n\nCode\njoined_trimmed[\"log_price\"] = np.log(joined_trimmed[\"sale_price\"])\n\n\n\n\nCode\n# 70/30 training / test split \ntrain_set, test_set = train_test_split(joined_trimmed, test_size=0.3, random_state=42)\n\n# the target labels\ny_train = np.log(train_set[\"log_price\"]).values\ny_test = np.log(test_set[\"log_price\"]).values\n\n# features\nfeature_cols = cat_cols + num_cols\nX_train = train_set[feature_cols]\nX_test = test_set[feature_cols]\n\n\n\n\nCode\n# column transformers + pipeline for random forest\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder()\n\ntransformer = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), num_cols),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n    ]\n)\n\npipe = make_pipeline(transformer, RandomForestRegressor(\n                                       random_state=42)\n)\n\n\n\n\nCode\n# Fit the training set\npipe.fit(train_set, y_train);\n\n\n\n\nCode\n# Test Score\npipe.score(test_set, y_test)\n\n\n0.5467018893536927\n\n\n\n\nCode\n# Setup for GridSearchCV\n\nmodel_step = \"randomforestregressor\"\nparam_grid = {\n    f\"{model_step}__n_estimators\": [5, 10, 15, 20, 30, 50, 100, 200],\n    f\"{model_step}__max_depth\": [2, 5, 7, 9, 13, 21, 33, 51],\n}\n\n\n\n\nCode\n\n# Create the grid and use 3-fold CV\ngrid = GridSearchCV(forest_pipeline, param_grid, cv=3, verbose=1)\n\n# Run the search\ngrid.fit(X_train, y_train)\n\n\n\nFitting 3 folds for each of 64 candidates, totalling 192 fits\n\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['fireplaces',\n                                                                          'garage_spaces',\n                                                                          'number_of_bathrooms',\n                                                                          'number_of_bedrooms',\n                                                                          'number_stories',\n                                                                          'total_area',\n                                                                          'total_livable_area']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['exterior_condition',\n                                                                          'interior_condition',\n                                                                          'quality_grade',\n                                                                          'parcel_shape',\n                                                                          'view_type',\n                                                                          'year_built',\n                                                                          'listname'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid={'randomforestregressor__max_depth': [2, 5, 7, 9, 13,\n                                                              21, 33, 51],\n                         'randomforestregressor__n_estimators': [5, 10, 15, 20,\n                                                                 30, 50, 100,\n                                                                 200]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['fireplaces',\n                                                                          'garage_spaces',\n                                                                          'number_of_bathrooms',\n                                                                          'number_of_bedrooms',\n                                                                          'number_stories',\n                                                                          'total_area',\n                                                                          'total_livable_area']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['exterior_condition',\n                                                                          'interior_condition',\n                                                                          'quality_grade',\n                                                                          'parcel_shape',\n                                                                          'view_type',\n                                                                          'year_built',\n                                                                          'listname'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid={'randomforestregressor__max_depth': [2, 5, 7, 9, 13,\n                                                              21, 33, 51],\n                         'randomforestregressor__n_estimators': [5, 10, 15, 20,\n                                                                 30, 50, 100,\n                                                                 200]},\n             verbose=1)estimator: PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['fireplaces',\n                                                   'garage_spaces',\n                                                   'number_of_bathrooms',\n                                                   'number_of_bedrooms',\n                                                   'number_stories',\n                                                   'total_area',\n                                                   'total_livable_area']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['exterior_condition',\n                                                   'interior_condition',\n                                                   'quality_grade',\n                                                   'parcel_shape', 'view_type',\n                                                   'year_built',\n                                                   'listname'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=42))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['fireplaces', 'garage_spaces',\n                                  'number_of_bathrooms', 'number_of_bedrooms',\n                                  'number_stories', 'total_area',\n                                  'total_livable_area']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['exterior_condition', 'interior_condition',\n                                  'quality_grade', 'parcel_shape', 'view_type',\n                                  'year_built', 'listname'])])num['fireplaces', 'garage_spaces', 'number_of_bathrooms', 'number_of_bedrooms', 'number_stories', 'total_area', 'total_livable_area']StandardScalerStandardScaler()cat['exterior_condition', 'interior_condition', 'quality_grade', 'parcel_shape', 'view_type', 'year_built', 'listname']OneHotEncoderOneHotEncoder(handle_unknown='ignore')RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\n\nCode\n# The best estimator\ngrid.best_estimator_\n\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['fireplaces',\n                                                   'garage_spaces',\n                                                   'number_of_bathrooms',\n                                                   'number_of_bedrooms',\n                                                   'number_stories',\n                                                   'total_area',\n                                                   'total_livable_area']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['exterior_condition',\n                                                   'interior_condition',\n                                                   'quality_grade',\n                                                   'parcel_shape', 'view_type',\n                                                   'year_built',\n                                                   'listname'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(max_depth=51, n_estimators=200,\n                                       random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['fireplaces',\n                                                   'garage_spaces',\n                                                   'number_of_bathrooms',\n                                                   'number_of_bedrooms',\n                                                   'number_stories',\n                                                   'total_area',\n                                                   'total_livable_area']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['exterior_condition',\n                                                   'interior_condition',\n                                                   'quality_grade',\n                                                   'parcel_shape', 'view_type',\n                                                   'year_built',\n                                                   'listname'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(max_depth=51, n_estimators=200,\n                                       random_state=42))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['fireplaces', 'garage_spaces',\n                                  'number_of_bathrooms', 'number_of_bedrooms',\n                                  'number_stories', 'total_area',\n                                  'total_livable_area']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['exterior_condition', 'interior_condition',\n                                  'quality_grade', 'parcel_shape', 'view_type',\n                                  'year_built', 'listname'])])num['fireplaces', 'garage_spaces', 'number_of_bathrooms', 'number_of_bedrooms', 'number_stories', 'total_area', 'total_livable_area']StandardScalerStandardScaler()cat['exterior_condition', 'interior_condition', 'quality_grade', 'parcel_shape', 'view_type', 'year_built', 'listname']OneHotEncoderOneHotEncoder(handle_unknown='ignore')RandomForestRegressorRandomForestRegressor(max_depth=51, n_estimators=200, random_state=42)\n\n\n\n\nCode\nnew_pipe = make_pipeline(transformer, RandomForestRegressor(max_depth=51, n_estimators=200,\n                                       random_state=42)\n)\n\n\n\n\nCode\n# Fit the training set\nnew_pipe.fit(train_set, y_train);\n\n\n\n\nCode\n# Test Score\nnew_pipe.score(test_set, y_test)\n\n\n0.5509458176502539\n\n\n\n\n2.5 Calculate the percent error of your model predictions for each sale in the test set\nFit your best model and use it to make predictions on the test set.\nNote: This should be the percent error in terms of sale price. You’ll need to convert if your model predicted the log of sales price!\n\n\nCode\n# this step is not necessary, just curious what the MAPE is. \n\ndef evaluate_mape(model, X_test, y_test):\n    \"\"\"\n    Given a model and test features/targets, print out the \n    mean absolute error and accuracy\n    \"\"\"\n    # Make the predictions\n    predictions = np.exp(model.predict(X_test))\n\n    # Percentage error\n    abs_errors = abs(predictions - y_test)\n    avg_error = np.mean(abs_errors)\n\n    # Mean absolute percentage error\n    mape = 100 * np.mean(abs_errors / y_test)\n\n\n    # Accuracy\n    accuracy = 100 - mape\n\n    print(\"Model Performance\")\n    print(f\"Average Absolute Error: {avg_error:0.4f}\")\n    print(f\"Accuracy = {accuracy:0.2f}%.\")\n\n    return accuracy\n\n\n\n\nCode\ny_test_no_log = np.exp(y_test)\n\n\n\n\nCode\nbase_accuracy = evaluate_mape(new_pipe, X_test, y_test_no_log)\n\n\nModel Performance\nAverage Absolute Error: 0.3368\nAccuracy = 97.12%.\n\n\n\n\nCode\n# This is the actual percent error calculation part \n\ndef percent_error(model, X_test, y_test):\n    \n    predictions = np.exp(model.predict(X_test))\n    \n    #percentage error\n    p_errors = 100 * (predictions - y_test) / y_test\n    \n    return p_errors\n\n\n\n\nCode\np_errors = percent_error(new_pipe, X_test, y_test_no_log)\n\n\n\n\n2.6 Make a data frame with percent errors and census tract info for each sale in the test set\nCreate a data frame that has the property geometries, census tract data, and percent errors for all of the sales in the test set.\nNotes\n\nWhen using the “train_test_split()” function, the index of the test data frame includes the labels from the original sales data frame\nYou can use this index to slice out the test data from the original sales data frame, which should include the census tract info and geometries\nAdd a new column to this data frame holding the percent error data\nMake sure to use the percent error and not the absolute percent error\n\n\n\nCode\ntest_df = joined.loc[test_set.index]\n\n\n\n\nCode\ntest_df['percent_error'] = p_errors\n\n\n\n\n2.8 Plot a map of the median percent error by census tract\n\nYou’ll want to group your data frame of test sales by the GEOID10 column and take the median of you percent error column\nMerge the census tract geometries back in and use geopandas to plot.\n\n\n\nCode\nmedian_p_errors = test_df.groupby('GEOID10')['percent_error'].median()\n\n\n\n\nCode\nmedian_p_errors = pd.DataFrame(median_p_errors).reset_index()\n\n\n\n\nCode\ngeo_median = tracts.merge(median_p_errors, on='GEOID10')\n\n\n\n\nCode\ngeo_median.explore(\n    column=\"qct\",\n    tooltip=\"qct\",\n    )\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nCode\ngeo_median.explore(\n    column=\"percent_error\",\n    tooltip=\"percent_error\",\n    popup=True,\n    cmap=\"coolwarm\",\n    tiles=\"CartoDB positron\",\n    legend=True\n    )\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n2.9 Compare the percent errors in Qualifying Census Tracts and other tracts\nQualifying Census Tracts are a poverty designation that HUD uses to allocate housing tax credits\n\nI’ve included a list of the census tract names that qualify in Philadelphia\nAdd a new column to your dataframe of test set sales that is True/False depending on if the tract is a QCT\nThen, group by this new column and calculate the median percent error\n\nYou should find that the algorithm’s accuracy is significantly worse in these low-income, qualifying census tracts\n\n\nCode\nqct = ['5',\n '20',\n '22',\n '28.01',\n '30.01',\n '30.02',\n '31',\n '32',\n '33',\n '36',\n '37.01',\n '37.02',\n '39.01',\n '41.01',\n '41.02',\n '56',\n '60',\n '61',\n '62',\n '63',\n '64',\n '65',\n '66',\n '67',\n '69',\n '70',\n '71.01',\n '71.02',\n '72',\n '73',\n '74',\n '77',\n '78',\n '80',\n '81.01',\n '81.02',\n '82',\n '83.01',\n '83.02',\n '84',\n '85',\n '86.01',\n '86.02',\n '87.01',\n '87.02',\n '88.01',\n '88.02',\n '90',\n '91',\n '92',\n '93',\n '94',\n '95',\n '96',\n '98.01',\n '100',\n '101',\n '102',\n '103',\n '104',\n '105',\n '106',\n '107',\n '108',\n '109',\n '110',\n '111',\n '112',\n '113',\n '119',\n '121',\n '122.01',\n '122.03',\n '131',\n '132',\n '137',\n '138',\n '139',\n '140',\n '141',\n '144',\n '145',\n '146',\n '147',\n '148',\n '149',\n '151.01',\n '151.02',\n '152',\n '153',\n '156',\n '157',\n '161',\n '162',\n '163',\n '164',\n '165',\n '167.01',\n '167.02',\n '168',\n '169.01',\n '169.02',\n '170',\n '171',\n '172.01',\n '172.02',\n '173',\n '174',\n '175',\n '176.01',\n '176.02',\n '177.01',\n '177.02',\n '178',\n '179',\n '180.02',\n '188',\n '190',\n '191',\n '192',\n '195.01',\n '195.02',\n '197',\n '198',\n '199',\n '200',\n '201.01',\n '201.02',\n '202',\n '203',\n '204',\n '205',\n '206',\n '208',\n '239',\n '240',\n '241',\n '242',\n '243',\n '244',\n '245',\n '246',\n '247',\n '249',\n '252',\n '253',\n '265',\n '267',\n '268',\n '271',\n '274.01',\n '274.02',\n '275',\n '276',\n '277',\n '278',\n '279.01',\n '279.02',\n '280',\n '281',\n '282',\n '283',\n '284',\n '285',\n '286',\n '287',\n '288',\n '289.01',\n '289.02',\n '290',\n '291',\n '293',\n '294',\n '298',\n '299',\n '300',\n '301',\n '302',\n '305.01',\n '305.02',\n '309',\n '311.01',\n '312',\n '313',\n '314.01',\n '314.02',\n '316',\n '318',\n '319',\n '321',\n '325',\n '329',\n '330',\n '337.01',\n '345.01',\n '357.01',\n '376',\n '377',\n '380',\n '381',\n '382',\n '383',\n '389',\n '390']\n\n\n\n\nCode\ngeo_median.head()\n\n\n\n\n\n\n\n\n\nOBJECTID\nSTATEFP10\nCOUNTYFP10\nTRACTCE10\nGEOID10\nNAME10\nNAMELSAD10\nMTFCC10\nFUNCSTAT10\nALAND10\nAWATER10\nINTPTLAT10\nINTPTLON10\nLOGRECNO\ngeometry\npercent_error\nqct\n\n\n\n\n0\n1\n42\n101\n009400\n42101009400\n94\nCensus Tract 94\nG5020\nS\n366717\n0\n+39.9632709\n-075.2322437\n10429\nPOLYGON ((-75.22927 39.96054, -75.22865 39.960...\n1.476289\nTrue\n\n\n1\n2\n42\n101\n009500\n42101009500\n95\nCensus Tract 95\nG5020\nS\n319070\n0\n+39.9658709\n-075.2379140\n10430\nPOLYGON ((-75.23536 39.96852, -75.23545 39.969...\n1.438452\nTrue\n\n\n2\n3\n42\n101\n009600\n42101009600\n96\nCensus Tract 96\nG5020\nS\n405273\n0\n+39.9655396\n-075.2435075\n10431\nPOLYGON ((-75.24343 39.96230, -75.24339 39.962...\n2.156239\nTrue\n\n\n3\n4\n42\n101\n013800\n42101013800\n138\nCensus Tract 138\nG5020\nS\n341256\n0\n+39.9764504\n-075.1771771\n10468\nPOLYGON ((-75.17341 39.97779, -75.17386 39.977...\n-0.349019\nTrue\n\n\n4\n6\n42\n101\n014000\n42101014000\n140\nCensus Tract 140\nG5020\nS\n439802\n0\n+39.9735358\n-075.1630966\n10470\nPOLYGON ((-75.16141 39.97044, -75.16056 39.970...\n1.107620\nTrue\n\n\n\n\n\n\n\n\n\nCode\ngeo_median['qct'] = geo_median['NAME10'].isin(qct)\n\ngeo_median.groupby('qct')['percent_error'].median()\n\n\nqct\nFalse   -0.563257\nTrue    -0.135236\nName: percent_error, dtype: float64"
  }
]