[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "My name is Ann Zi’an Zhang, currently a student at University of Pennsylvania studying city planning and urban spatial analytics. I am interested in data-driven problem-solving, especially in urban context.\nPrior to Penn, I graduated from Wesleyan University in Connecticut, U.S., majoring in Art History, Psychology, and Science in Society Program. On my way to realize the “practical idealism” !"
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium."
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "analysis/Ass-2.html",
    "href": "analysis/Ass-2.html",
    "title": "Where to spend leisure time in NYC?",
    "section": "",
    "text": "---\nformat:\n    html:\n        toc: false\n        page-layout: full\nexecute:\n        echo: false\n\n---"
  },
  {
    "objectID": "analysis/Ass-2.html#imports",
    "href": "analysis/Ass-2.html#imports",
    "title": "Where to spend leisure time in NYC?",
    "section": "Imports",
    "text": "Imports\n\nimport altair as alt\nimport geopandas as gpd\nimport hvplot.pandas\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport datetime\nimport math\n\n%matplotlib inline"
  },
  {
    "objectID": "analysis/Ass-2.html#nyc-base-maps",
    "href": "analysis/Ass-2.html#nyc-base-maps",
    "title": "Where to spend leisure time in NYC?",
    "section": "NYC Base Maps",
    "text": "NYC Base Maps\n\nTracts, Neighborhood, and Borough\n\nnyc_tracts = pd.read_csv(\"data/2022 Census Tracts.csv\")\nnyc_tracts['geometry'] = gpd.GeoSeries.from_wkt(nyc_tracts['the_geom'])\ngeo_tracts = gpd.GeoDataFrame(nyc_tracts, geometry='geometry')\ngeo_tracts = geo_tracts.set_crs(epsg=4326)\ntracts = geo_tracts[['BoroName', 'CT2020', 'BoroCT2020', 'NTAName', 'Shape_Area','geometry','GEOID']].copy()\ntracts_clean = tracts[['BoroName', 'CT2020', 'NTAName', 'geometry','GEOID']]\nBoro_NTA = tracts_clean[['BoroName', 'NTAName']].drop_duplicates(subset = \"NTAName\")"
  },
  {
    "objectID": "analysis/Ass-2.html#dataset-selection-setup",
    "href": "analysis/Ass-2.html#dataset-selection-setup",
    "title": "Where to spend leisure time in NYC?",
    "section": "Dataset Selection & Setup",
    "text": "Dataset Selection & Setup\nAfter glancing through NYC Open Data portal, I have selected art galleries, museums, libraries, theatres, and parks as common leisure spaces. In addition, I explored data of open streets, a program thatt had been implemented in many global cities including New York City to transform roads into public spaces for cultural and all kinds of events on particular days (mostly on weekends).\nThis step is to bring in all datasets, clean them up, and aggregate different types of leisure spaces. The aggregated dataframe is then spacially joined with tracts, neighborhood, and boroughs, ready for further investigation on their geospatial distributions.\nSimilar data wrangling is performed on parks. Since parks come in polygon instead of points, which may result in problems with spatial joins, in the case when one park falls in two or more tracts or neighborhood. Hence, the geometry of parks’ centroids is adopted to replace the polygon geometry for further analysis.\nThe Open Street data comes with more detailed information on the approved time for each street. The days of week (e.g. Monday) and time of the day (e.g. 7:30) that it opens and closes are rearranged by melting and pivoting.\nAdditionally, 2020 Census Data on population and median household income are brought in for analysis.\n\nIndoor Leisure Spaces: Art Galleries, Museums, Libraries, and Theatres\n\n# establish geodataframe\nart_galleries = gpd.read_file(\"data/art galleries.geojson\")\ngeo_art_galleries = gpd.GeoDataFrame(art_galleries, geometry='geometry')\ngeo_art_galleries = art_galleries.set_crs(epsg=4326)\n\n# add type\nart_galleries_clean = geo_art_galleries[['name','zip','address1','geometry']]\nart_galleries_clean.loc[:,\"Type\"]= \"Art Galleries\"\nart_galleries_clean.rename(\n    columns={\"address1\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1429971093.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  art_galleries_clean.rename(\n\n\n\n# Museums \n\n# establish geodataframe\nmuseums = gpd.read_file(\"data/museums.geojson\")\ngeo_museums= gpd.GeoDataFrame(museums, geometry='geometry')\ngeo_museums = geo_museums.set_crs(epsg=4326)\ngeo_museums\n\n# add type\nmuseums_clean = geo_museums[['name','zip','adress1','geometry']]\nmuseums_clean.loc[:,\"Type\"]= \"Museums\"\nmuseums_clean.rename(\n    columns={\"adress1\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\nmuseums_clean\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/3071166290.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  museums_clean.rename(\n\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\n\n\n\n\n0\nAlexander Hamilton U.S. Custom House\n10004.0\n1 Bowling Grn\nPOINT (-74.01376 40.70382)\nMuseums\n\n\n1\nAlice Austen House Museum\n10305.0\n2 Hylan Blvd\nPOINT (-74.06303 40.61512)\nMuseums\n\n\n2\nAmerican Academy of Arts and Letters\n10032.0\n633 W. 155th St.\nPOINT (-73.94730 40.83385)\nMuseums\n\n\n3\nAmerican Folk Art Museum\n10019.0\n45 West 53rd Street\nPOINT (-73.97810 40.76162)\nMuseums\n\n\n4\nAmerican Immigration History Center\n0.0\nEllis Island\nPOINT (-74.03968 40.69906)\nMuseums\n\n\n...\n...\n...\n...\n...\n...\n\n\n125\nAmerican Sephardi Federation / Sephardic House\n10011.0\n15 W. 16th St.\nPOINT (-73.99389 40.73808)\nMuseums\n\n\n126\nYIVO Institute for Jewish Research\n10011.0\n15 W. 16th St.\nPOINT (-73.99379 40.73796)\nMuseums\n\n\n127\nAmerican Jewish Historical Society\n10011.0\n15 W. 16th St.\nPOINT (-73.99393 40.73802)\nMuseums\n\n\n128\nYeshiva University Museum\n10011.0\n15 W. 16th St.\nPOINT (-73.99382 40.73805)\nMuseums\n\n\n129\nCenter For Jewish History\n10011.0\n15 W. 16th St.\nPOINT (-73.99387 40.73799)\nMuseums\n\n\n\n\n130 rows × 5 columns\n\n\n\n\n# Libraries\n\n# establish geodataframe\nlibrary = gpd.read_file(\"data/libraries.geojson\")\ngeo_library = gpd.GeoDataFrame(library, geometry='geometry')\ngeo_library= geo_library.set_crs(epsg=4326)\ngeo_library\n\n# add type\nlibrary_clean = geo_library[['name','zip','streetname','geometry']]\nlibrary_clean.loc[:,\"Type\"]= \"Libraries\"\nlibrary_clean.rename(\n    columns={\"streetname\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\nlibrary_clean\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/3524914390.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  library_clean.rename(\n\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\n\n\n\n\n0\n115th Street\n10026\nWest 115th Street\nPOINT (-73.95353 40.80298)\nLibraries\n\n\n1\n125th Street\n10035\nEast 125th Street\nPOINT (-73.93485 40.80302)\nLibraries\n\n\n2\n53rd Street\n10019\nWest 53rd Street\nPOINT (-73.97736 40.76081)\nLibraries\n\n\n3\n58th Street\n10022\nEast 58th Street\nPOINT (-73.96938 40.76219)\nLibraries\n\n\n4\n67th Street\n10065\nEast 67th Street\nPOINT (-73.95955 40.76492)\nLibraries\n\n\n...\n...\n...\n...\n...\n...\n\n\n211\nSunnyside\n11104\nGreenpoint Avenue\nPOINT (-73.92167 40.74085)\nLibraries\n\n\n212\nWhitestone\n11357\n14 Road\nPOINT (-73.81070 40.78854)\nLibraries\n\n\n213\nWindsor Park\n11364\nBell Boulevard\nPOINT (-73.75562 40.73450)\nLibraries\n\n\n214\nWoodhaven\n11421\nForest Parkway\nPOINT (-73.86146 40.69453)\nLibraries\n\n\n215\nWoodside\n11377\nSkillman Avenue\nPOINT (-73.90979 40.74534)\nLibraries\n\n\n\n\n216 rows × 5 columns\n\n\n\n\n# Theaters\n# establish geodataframe\ntheaters = gpd.read_file(\"data/Theaters.geojson\")\ngeo_theaters = gpd.GeoDataFrame(theaters, geometry='geometry')\ngeo_theaters= geo_theaters.set_crs(epsg=4326)\ngeo_theaters\n\n\n# add type\ntheaters_clean = geo_theaters[['name','zip','address1','geometry']]\ntheaters_clean.loc[:,\"Type\"]= \"Theatres\"\ntheaters_clean.rename(\n    columns={\"address1\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\ntheaters_clean\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1518346627.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  theaters_clean.rename(\n\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\n\n\n\n\n0\n45th Street Theater\n10036.0\n354 West 45th Street\nPOINT (-73.99062 40.75985)\nTheatres\n\n\n1\n47th Street Theater\n10036.0\n304 West 47th Street\nPOINT (-73.98811 40.76047)\nTheatres\n\n\n2\n59E59\n10022.0\n59 East 59th Street\nPOINT (-73.97038 40.76340)\nTheatres\n\n\n3\nAcorn Theater\n10036.0\n410 West 42nd Street\nPOINT (-73.99332 40.75854)\nTheatres\n\n\n4\nAl Hirschfeld Theater\n10036.0\n302 W 45th Street\nPOINT (-73.98921 40.75926)\nTheatres\n\n\n...\n...\n...\n...\n...\n...\n\n\n112\nWestside Theater\n10036.0\n407 W 43rd St\nPOINT (-73.99255 40.75953)\nTheatres\n\n\n113\nWings Theatre\n10014.0\n154 Christopher St\nPOINT (-74.00889 40.73240)\nTheatres\n\n\n114\nWinter Garden Theatre\n10019.0\n1634 Broadway\nPOINT (-73.98348 40.76152)\nTheatres\n\n\n115\nYork Theatre\n10022.0\n619 Lexington Ave\nPOINT (-73.96998 40.75836)\nTheatres\n\n\n116\nDelacorte Theater\n0.0\nCentral Park - Mid-Park at 80th Street\nPOINT (-73.96882 40.78018)\nTheatres\n\n\n\n\n117 rows × 5 columns\n\n\n\n\nAggregate all types of indoor leisure space\n\nframes = [art_galleries_clean, museums_clean, library_clean, theaters_clean]\n\ntotal_indoor = pd.concat(frames)\n\ntotal_indoor\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\n\n\n\n\n0\nO'reilly William & Co Ltd\n10021.0\n52 E 76th St\nPOINT (-73.96273 40.77380)\nArt Galleries\n\n\n1\nOrganization of Independent Artists - Gallery 402\n10013.0\n19 Hudson St.\nPOINT (-74.00939 40.71647)\nArt Galleries\n\n\n2\nOwen Gallery\n10021.0\n19 E 75th St\nPOINT (-73.96435 40.77400)\nArt Galleries\n\n\n3\nP P O W Gallerie\n10001.0\n511 W 25th St\nPOINT (-74.00389 40.74959)\nArt Galleries\n\n\n4\nP P O W Inc\n10013.0\n476 Broome St\nPOINT (-74.00176 40.72291)\nArt Galleries\n\n\n...\n...\n...\n...\n...\n...\n\n\n112\nWestside Theater\n10036.0\n407 W 43rd St\nPOINT (-73.99255 40.75953)\nTheatres\n\n\n113\nWings Theatre\n10014.0\n154 Christopher St\nPOINT (-74.00889 40.73240)\nTheatres\n\n\n114\nWinter Garden Theatre\n10019.0\n1634 Broadway\nPOINT (-73.98348 40.76152)\nTheatres\n\n\n115\nYork Theatre\n10022.0\n619 Lexington Ave\nPOINT (-73.96998 40.75836)\nTheatres\n\n\n116\nDelacorte Theater\n0.0\nCentral Park - Mid-Park at 80th Street\nPOINT (-73.96882 40.78018)\nTheatres\n\n\n\n\n1380 rows × 5 columns\n\n\n\n\n\nSpatial Join\n\ngeo_total_indoor = gpd.sjoin(\n    total_indoor,  # The point data for 311 tickets\n    tracts_clean.to_crs(total_indoor.crs),  # The neighborhoods (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\ngeo_total_indoor.head()\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\nindex_right\nBoroName\nCT2020\nNTAName\nGEOID\n\n\n\n\n0\nO'reilly William & Co Ltd\n10021.0\n52 E 76th St\nPOINT (-73.96273 40.77380)\nArt Galleries\n84.0\nManhattan\n13000.0\nUpper East Side-Carnegie Hill\n3.606101e+10\n\n\n1\nOrganization of Independent Artists - Gallery 402\n10013.0\n19 Hudson St.\nPOINT (-74.00939 40.71647)\nArt Galleries\n17.0\nManhattan\n3900.0\nTribeca-Civic Center\n3.606100e+10\n\n\n2\nOwen Gallery\n10021.0\n19 E 75th St\nPOINT (-73.96435 40.77400)\nArt Galleries\n84.0\nManhattan\n13000.0\nUpper East Side-Carnegie Hill\n3.606101e+10\n\n\n3\nP P O W Gallerie\n10001.0\n511 W 25th St\nPOINT (-74.00389 40.74959)\nArt Galleries\n1134.0\nManhattan\n9901.0\nChelsea-Hudson Yards\n3.606101e+10\n\n\n4\nP P O W Inc\n10013.0\n476 Broome St\nPOINT (-74.00176 40.72291)\nArt Galleries\n1156.0\nManhattan\n4900.0\nSoHo-Little Italy-Hudson Square\n3.606100e+10\n\n\n\n\n\n\n\n\n\n\nOutdoor Leisure Space: Parks\n\n# Parks\n# establish geodataframe\nparks = gpd.read_file(\"data/Parks Properties.geojson\")\ngeo_parks = gpd.GeoDataFrame(parks, geometry='geometry')\ngeo_parks= geo_parks.set_crs(epsg=4326)\ngeo_parks\n\n\n# add type\nparks_clean = geo_parks[['signname','geometry','acres']]\nparks_clean.loc[:,\"Type\"]= \"Parks\"\nparks_clean.rename(\n    columns={\"location\": \"Address\", \"signname\": \"Name\", \"zipcode\": \"Zip\"},\n    inplace=True,)\n\nparks_clean['new_geom'] = parks_clean['geometry']\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1177565529.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  parks_clean.rename(\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n\n\n\n\n# Since many parks are in multi-polygons, which will lead to errors when doing spatial join later, a centroid of each park is generated here for smoother spatial join.\nparks_clean['geometry'] = parks_clean['geometry'].centroid\n\ngeo_parks_clean = gpd.sjoin(\n    parks_clean,  # The point data for 311 tickets\n    tracts_clean.to_crs(parks_clean.crs),  # The neighborhoods (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\ngeo_parks_clean\n\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/3082874007.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  parks_clean['geometry'] = parks_clean['geometry'].centroid\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n\n\n\n\n\n\n\n\n\nName\ngeometry\nacres\nType\nnew_geom\nindex_right\nBoroName\nCT2020\nNTAName\nGEOID\n\n\n\n\n0\nInwood Hill Park\nPOINT (-73.92544 40.87257)\n196.398\nParks\nMULTIPOLYGON (((-73.92093 40.86999, -73.92145 ...\n2191.0\nManhattan\n29700.0\nInwood Hill Park\n3.606103e+10\n\n\n1\nChallenge Playground\nPOINT (-73.72796 40.75662)\n2.035\nParks\nMULTIPOLYGON (((-73.72738 40.75605, -73.72783 ...\n936.0\nQueens\n152902.0\nDouglaston-Little Neck\n3.608115e+10\n\n\n2\nSunset Cove Park\nPOINT (-73.82300 40.59853)\n9.375\nParks\nMULTIPOLYGON (((-73.82218 40.59892, -73.82221 ...\n1129.0\nQueens\n107201.0\nBreezy Point-Belle Harbor-Rockaway Park-Broad ...\n3.608111e+10\n\n\n3\nGrand Central Parkway Extension\nPOINT (-73.85317 40.75316)\n249.389\nParks\nMULTIPOLYGON (((-73.85875 40.76741, -73.85976 ...\n625.0\nQueens\n39902.0\nNorth Corona\n3.608104e+10\n\n\n4\nIdlewild Park\nPOINT (-73.75229 40.65043)\n180.85\nParks\nMULTIPOLYGON (((-73.75809 40.65427, -73.75845 ...\n1013.0\nQueens\n66404.0\nSpringfield Gardens (South)-Brookville\n3.608107e+10\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2040\nMaria Hernandez Park\nPOINT (-73.92386 40.70317)\n6.873\nParks\nMULTIPOLYGON (((-73.92251 40.70351, -73.92381 ...\n1729.0\nBrooklyn\n42900.0\nBushwick (West)\n3.604704e+10\n\n\n2041\nCrotona Parkway Malls\nPOINT (-73.88477 40.84405)\n8.75\nParks\nMULTIPOLYGON (((-73.88496 40.84470, -73.88496 ...\n336.0\nBronx\n36300.0\nWest Farms\n3.600504e+10\n\n\n2042\nPark\nPOINT (-73.89807 40.84408)\n0.511\nParks\nMULTIPOLYGON (((-73.89759 40.84410, -73.89773 ...\n1229.0\nBronx\n16500.0\nClaremont Village-Claremont (East)\n3.600502e+10\n\n\n2043\nCunningham Park\nPOINT (-73.76880 40.73382)\n358.0\nParks\nMULTIPOLYGON (((-73.77466 40.72442, -73.77439 ...\n2241.0\nQueens\n128300.0\nCunningham Park\n3.608113e+10\n\n\n2044\nRoberto Clemente Ballfield\nPOINT (-73.96767 40.70635)\n1.93\nParks\nMULTIPOLYGON (((-73.96761 40.70581, -73.96735 ...\n2220.0\nBrooklyn\n54500.0\nSouth Williamsburg\n3.604705e+10\n\n\n\n\n2045 rows × 10 columns\n\n\n\n\n\nOutdoor Leisure Space: Open Streets\n\nopen_streets = gpd.read_file(\"data/Open Streets Locations.geojson\")\ngeo_open_streets = gpd.GeoDataFrame(open_streets, geometry='geometry', crs=2263)\ngeo_open_streets = open_streets.to_crs(epsg=4326)\n\nopen_streets_clean = geo_open_streets[['appronstre','apprtostre','apprdayswe','boroughname','reviewstat','shape_stle','geometry']]\n\n\njoin_test = gpd.sjoin(\n    open_streets_clean,  # The point data for 311 tickets\n    tracts_clean.to_crs(open_streets_clean.crs),  # The neighborhoods (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\njoin_test.head()\n\n\n\n\n\n\n\n\nappronstre\napprtostre\napprdayswe\nboroughname\nreviewstat\nshape_stle\ngeometry\nindex_right\nBoroName\nCT2020\nNTAName\nGEOID\n\n\n\n\n0\nDEISIUS STREET\nSTECHER STREET\nmon,tue,wed,thu,fri\nStaten Island\napprovedFullSchools\n264.932398036\nMULTILINESTRING ((-74.18738 40.53028, -74.1882...\n1318.0\nStaten Island\n17600.0\nAnnadale-Huguenot-Prince's Bay-Woodrow\n3.608502e+10\n\n\n1\nSUFFOLK AVENUE\nHAROLD STREET\nfri,sat,sun\nStaten Island\napprovedFull\n313.087821487\nMULTILINESTRING ((-74.12784 40.60288, -74.1277...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n\n\n2\nSUFFOLK AVENUE\nHAROLD STREET\nfri,sat,sun\nStaten Island\napprovedFull\n142.063500219\nMULTILINESTRING ((-74.12772 40.60202, -74.1276...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n\n\n3\nVERMONT COURT\nSUFFOLK AVENUE\nfri,sat,sun\nStaten Island\napprovedFull\n421.392020366\nMULTILINESTRING ((-74.12620 40.60209, -74.1277...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n\n\n4\n9 STREET\nROSE AVENUE\nfri,sat,sun\nStaten Island\napprovedFull\n448.103939286\nMULTILINESTRING ((-74.11481 40.57316, -74.1157...\n1300.0\nStaten Island\n13400.0\nNew Dorp-Midland Beach\n3.608501e+10\n\n\n\n\n\n\n\n\njoin_test['monday'] = join_test['apprdayswe'].str.count('mon')\njoin_test['tuesday'] = join_test['apprdayswe'].str.count('tue')\njoin_test['wednesday'] = join_test['apprdayswe'].str.count('wed')\njoin_test['thursday'] = join_test['apprdayswe'].str.count('thu')\njoin_test['friday'] = join_test['apprdayswe'].str.count('fri')\njoin_test['saturday'] = join_test['apprdayswe'].str.count('sat')\njoin_test['sunday'] = join_test['apprdayswe'].str.count('sun')\njoin_test['dayscount'] = join_test[['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']].sum(axis=1)\n\njoin_test\n\n\n\n\n\n\n\n\nappronstre\napprtostre\napprdayswe\nboroughname\nreviewstat\nshape_stle\ngeometry\nindex_right\nBoroName\nCT2020\nNTAName\nGEOID\nmonday\ntuesday\nwednesday\nthursday\nfriday\nsaturday\nsunday\ndayscount\n\n\n\n\n0\nDEISIUS STREET\nSTECHER STREET\nmon,tue,wed,thu,fri\nStaten Island\napprovedFullSchools\n264.932398036\nMULTILINESTRING ((-74.18738 40.53028, -74.1882...\n1318.0\nStaten Island\n17600.0\nAnnadale-Huguenot-Prince's Bay-Woodrow\n3.608502e+10\n1\n1\n1\n1\n1\n0\n0\n5\n\n\n1\nSUFFOLK AVENUE\nHAROLD STREET\nfri,sat,sun\nStaten Island\napprovedFull\n313.087821487\nMULTILINESTRING ((-74.12784 40.60288, -74.1277...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n0\n0\n0\n0\n1\n1\n1\n3\n\n\n2\nSUFFOLK AVENUE\nHAROLD STREET\nfri,sat,sun\nStaten Island\napprovedFull\n142.063500219\nMULTILINESTRING ((-74.12772 40.60202, -74.1276...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n0\n0\n0\n0\n1\n1\n1\n3\n\n\n3\nVERMONT COURT\nSUFFOLK AVENUE\nfri,sat,sun\nStaten Island\napprovedFull\n421.392020366\nMULTILINESTRING ((-74.12620 40.60209, -74.1277...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n0\n0\n0\n0\n1\n1\n1\n3\n\n\n4\n9 STREET\nROSE AVENUE\nfri,sat,sun\nStaten Island\napprovedFull\n448.103939286\nMULTILINESTRING ((-74.11481 40.57316, -74.1157...\n1300.0\nStaten Island\n13400.0\nNew Dorp-Midland Beach\n3.608501e+10\n0\n0\n0\n0\n1\n1\n1\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\nBECK STREET\nAVENUE ST JOHN\nwed\nBronx\napprovedFull\n619.314479576\nMULTILINESTRING ((-73.90222 40.81449, -73.9014...\n200.0\nBronx\n8300.0\nLongwood\n3.600501e+10\n0\n0\n1\n0\n0\n0\n0\n1\n\n\n362\nNEWKIRK AVENUE\nEAST 17 STREET\nsun\nBrooklyn\napprovedFull\n284.786787549\nMULTILINESTRING ((-73.96422 40.63510, -73.9641...\n1807.0\nBrooklyn\n52000.0\nFlatbush (West)-Ditmas Park-Parkville\n3.604705e+10\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n363\n18 STREET\n4 AVENUE\nmon,tue,wed,thu,fri\nBrooklyn\napprovedFullSchools\n768.660222317\nMULTILINESTRING ((-73.99201 40.66323, -73.9916...\n2248.0\nBrooklyn\n14300.0\nSunset Park (West)\n3.604701e+10\n1\n1\n1\n1\n1\n0\n0\n5\n\n\n364\n34 AVENUE\nJUNCTION BOULEVARD\nmon,tue,wed,thu,fri,sat,sun\nQueens\napprovedLimited\n272.995599601\nMULTILINESTRING ((-73.89716 40.75246, -73.8970...\n2168.0\nQueens\n29100.0\nJackson Heights\n3.608103e+10\n1\n1\n1\n1\n1\n1\n1\n7\n\n\n365\nDECATUR STREET\nSARATOGA AVENUE\nsat\nBrooklyn\napprovedLimited\n769.964464111\nMULTILINESTRING ((-73.92001 40.68299, -73.9176...\n1682.0\nBrooklyn\n37700.0\nBedford-Stuyvesant (East)\n3.604704e+10\n0\n0\n0\n0\n0\n1\n0\n1\n\n\n\n\n366 rows × 20 columns\n\n\n\n\nopen_streets_days = join_test[['appronstre', 'BoroName', 'monday', 'tuesday', 'wednesday','thursday', 'friday', 'saturday', 'sunday']]\nopen_streets_days\n\n\n\n\n\n\n\n\nappronstre\nBoroName\nmonday\ntuesday\nwednesday\nthursday\nfriday\nsaturday\nsunday\n\n\n\n\n0\nDEISIUS STREET\nStaten Island\n1\n1\n1\n1\n1\n0\n0\n\n\n1\nSUFFOLK AVENUE\nStaten Island\n0\n0\n0\n0\n1\n1\n1\n\n\n2\nSUFFOLK AVENUE\nStaten Island\n0\n0\n0\n0\n1\n1\n1\n\n\n3\nVERMONT COURT\nStaten Island\n0\n0\n0\n0\n1\n1\n1\n\n\n4\n9 STREET\nStaten Island\n0\n0\n0\n0\n1\n1\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\nBECK STREET\nBronx\n0\n0\n1\n0\n0\n0\n0\n\n\n362\nNEWKIRK AVENUE\nBrooklyn\n0\n0\n0\n0\n0\n0\n1\n\n\n363\n18 STREET\nBrooklyn\n1\n1\n1\n1\n1\n0\n0\n\n\n364\n34 AVENUE\nQueens\n1\n1\n1\n1\n1\n1\n1\n\n\n365\nDECATUR STREET\nBrooklyn\n0\n0\n0\n0\n0\n1\n0\n\n\n\n\n366 rows × 9 columns\n\n\n\n\n\nCensus Data\n\nPop_2020 = gpd.read_file(\"data/NYC_tracts_2020.geojson\")\nPop_2020 = Pop_2020[['GEOID', 'estimate']]\nPop_2020['GEOID']=Pop_2020['GEOID'].astype(int)\n\ntracts_pop = tracts_clean.merge(Pop_2020, on='GEOID', how='left') \nneighbor_pop = tracts_pop.groupby(['NTAName']).sum(['estimate']).reset_index()\n\n\nincome = gpd.read_file(\"data/NYC_Income.geojson\")\ngeo_income = gpd.GeoDataFrame(income, geometry='geometry')\ngeo_income = geo_income.set_crs(epsg=4326)\ngeo_income['GEOID']=geo_income['GEOID'].astype('int')\n\ntracts_income = tracts_clean.merge(geo_income.drop(columns='geometry'), on='GEOID', how='left').dropna()\nneighbor_income = tracts_income.groupby(['NTAName', 'BoroName']).median(['estimate']).reset_index()"
  },
  {
    "objectID": "analysis/Ass-2.html#chart-i-matplotlib-parks-in-neighborhoods",
    "href": "analysis/Ass-2.html#chart-i-matplotlib-parks-in-neighborhoods",
    "title": "Where to spend leisure time in NYC?",
    "section": "Chart I: Matplotlib – Parks in Neighborhoods",
    "text": "Chart I: Matplotlib – Parks in Neighborhoods\nI first hope to investigate into the distribution of Parks in different neighborhoods and boroughs in relation to population. On the one hand, higher population means more people will have need for a bigger public green space for leisure time. On the other hand, less populated neighborhoods tend to have more spaces for parks. And since the count of parks doesn’t perfectly reflect how much space is available, I am using acrage data instead of counts for this analysis.\nTo investigate, I utilized Matplotlib, which is great for making simple scatterplot charts that speaks for simple linear relationship, if there is any.\n\ngeo_parks_clean['acres'] = geo_parks_clean['acres'].astype(float)\nparks_acres_neighborhood = geo_parks_clean.groupby('NTAName').sum().drop(columns=['index_right','CT2020']).reset_index()\nparks_acres_pop = neighbor_pop.merge(parks_acres_neighborhood, on='NTAName', how='left').dropna()\nparks_pop = parks_acres_pop.merge(Boro_NTA, on='NTAName', how='left').dropna()\nparks_pop['acres'].describe()\n\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1268464147.py:2: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  parks_acres_neighborhood = geo_parks_clean.groupby('NTAName').sum().drop(columns=['index_right','CT2020']).reset_index()\n\n\ncount     237.000000\nmean      105.945812\nstd       224.643587\nmin         0.005000\n25%         7.651000\n50%        20.653000\n75%        85.527000\nmax      1930.636136\nName: acres, dtype: float64\n\n\nAfter a quick glance of the park data, I notice some extreme outliers with extremely large parks that not only serves adjacent neighborhoods but the whole city. I excluded those outliers to have a better sense of how much acrage of parks common neighborhoods get.\n\nparks_pop_filtered = parks_pop.loc[(parks_pop['acres'] &lt; 86) & (parks_pop['acres'] &gt; 7) & (parks_pop['estimate'] &gt;0)]\nparks_pop_filtered\n\n\n\n\n\n\n\n\nNTAName\nCT2020\nGEOID_x\nestimate\nacres\nGEOID_y\nBoroName\n\n\n\n\n5\nAstoria (East)-Woodside (North)\n241600\n505134241600\n34825.0\n8.642\n2.886482e+11\nQueens\n\n\n6\nAstoria (North)-Ditmars-Steinway\n214102\n613377214102\n47134.0\n11.530\n4.690532e+11\nQueens\n\n\n10\nBarren Island-Floyd Bennett Field\n70202\n36047070202\n26.0\n64.665\n3.604707e+10\nBrooklyn\n\n\n11\nBath Beach\n235800\n396517235800\n32716.0\n21.398\n1.081411e+11\nBrooklyn\n\n\n15\nBedford Park\n448115\n396055448115\n55702.0\n23.917\n1.800252e+11\nBronx\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n225\nWest Farms\n136300\n180025136300\n18206.0\n13.540\n3.960554e+11\nBronx\n\n\n230\nWhitestone-Beechhurst\n703300\n252567703300\n28353.0\n29.086\n2.164866e+11\nQueens\n\n\n232\nWilliamsburg\n695100\n468611695100\n59410.0\n81.282\n1.045365e+12\nBrooklyn\n\n\n233\nWindsor Terrace-South Slope\n363808\n288376363808\n25442.0\n8.408\n4.325650e+11\nBrooklyn\n\n\n235\nWoodside\n406004\n505134406004\n45417.0\n10.217\n5.412154e+11\nQueens\n\n\n\n\n113 rows × 7 columns\n\n\n\n\n\ncolor_map = {\"Bronx\": \"#550527\", \"Brooklyn\": \"#688E26\", \"Manhattan\": \"#FAA613\", \"Queens\": \"#F44708\", \"Staten Island\": \"#A10702\"}\n\nfig, ax = plt.subplots(figsize=(11,6))\n\nfor BoroName, group_df in parks_pop_filtered.groupby(\"BoroName\"):\n    \n    ax.scatter(\n        group_df[\"estimate\"],\n        group_df[\"acres\"],\n        marker=\"P\",\n        label=BoroName,\n        color=color_map[BoroName],\n        alpha=0.75,\n        zorder=10\n    )\n\nax.legend(loc=\"best\")\nax.set(\n    title = \"Park Space (acres) Relative to Population in Neighborhoods, by Boroughs\",\n    xlabel = \"Population in each neighborhood (2020 Census)\",\n    ylabel = \"Parks in Acres\")\nax.grid(True)\n\nplt.show()\n\n\n\n\nThis chart does not suggest a strong linear relationship between population and park acrages. Overall, Manhattan, Queens and Brooklyn host more large parks, but some neighborhoods are particularly underserved, with very high population and low park acrage (points towards lower right of the chart)."
  },
  {
    "objectID": "analysis/Ass-2.html#chart-ii-seaborn-x2",
    "href": "analysis/Ass-2.html#chart-ii-seaborn-x2",
    "title": "Where to spend leisure time in NYC?",
    "section": "Chart II: Seaborn (x2)",
    "text": "Chart II: Seaborn (x2)\nLooking at data for indoor leisure space and open street data, I utilize seaborn to create two types of charts that suite the nature of the data sets.\nThe first is a grouped bar charts. Similar to park distribution, I hope to have a quick glimpse of number of each type of leisure spaces in each borough, and hope to identify any general spatial patterns or inequalities. A grouped chart is great a revealing such pattern.\nThe second is a heatmap for open street data. The heatmap explores the number of open streets approved in different boroughs on different days of the week.\n\nIndoor Leisure Spaces: Grouped Bar Charts\n\nindoor_clean = geo_total_indoor.groupby(['Type','BoroName']).count().reset_index().drop(columns=['Zip','Address','geometry','index_right','CT2020','NTAName']).pivot(index='Type',columns='BoroName', values='Name').fillna(0).reset_index()\nindoor_melt = indoor_clean.melt(id_vars='Type', value_vars=['Bronx','Brooklyn','Manhattan', 'Queens', 'Staten Island'])\nindoor_melt\n\n\n\n\n\n\n\n\nType\nBoroName\nvalue\n\n\n\n\n0\nArt Galleries\nBronx\n6.0\n\n\n1\nLibraries\nBronx\n35.0\n\n\n2\nMuseums\nBronx\n8.0\n\n\n3\nTheatres\nBronx\n0.0\n\n\n4\nArt Galleries\nBrooklyn\n61.0\n\n\n5\nLibraries\nBrooklyn\n59.0\n\n\n6\nMuseums\nBrooklyn\n12.0\n\n\n7\nTheatres\nBrooklyn\n0.0\n\n\n8\nArt Galleries\nManhattan\n823.0\n\n\n9\nLibraries\nManhattan\n44.0\n\n\n10\nMuseums\nManhattan\n87.0\n\n\n11\nTheatres\nManhattan\n115.0\n\n\n12\nArt Galleries\nQueens\n24.0\n\n\n13\nLibraries\nQueens\n65.0\n\n\n14\nMuseums\nQueens\n12.0\n\n\n15\nTheatres\nQueens\n2.0\n\n\n16\nArt Galleries\nStaten Island\n3.0\n\n\n17\nLibraries\nStaten Island\n13.0\n\n\n18\nMuseums\nStaten Island\n9.0\n\n\n19\nTheatres\nStaten Island\n0.0\n\n\n\n\n\n\n\n\nsns.set_theme(style=\"whitegrid\")\n\ncolor_map = [\"#550527\", \"#688E26\", \"#FAA613\", \"#F44708\", \"#A10702\"]\nsns.set_palette(color_map)\n\nsns.catplot(\n    data=indoor_melt, kind=\"bar\",\n    x=\"Type\", \n    y=\"value\", \n    hue=\"BoroName\",\n    aspect=2, \n    alpha=1\n).set_axis_labels(\n    \"Type of Leisure Space\", \"Counts\"\n).set(title=\"Distribution of 4 Types of Leisure Spaces in Each Borough\")\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\nDays and Location of Open Streets in NYC: A Heatmap\n\n#heatmap for open_streets: borough x Days of the week \nopen_streets_days_melt= open_streets_days.melt(id_vars=['appronstre','BoroName'], value_vars=['monday','tuesday','wednesday', 'thursday', 'friday','saturday','sunday'])\n\n\n\n\n\n\n\n\nappronstre\nBoroName\nvariable\nvalue\n\n\n\n\n0\nDEISIUS STREET\nStaten Island\nmonday\n1\n\n\n1\nSUFFOLK AVENUE\nStaten Island\nmonday\n0\n\n\n2\nSUFFOLK AVENUE\nStaten Island\nmonday\n0\n\n\n3\nVERMONT COURT\nStaten Island\nmonday\n0\n\n\n4\n9 STREET\nStaten Island\nmonday\n0\n\n\n...\n...\n...\n...\n...\n\n\n2557\nBECK STREET\nBronx\nsunday\n0\n\n\n2558\nNEWKIRK AVENUE\nBrooklyn\nsunday\n1\n\n\n2559\n18 STREET\nBrooklyn\nsunday\n0\n\n\n2560\n34 AVENUE\nQueens\nsunday\n1\n\n\n2561\nDECATUR STREET\nBrooklyn\nsunday\n0\n\n\n\n\n2562 rows × 4 columns\n\n\n\n\nopen_streets_seaborn = open_streets_days_melt.groupby(['variable','BoroName']).sum().reset_index()\n\n# sort the order of day from monday to sunday \n\norder=['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\nopen_streets_seaborn['variable'] = pd.Categorical(open_streets_seaborn['variable'], categories=order, ordered=True)\nopen_streets_seaborn = open_streets_seaborn.sort_values(by='variable')\n\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/3045256453.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  open_streets_seaborn = open_streets_days_melt.groupby(['variable','BoroName']).sum().reset_index()\n\n\n\nimport seaborn as sns\n\nsns.set_theme()\n\n# Load the example flights dataset and convert to long-form\n\nopen_street_heatmap = (\n    open_streets_seaborn\n    .pivot(index=\"BoroName\", columns=\"variable\", values=\"value\")\n)\n\n# Draw a heatmap with the numeric values in each cell\nf, ax = plt.subplots(figsize=(9, 6))\n\nax = sns.heatmap(open_street_heatmap, annot=True, linewidths=.5)\nax.set(xlabel=\"Days in a week\", ylabel=\"Borough\", title=\"Number of Streets Open on Specific Days\")\n\n\n[Text(0.5, 33.249999999999986, 'Days in a week'),\n Text(79.75, 0.5, 'Borough'),\n Text(0.5, 1.0, 'Number of Streets Open on Specific Days')]\n\n\n\n\n\nFrom both charts, we can see Manhattan has disproportional number of art galleries and open streets in comparison to other boroughs. Museums and theatres are predominantly located on Manhattan as well. Interestingly, the distribution of libraries seems more even. However, overall, we are seeing leisure spaces are disproportional abundant and diverse in Manhattan, then brooklyn and queens, leaving Bronx and Staten Island less resourceful."
  },
  {
    "objectID": "analysis/Ass-2.html#chart-iii-altair-charts-x3",
    "href": "analysis/Ass-2.html#chart-iii-altair-charts-x3",
    "title": "Where to spend leisure time in NYC?",
    "section": "Chart III: Altair Charts (x3)",
    "text": "Chart III: Altair Charts (x3)\nFurthering the exploration on indoor leisure spaces, I created one chart on its ditribution in relation with median household income, to see if it embeds any socio-economic inequality. I then creatd a map to visualize this relationship, which can also be helpful to locate different kinds of leisure spaces in the city.\nMy third chart is an interactive bar chart on open streets in Brookylen, where audience can choose the day of the week to see all open streets approved for that day and the time they open and close. This could potentially be developed into a tool for residents and tourists to track open streets.\n\nBrush Selection: Indoor Space and Income\n\nneighbor_indoor = geo_total_indoor.groupby(['NTAName','Type']).count().reset_index().drop(['Zip', 'Address', 'geometry','index_right','BoroName', 'CT2020'], axis=1)\nneighbor_indoor = neighbor_indoor.rename(columns={'Name': 'Count'})\nneighbor_income_indoor = neighbor_income.merge(neighbor_indoor, on='NTAName', how='left')\n\nneighbor_income_indoor_pivot=neighbor_income_indoor.pivot(index=['NTAName', 'BoroName','estimate'], columns=\"Type\", values=\"Count\").reset_index().fillna(0)\nneighbor_income_indoor_pivot_1 = neighbor_income_indoor_pivot.drop(neighbor_income_indoor_pivot.columns[[3]],axis=1)\nincome_indoor_pivot = neighbor_income_indoor_pivot_1.melt(id_vars=[\"NTAName\", \"estimate\", \"BoroName\"], value_vars=[\"Art Galleries\", \"Libraries\", \"Museums\", \"Theatres\"], var_name=\"Types\",value_name=\"Count\")\nincome_indoor_pivot_filtered = income_indoor_pivot.loc[(income_indoor_pivot['Count'] &lt;20)]\nincome_indoor_pivot_filtered = income_indoor_pivot_filtered.rename(columns={'estimate': 'Median Household Income'})\n\n\nbrush = alt.selection_interval()\n\nBrush_Chart = (\nalt.Chart(income_indoor_pivot_filtered)\n   .mark_point()\n   .encode(\n       x=alt.X(\"Median Household Income:Q\", scale=alt.Scale(zero=False)),\n       y=alt.Y(\"Count:Q\", scale=alt.Scale(zero=False)),\n       color=alt.condition(brush, \"BoroName:N\", alt.value(\"lightgray\")),\n       tooltip=[\"NTAName\",\"BoroName:N\", \"Median Household Income:Q\", \"Count:Q\"])\n   .add_params(brush)\n   .properties(width=200, height=200)\n   .facet(column=\"Types:N\")\n)\n\nBrush_Chart\n\n\n\n\n\n\n\n\n\nMap: Income and Indoor Leisure Spaces - Relationships\n\nNTA = pd.read_csv(\"data/2020 Neighborhood.csv\")\nNTA['geometry'] = gpd.GeoSeries.from_wkt(NTA['the_geom'])\nNTA_geo = gpd.GeoDataFrame(NTA, geometry='geometry', crs=4326)\nNTA_geo = NTA_geo.to_crs(epsg=2263)\ngeo_total_indoor = geo_total_indoor.to_crs(epsg=2263)\n\ngeo_NTA = NTA_geo[['NTAName', 'geometry']]\n\ntracts_income_1 = tracts_clean.merge(geo_income.drop(columns='geometry'), on='GEOID', how='left').dropna()\nneighbor_income_1 = tracts_income.groupby(['NTAName', 'BoroName']).median(['estimate']).reset_index()\n\nNTA_income = geo_NTA.merge(neighbor_income_1, on='NTAName', how='left')\n\n\ngeo_total_indoor_1 = geo_total_indoor\n\ngeo_total_indoor_1['lon'] = geo_total_indoor_1['geometry'].x\ngeo_total_indoor_1['lat'] = geo_total_indoor_1['geometry'].y\n\n\nIncome = (\n    alt.Chart(NTA_income)\n    .mark_geoshape(stroke=\"white\")\n    .encode(\n        tooltip=[\"NTAName:N\", \"estimate:Q\", \"moe:Q\"],\n        color=alt.Color(\"estimate:Q\", scale=alt.Scale(scheme=\"greys\")),\n    )\n    # Important! Otherwise altair will try to re-project your data\n    .project(type=\"identity\", reflectY=True)\n    .properties(width=1000, height=800).interactive()\n)\n\nIndoorSpaces = (\n    alt.Chart(geo_total_indoor_1)\n    .mark_circle(size=10)\n    .encode(tooltip=['Name','Type','Address'],\n           longitude=\"lon\", latitude=\"lat\",\n           color=alt.Color('Type:N', scale=alt.Scale(scheme=\"lightmulti\"))\n         ).project(type=\"identity\", reflectY=True)\n)\n\n\n\nmap_1 = Income + IndoorSpaces\nmap_1\n\n\n\n\n\n\n\n\nSimilarly, libraries seem to be the least discriminatory type of leisure space. For art galleries and museums, while there are some distributed in mid to lower income neighborhood, the higher income neighborhood has higher density of such leisure space. Theatres, at the same time, is mostly located in mid to higher income neighborhood. This trend can be clearly see on the map too. Such spatial distribution means, for residents in lower to mid income neighborhood, they might have to travel further for accessing those spaces.\n\n\nOpen Street Time\n\nopen_csv = pd.read_csv(\"data/Open Streets CSV.csv\")\nopen_csv['geometry'] = gpd.GeoSeries.from_wkt(open_csv['the Geom'])\nopen_csv_geo = gpd.GeoDataFrame(open_csv, geometry='geometry', crs=2263)\nopen_csv_geo = open_csv_geo.to_crs(epsg=4326)\n\nopen_7days_time = open_csv_geo.drop(['apprDaysWe','Object ID', 'Organization Name', 'Approved From Street', 'Approved To Street', 'apprStartD', 'apprEndDat', 'Shape_STLe', 'segmentidt', 'segmentidf', 'lionversion', 'the Geom'], axis=1)\nopen_7days_time = open_7days_time.drop_duplicates(subset = \"Approved On Street\")\nopen_7days_time_melt = open_7days_time.melt(id_vars=['Approved On Street', 'Borough Name'], value_vars=['Approved Monday Open', 'Approved Monday Close', 'Approved Tuesday Open', 'Approved Tuesday Close', 'Approved Wednesday Open', 'Approved Wednesday Close', 'Approved Thursday Open', 'Approved Thursday Close', 'Approved Friday Open', 'Approved Friday Close', 'Approved Saturday Open', 'Approved Saturday Close', 'Approved Sunday Open', 'Approved Sunday Close']).dropna()\n#open_7days_time_melt['value'] =  pd.to_datetime(open_7days_time_melt['value']).dt.time\nopen_time_Brooklyn = open_7days_time_melt[(open_7days_time_melt['Borough Name'] == 'Brooklyn')]\nopen_time_Brooklyn\n\n\n\n\n\n\n\n\nApproved On Street\nBorough Name\nvariable\nvalue\n\n\n\n\n6\nRIDGE BOULEVARD\nBrooklyn\nApproved Monday Open\n10:00\n\n\n9\n82 STREET\nBrooklyn\nApproved Monday Open\n08:30\n\n\n10\n48 STREET\nBrooklyn\nApproved Monday Open\n13:30\n\n\n11\n43 STREET\nBrooklyn\nApproved Monday Open\n09:30\n\n\n12\nALBEMARLE ROAD\nBrooklyn\nApproved Monday Open\n08:00\n\n\n...\n...\n...\n...\n...\n\n\n2282\nJEFFERSON AVENUE\nBrooklyn\nApproved Sunday Close\n21:00\n\n\n2287\nSHARON STREET\nBrooklyn\nApproved Sunday Close\n20:00\n\n\n2288\nTROUTMAN STREET\nBrooklyn\nApproved Sunday Close\n22:00\n\n\n2289\nRANDOLPH STREET\nBrooklyn\nApproved Sunday Close\n23:00\n\n\n2354\nLEXINGTON AVENUE\nBrooklyn\nApproved Sunday Close\n20:00\n\n\n\n\n378 rows × 4 columns\n\n\n\n\nopen_time_Brooklyn['time'] = open_7days_time_melt['variable'].str.extract('(Open|Close)')\nopen_time_Brooklyn['dayweek'] = open_7days_time_melt['variable'].str.extract('(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)')\nopen_time_Brooklyn= open_time_Brooklyn.pivot(index=['Approved On Street', 'dayweek'], columns=\"time\", values=\"value\").reset_index()\n\norder=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nopen_time_Brooklyn['dayweek'] = pd.Categorical(open_time_Brooklyn['dayweek'], categories=order, ordered=True)\nopen_time_Brooklyn = open_time_Brooklyn.sort_values(by='dayweek').reset_index()\n\nopen_time_Brooklyn\n\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1900940832.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  open_time_Brooklyn['time'] = open_7days_time_melt['variable'].str.extract('(Open|Close)')\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1900940832.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  open_time_Brooklyn['dayweek'] = open_7days_time_melt['variable'].str.extract('(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)')\n\n\n\n\n\n\n\n\ntime\nindex\nApproved On Street\ndayweek\nClose\nOpen\n\n\n\n\n0\n139\nSUMMIT STREET\nMonday\n14:30\n12:30\n\n\n1\n151\nUNDERHILL AVENUE\nMonday\n20:00\n08:00\n\n\n2\n28\n82 STREET\nMonday\n15:30\n08:30\n\n\n3\n143\nTHATFORD AVENUE\nMonday\n18:00\n11:00\n\n\n4\n33\nAITKEN PLACE\nMonday\n15:00\n11:00\n\n\n...\n...\n...\n...\n...\n...\n\n\n184\n147\nTOMPKINS AVENUE\nSunday\n20:00\n11:00\n\n\n185\n51\nBEVERLEY ROAD\nSunday\n18:00\n10:00\n\n\n186\n112\nRANDOLPH STREET\nSunday\n23:00\n12:00\n\n\n187\n10\n4 STREET\nSunday\n22:00\n08:00\n\n\n188\n157\nVANDERBILT AVENUE\nSunday\n23:00\n11:00\n\n\n\n\n189 rows × 5 columns\n\n\n\n\nselection = alt.selection_multi(fields=['dayweek'])\ncolor = alt.condition(selection,\n                      alt.Color('dayweek:N', legend=None, \n                      scale=alt.Scale(scheme='category10')),\n                      alt.value('lightgray'))\n\nopacity = alt.condition(selection,\n                        alt.value(1), alt.value(0))\n\n\n\nbar = alt.Chart(open_time_Brooklyn).mark_bar().encode(\n        x='Open',\n        x2='Close',\n        y='Approved On Street',\n        color=color,\n        opacity=opacity,\n        tooltip=['Open', 'Close', 'Approved On Street', 'dayweek']).properties(\n        width=500,\n        height=1000).interactive()\n\nlegend = alt.Chart(open_time_Brooklyn).mark_bar().encode(\n    y=alt.Y('dayweek:N', axis=alt.Axis(orient='right')),\n    color=color\n).add_selection(\nselection\n)\n\nA_Chart = bar | legend\nA_Chart\n\n\n\n\n\n\n\n\nThis interactive bar chart on Brooklyn Open Street serves as a pilot that can be adapted for data of all five boroughs. By comparing to open and close time, we can observe that many streets are approved to be open streets with later opening and closing time on weekends. To improve this bar chart the status of each street can be added (i.e., whether they are approved to be closed fully or partially or only on school days), which is important for visitors as well."
  },
  {
    "objectID": "analysis/Ass-2.html#dashboard",
    "href": "analysis/Ass-2.html#dashboard",
    "title": "Where to spend leisure time in NYC?",
    "section": "Dashboard",
    "text": "Dashboard\nThe dashboard below is an upgrade from the indoor space and income grouped scatter plot. This dashboard makes it easier to explore the spatial distribution of indoor spaces of certain type or in neighborhoods with certain level of income. Through selecting points with higher counts, we can see neighborhoods with higher density of identified indoor leisure spaces are mostly located in Manhattan.\n\nbrush = alt.selection_interval()\n\npoints = alt.Chart(income_indoor_pivot_filtered).mark_point().encode(\n       x=alt.X(\"Median Household Income:Q\", scale=alt.Scale(zero=False)),\n       y=alt.Y(\"Count:Q\", scale=alt.Scale(zero=False)),\n       color=alt.condition(brush, \"BoroName:N\", alt.value(\"lightgray\")),\n       tooltip=[\"NTAName\",\"BoroName:N\", \"Median Household Income:Q\", \"Count:Q\"]).add_params(brush\n).properties(width=200, height=200\n).facet(column=\"Types:N\")\n\n\nbars = alt.Chart(income_indoor_pivot_filtered).mark_bar().encode(\n    y='BoroName:N',\n    color='BoroName:N',\n    x='Count:Q'\n).transform_filter(\n    brush\n)\n\nDashboard = points & bars\nDashboard"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap"
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features.\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader."
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007"
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 550 Final Project Template",
    "section": "",
    "text": "We can create beautiful websites that describe complex technical analyses in Python using Quarto and deploy them online using GitHub Pages. This combination of tools is a really powerful way to create and share your work. This website is a demo that is meant to be used to create your own Quarto website for the final project in MUSA 550.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online.\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "MUSA 550 Final Project Template",
    "section": "",
    "text": "We can create beautiful websites that describe complex technical analyses in Python using Quarto and deploy them online using GitHub Pages. This combination of tools is a really powerful way to create and share your work. This website is a demo that is meant to be used to create your own Quarto website for the final project in MUSA 550.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online.\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project."
  },
  {
    "objectID": "index.html#find-out-more",
    "href": "index.html#find-out-more",
    "title": "MUSA 550 Final Project Template",
    "section": "Find out more",
    "text": "Find out more\nThe code for this repository is hosted on our course’s GitHub page: https://github.com/MUSA-550-Fall-2023/quarto-website-template.\nWe covered the basics of getting started with Quarto and GitHub Pages in week 9. Take a look at the slides for lecture 9A to find out more."
  },
  {
    "objectID": "analysis/assignment-1.html",
    "href": "analysis/assignment-1.html",
    "title": "Assignment #1: The Donut Effect for Philadelphia ZIP Codes",
    "section": "",
    "text": "In this assignment, we will practice our pandas skills and explore the “Donut Effect” within Philadelphia. The “Donut Effect” describes the following phenomenon: with more flexible working options and pandemic-driven density fears, people left urban dense cores and opted for more space in city suburbs, driving home and rental prices up in the suburbs relative to city centers.\nWe will be working with Zillow data for the Zillow Home Value Index (ZHVI) for Philadelphia ZIP codes. The goal will be to calculate home price appreciation in Philadelphia, comparing those ZIP codes in Center City (the central business district) to those not in Center City."
  },
  {
    "objectID": "analysis/assignment-1.html#load-the-data",
    "href": "analysis/assignment-1.html#load-the-data",
    "title": "Assignment #1: The Donut Effect for Philadelphia ZIP Codes",
    "section": "1. Load the data",
    "text": "1. Load the data\nI’ve already downloaded the relevant data file and put in the data/ folder. Let’s load it using pandas.\nNote: Be sure to use a relative file path to make it easier to load your data when grading. See this guide for more info.\n\nimport pandas as pd\n\n\nimport numpy as np\n\n\nzip_df = pd.read_csv(\"data/Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")"
  },
  {
    "objectID": "analysis/assignment-1.html#trim-the-data-to-just-philadelphia",
    "href": "analysis/assignment-1.html#trim-the-data-to-just-philadelphia",
    "title": "Assignment #1: The Donut Effect for Philadelphia ZIP Codes",
    "section": "2. Trim the data to just Philadelphia",
    "text": "2. Trim the data to just Philadelphia\nSelect the subset of the dataframe for Philadelphia, PA.\n\n# or use isin to trim data\n\nstate_trim = zip_df[\"StateName\"].isin([\"PA\"])\nstate_trim_df= zip_df.loc[state_trim]\n\nphilly_df = state_trim_df.loc[state_trim_df[\"City\"] == \"Philadelphia\"]\nphilly_df.head()\n\n\n\n\n\n\n\n\nRegionID\nSizeRank\nRegionName\nRegionType\nStateName\nState\nCity\nMetro\nCountyName\n2000-01-31\n...\n2021-10-31\n2021-11-30\n2021-12-31\n2022-01-31\n2022-02-28\n2022-03-31\n2022-04-30\n2022-05-31\n2022-06-30\n2022-07-31\n\n\n\n\n125\n65810\n126\n19143\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n60701.0\n...\n173114.0\n172087.0\n171445.0\n171542.0\n171680.0\n171878.0\n171607.0\n171333.0\n171771.0\n172611.0\n\n\n247\n65779\n249\n19111\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n85062.0\n...\n257911.0\n260104.0\n262257.0\n263715.0\n264809.0\n265684.0\n267222.0\n269460.0\n272201.0\n274446.0\n\n\n338\n65791\n340\n19124\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n47155.0\n...\n156225.0\n157780.0\n159029.0\n159274.0\n159886.0\n160780.0\n161929.0\n163625.0\n165020.0\n166009.0\n\n\n423\n65787\n426\n19120\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n59285.0\n...\n161167.0\n161807.0\n162634.0\n162972.0\n163597.0\n164008.0\n164887.0\n165860.0\n167321.0\n168524.0\n\n\n509\n65772\n512\n19104\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n74255.0\n...\n220270.0\n221454.0\n222006.0\n220760.0\n217933.0\n216447.0\n216424.0\n218663.0\n220453.0\n223443.0\n\n\n\n\n5 rows × 280 columns"
  },
  {
    "objectID": "analysis/assignment-1.html#melt-the-data-into-tidy-format",
    "href": "analysis/assignment-1.html#melt-the-data-into-tidy-format",
    "title": "Assignment #1: The Donut Effect for Philadelphia ZIP Codes",
    "section": "3. Melt the data into tidy format",
    "text": "3. Melt the data into tidy format\nLet’s transform the data from wide to tidy using the pd.melt() function. Create a new column in your data called “ZHVI” that holds the ZHVI values.\n\ndef looks_like_a_date(col):\n    \"\"\"A function that tests if a string starts with '20'\"\"\"\n    \n    return col.startswith(\"20\")\n\n\nphilly_tidy = philly_df.melt(\n    id_vars = [\"RegionName\",\"RegionID\"],\n    value_vars = list(filter(looks_like_a_date, philly_df.columns)),\n    var_name = \"Date\",\n    value_name = \"ZHVI\",\n)\n    \n\n\nphilly_tidy\n\n\n\n\n\n\n\n\nRegionName\nRegionID\nDate\nZHVI\n\n\n\n\n0\n19143\n65810\n2000-01-31\n60701.0\n\n\n1\n19111\n65779\n2000-01-31\n85062.0\n\n\n2\n19124\n65791\n2000-01-31\n47155.0\n\n\n3\n19120\n65787\n2000-01-31\n59285.0\n\n\n4\n19104\n65772\n2000-01-31\n74255.0\n\n\n...\n...\n...\n...\n...\n\n\n12461\n19153\n65820\n2022-07-31\n247560.0\n\n\n12462\n19118\n65785\n2022-07-31\n746009.0\n\n\n12463\n19102\n65770\n2022-07-31\n347614.0\n\n\n12464\n19127\n65794\n2022-07-31\n318732.0\n\n\n12465\n19137\n65804\n2022-07-31\n222107.0\n\n\n\n\n12466 rows × 4 columns"
  },
  {
    "objectID": "analysis/assignment-1.html#split-the-data-for-zip-codes-inoutside-center-city",
    "href": "analysis/assignment-1.html#split-the-data-for-zip-codes-inoutside-center-city",
    "title": "Assignment #1: The Donut Effect for Philadelphia ZIP Codes",
    "section": "4. Split the data for ZIP codes in/outside Center City",
    "text": "4. Split the data for ZIP codes in/outside Center City\nTo compare home appreciation in Center City vs. outside Center City, we’ll need to split the data into two dataframes, one that holds the Center City ZIP codes and one that holds the data for the rest of the ZIP codes in Philadelphia.\nTo help with this process, I’ve included a list of ZIP codes that make up the “greater Center City” region of Philadelphia. Use this list to split the melted data into two dataframes.\n\ngreater_center_city_zip_codes = [\n    19123,\n    19102,\n    19103,\n    19106,\n    19107,\n    19109,\n    19130,\n    19146,\n    19147,\n]\n\n\n# CENTER CITY\ncenter_city_zip = philly_tidy[\"RegionName\"].isin(greater_center_city_zip_codes)\ncenter_city_zip\ncenter_city = philly_tidy.loc[center_city_zip].copy()\ncenter_city\n\n\n\n\n\n\n\n\nRegionName\nRegionID\nDate\nZHVI\n\n\n\n\n9\n19146\n65813\n2000-01-31\n97460.0\n\n\n12\n19147\n65814\n2000-01-31\n119919.0\n\n\n14\n19103\n65771\n2000-01-31\n183436.0\n\n\n18\n19130\n65797\n2000-01-31\n128477.0\n\n\n33\n19107\n65775\n2000-01-31\n128049.0\n\n\n...\n...\n...\n...\n...\n\n\n12438\n19130\n65797\n2022-07-31\n431501.0\n\n\n12453\n19107\n65775\n2022-07-31\n330958.0\n\n\n12457\n19123\n65790\n2022-07-31\n443152.0\n\n\n12458\n19106\n65774\n2022-07-31\n407423.0\n\n\n12463\n19102\n65770\n2022-07-31\n347614.0\n\n\n\n\n2168 rows × 4 columns\n\n\n\n\n# Out_of_Center_City\nout_of_center_city_zip = ~philly_tidy[\"RegionName\"].isin(greater_center_city_zip_codes)\nout_of_center_city_zip\nout_of_center_city = philly_tidy.loc[out_of_center_city_zip].copy()\nout_of_center_city\n\n\n\n\n\n\n\n\nRegionName\nRegionID\nDate\nZHVI\n\n\n\n\n0\n19143\n65810\n2000-01-31\n60701.0\n\n\n1\n19111\n65779\n2000-01-31\n85062.0\n\n\n2\n19124\n65791\n2000-01-31\n47155.0\n\n\n3\n19120\n65787\n2000-01-31\n59285.0\n\n\n4\n19104\n65772\n2000-01-31\n74255.0\n\n\n...\n...\n...\n...\n...\n\n\n12460\n19129\n65796\n2022-07-31\n302177.0\n\n\n12461\n19153\n65820\n2022-07-31\n247560.0\n\n\n12462\n19118\n65785\n2022-07-31\n746009.0\n\n\n12464\n19127\n65794\n2022-07-31\n318732.0\n\n\n12465\n19137\n65804\n2022-07-31\n222107.0\n\n\n\n\n10298 rows × 4 columns"
  },
  {
    "objectID": "analysis/assignment-1.html#compare-home-value-appreciation-in-philadelpia",
    "href": "analysis/assignment-1.html#compare-home-value-appreciation-in-philadelpia",
    "title": "Assignment #1: The Donut Effect for Philadelphia ZIP Codes",
    "section": "5. Compare home value appreciation in Philadelpia",
    "text": "5. Compare home value appreciation in Philadelpia\nIn this step, we’ll calculate the average percent increase in ZHVI from March 2020 to March 2022 for ZIP codes in/out of Center City. We’ll do this by:\n\nWriting a function (see the template below) that will calculate the percent increase in ZHVI from March 31, 2020 to March 31, 2022\nGroup your data and apply this function to calculate the ZHVI percent change for each ZIP code in Philadelphia. Do this for both of your dataframes from the previous step.\nCalculate the average value across ZIP codes for both sets of ZIP codes and then compare\n\nYou should see much larger growth for ZIP codes outside of Center City…the Donut Effect!\n\ndef calculate_percent_increase(group_df):\n    \n    march_20sel = group_df[\"Date\"] == \"2020-03-31\"\n    march_22sel = group_df[\"Date\"] == \"2022-03-31\"\n    \n    march_20 = group_df.loc[march_20sel].squeeze()\n    march_22 = group_df.loc[march_22sel].squeeze()\n    \n    columns = [\"ZHVI\"]\n    \n    return (march_22[columns] / march_20[columns] - 1) * 100\n\n\n# Center City Grouped\ngrouped_center_city = center_city.groupby(\"RegionName\")\nresult_center_city = grouped_center_city.apply(calculate_percent_increase)\nresult_center_city\n\n\n\n\n\n\n\n\nZHVI\n\n\nRegionName\n\n\n\n\n\n19102\n-1.716711\n\n\n19103\n-1.696176\n\n\n19106\n2.520802\n\n\n19107\n2.883181\n\n\n19123\n5.212747\n\n\n19130\n6.673031\n\n\n19146\n6.480788\n\n\n19147\n6.139806\n\n\n\n\n\n\n\n\n# Outside Center City Grouped\ngrouped_outside = out_of_center_city.groupby(\"RegionName\")\nresult_outside = grouped_outside.apply(calculate_percent_increase)\nresult_outside\n\n\n\n\n\n\n\n\nZHVI\n\n\nRegionName\n\n\n\n\n\n19104\n14.539797\n\n\n19111\n28.690446\n\n\n19114\n21.074312\n\n\n19115\n22.455454\n\n\n19116\n23.079842\n\n\n19118\n17.585001\n\n\n19119\n17.478667\n\n\n19120\n26.927423\n\n\n19121\n26.228643\n\n\n19122\n10.275804\n\n\n19124\n28.743474\n\n\n19125\n11.007135\n\n\n19126\n20.819254\n\n\n19127\n20.023926\n\n\n19128\n21.887555\n\n\n19129\n15.598565\n\n\n19131\n23.363129\n\n\n19132\n72.218386\n\n\n19133\n36.143992\n\n\n19134\n23.936841\n\n\n19135\n28.115259\n\n\n19136\n26.487833\n\n\n19137\n23.248505\n\n\n19138\n24.662626\n\n\n19139\n37.008969\n\n\n19140\n57.150847\n\n\n19141\n26.441684\n\n\n19142\n44.564396\n\n\n19143\n23.951077\n\n\n19144\n21.094020\n\n\n19145\n7.634693\n\n\n19148\n6.963237\n\n\n19149\n24.916458\n\n\n19150\n18.735248\n\n\n19151\n19.651429\n\n\n19152\n21.993528\n\n\n19153\n38.240461\n\n\n19154\n17.930932\n\n\n\n\n\n\n\n\n# Center City Growth\nresult_cc_mean = result_center_city.mean().squeeze()\n\n\n# Outside of Center City Growth\nresult_ot_mean = result_outside.mean().squeeze()\n\n\nHV_growth = pd.Series([result_cc_mean,result_ot_mean])\nLocation = pd.Series([\"Center City\", \"Outside Center City\"])\nfinal_result_df = pd.DataFrame({\"Location\": Location, \"Home Value growth (%)\": HV_growth})\nfinal_result_df\n\n\n\n\n\n\n\n\nLocation\nHome Value growth (%)\n\n\n\n\n0\nCenter City\n3.312183\n\n\n1\nOutside Center City\n25.022864\n\n\n\n\n\n\n\nDonut Effect it is!"
  }
]