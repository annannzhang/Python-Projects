[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "My name is Ann Zi’an Zhang, currently a student at University of Pennsylvania studying city planning and urban spatial analytics. I am interested in data-driven problem-solving, especially in urban context.\nPrior to Penn, I graduated from Wesleyan University in Connecticut, U.S., majoring in Art History, Psychology, and Science in Society Program."
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium."
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "analysis/Ass-2.html",
    "href": "analysis/Ass-2.html",
    "title": "Where to spend leisure time in NYC?",
    "section": "",
    "text": "---\nformat:\n    html:\n        toc: false\n        page-layout: full\nexecute:\n        echo: false\n\n---"
  },
  {
    "objectID": "analysis/Ass-2.html#imports",
    "href": "analysis/Ass-2.html#imports",
    "title": "Where to spend leisure time in NYC?",
    "section": "Imports",
    "text": "Imports\n\nimport altair as alt\nimport geopandas as gpd\nimport hvplot.pandas\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport datetime\nimport math\n\n%matplotlib inline"
  },
  {
    "objectID": "analysis/Ass-2.html#nyc-base-maps",
    "href": "analysis/Ass-2.html#nyc-base-maps",
    "title": "Where to spend leisure time in NYC?",
    "section": "NYC Base Maps",
    "text": "NYC Base Maps\n\nTracts, Neighborhood, and Borough\n\nnyc_tracts = pd.read_csv(\"data/2022 Census Tracts.csv\")\nnyc_tracts['geometry'] = gpd.GeoSeries.from_wkt(nyc_tracts['the_geom'])\ngeo_tracts = gpd.GeoDataFrame(nyc_tracts, geometry='geometry')\ngeo_tracts = geo_tracts.set_crs(epsg=4326)\ntracts = geo_tracts[['BoroName', 'CT2020', 'BoroCT2020', 'NTAName', 'Shape_Area','geometry','GEOID']].copy()\ntracts_clean = tracts[['BoroName', 'CT2020', 'NTAName', 'geometry','GEOID']]\nBoro_NTA = tracts_clean[['BoroName', 'NTAName']].drop_duplicates(subset = \"NTAName\")"
  },
  {
    "objectID": "analysis/Ass-2.html#dataset-selection-setup",
    "href": "analysis/Ass-2.html#dataset-selection-setup",
    "title": "Where to spend leisure time in NYC?",
    "section": "Dataset Selection & Setup",
    "text": "Dataset Selection & Setup\nAfter glancing through NYC Open Data portal, I have selected art galleries, museums, libraries, theatres, and parks as common leisure spaces. In addition, I explored data of open streets, a program thatt had been implemented in many global cities including New York City to transform roads into public spaces for cultural and all kinds of events on particular days (mostly on weekends).\nThis step is to bring in all datasets, clean them up, and aggregate different types of leisure spaces. The aggregated dataframe is then spacially joined with tracts, neighborhood, and boroughs, ready for further investigation on their geospatial distributions.\nSimilar data wrangling is performed on parks. Since parks come in polygon instead of points, which may result in problems with spatial joins, in the case when one park falls in two or more tracts or neighborhood. Hence, the geometry of parks’ centroids is adopted to replace the polygon geometry for further analysis.\nThe Open Street data comes with more detailed information on the approved time for each street. The days of week (e.g. Monday) and time of the day (e.g. 7:30) that it opens and closes are rearranged by melting and pivoting.\nAdditionally, 2020 Census Data on population and median household income are brought in for analysis.\n\nIndoor Leisure Spaces: Art Galleries, Museums, Libraries, and Theatres\n\n# establish geodataframe\nart_galleries = gpd.read_file(\"data/art galleries.geojson\")\ngeo_art_galleries = gpd.GeoDataFrame(art_galleries, geometry='geometry')\ngeo_art_galleries = art_galleries.set_crs(epsg=4326)\n\n# add type\nart_galleries_clean = geo_art_galleries[['name','zip','address1','geometry']]\nart_galleries_clean.loc[:,\"Type\"]= \"Art Galleries\"\nart_galleries_clean.rename(\n    columns={\"address1\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1429971093.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  art_galleries_clean.rename(\n\n\n\n# Museums \n\n# establish geodataframe\nmuseums = gpd.read_file(\"data/museums.geojson\")\ngeo_museums= gpd.GeoDataFrame(museums, geometry='geometry')\ngeo_museums = geo_museums.set_crs(epsg=4326)\ngeo_museums\n\n# add type\nmuseums_clean = geo_museums[['name','zip','adress1','geometry']]\nmuseums_clean.loc[:,\"Type\"]= \"Museums\"\nmuseums_clean.rename(\n    columns={\"adress1\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\nmuseums_clean\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/3071166290.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  museums_clean.rename(\n\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\n\n\n\n\n0\nAlexander Hamilton U.S. Custom House\n10004.0\n1 Bowling Grn\nPOINT (-74.01376 40.70382)\nMuseums\n\n\n1\nAlice Austen House Museum\n10305.0\n2 Hylan Blvd\nPOINT (-74.06303 40.61512)\nMuseums\n\n\n2\nAmerican Academy of Arts and Letters\n10032.0\n633 W. 155th St.\nPOINT (-73.94730 40.83385)\nMuseums\n\n\n3\nAmerican Folk Art Museum\n10019.0\n45 West 53rd Street\nPOINT (-73.97810 40.76162)\nMuseums\n\n\n4\nAmerican Immigration History Center\n0.0\nEllis Island\nPOINT (-74.03968 40.69906)\nMuseums\n\n\n...\n...\n...\n...\n...\n...\n\n\n125\nAmerican Sephardi Federation / Sephardic House\n10011.0\n15 W. 16th St.\nPOINT (-73.99389 40.73808)\nMuseums\n\n\n126\nYIVO Institute for Jewish Research\n10011.0\n15 W. 16th St.\nPOINT (-73.99379 40.73796)\nMuseums\n\n\n127\nAmerican Jewish Historical Society\n10011.0\n15 W. 16th St.\nPOINT (-73.99393 40.73802)\nMuseums\n\n\n128\nYeshiva University Museum\n10011.0\n15 W. 16th St.\nPOINT (-73.99382 40.73805)\nMuseums\n\n\n129\nCenter For Jewish History\n10011.0\n15 W. 16th St.\nPOINT (-73.99387 40.73799)\nMuseums\n\n\n\n\n130 rows × 5 columns\n\n\n\n\n# Libraries\n\n# establish geodataframe\nlibrary = gpd.read_file(\"data/libraries.geojson\")\ngeo_library = gpd.GeoDataFrame(library, geometry='geometry')\ngeo_library= geo_library.set_crs(epsg=4326)\ngeo_library\n\n# add type\nlibrary_clean = geo_library[['name','zip','streetname','geometry']]\nlibrary_clean.loc[:,\"Type\"]= \"Libraries\"\nlibrary_clean.rename(\n    columns={\"streetname\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\nlibrary_clean\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/3524914390.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  library_clean.rename(\n\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\n\n\n\n\n0\n115th Street\n10026\nWest 115th Street\nPOINT (-73.95353 40.80298)\nLibraries\n\n\n1\n125th Street\n10035\nEast 125th Street\nPOINT (-73.93485 40.80302)\nLibraries\n\n\n2\n53rd Street\n10019\nWest 53rd Street\nPOINT (-73.97736 40.76081)\nLibraries\n\n\n3\n58th Street\n10022\nEast 58th Street\nPOINT (-73.96938 40.76219)\nLibraries\n\n\n4\n67th Street\n10065\nEast 67th Street\nPOINT (-73.95955 40.76492)\nLibraries\n\n\n...\n...\n...\n...\n...\n...\n\n\n211\nSunnyside\n11104\nGreenpoint Avenue\nPOINT (-73.92167 40.74085)\nLibraries\n\n\n212\nWhitestone\n11357\n14 Road\nPOINT (-73.81070 40.78854)\nLibraries\n\n\n213\nWindsor Park\n11364\nBell Boulevard\nPOINT (-73.75562 40.73450)\nLibraries\n\n\n214\nWoodhaven\n11421\nForest Parkway\nPOINT (-73.86146 40.69453)\nLibraries\n\n\n215\nWoodside\n11377\nSkillman Avenue\nPOINT (-73.90979 40.74534)\nLibraries\n\n\n\n\n216 rows × 5 columns\n\n\n\n\n# Theaters\n# establish geodataframe\ntheaters = gpd.read_file(\"data/Theaters.geojson\")\ngeo_theaters = gpd.GeoDataFrame(theaters, geometry='geometry')\ngeo_theaters= geo_theaters.set_crs(epsg=4326)\ngeo_theaters\n\n\n# add type\ntheaters_clean = geo_theaters[['name','zip','address1','geometry']]\ntheaters_clean.loc[:,\"Type\"]= \"Theatres\"\ntheaters_clean.rename(\n    columns={\"address1\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\ntheaters_clean\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1518346627.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  theaters_clean.rename(\n\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\n\n\n\n\n0\n45th Street Theater\n10036.0\n354 West 45th Street\nPOINT (-73.99062 40.75985)\nTheatres\n\n\n1\n47th Street Theater\n10036.0\n304 West 47th Street\nPOINT (-73.98811 40.76047)\nTheatres\n\n\n2\n59E59\n10022.0\n59 East 59th Street\nPOINT (-73.97038 40.76340)\nTheatres\n\n\n3\nAcorn Theater\n10036.0\n410 West 42nd Street\nPOINT (-73.99332 40.75854)\nTheatres\n\n\n4\nAl Hirschfeld Theater\n10036.0\n302 W 45th Street\nPOINT (-73.98921 40.75926)\nTheatres\n\n\n...\n...\n...\n...\n...\n...\n\n\n112\nWestside Theater\n10036.0\n407 W 43rd St\nPOINT (-73.99255 40.75953)\nTheatres\n\n\n113\nWings Theatre\n10014.0\n154 Christopher St\nPOINT (-74.00889 40.73240)\nTheatres\n\n\n114\nWinter Garden Theatre\n10019.0\n1634 Broadway\nPOINT (-73.98348 40.76152)\nTheatres\n\n\n115\nYork Theatre\n10022.0\n619 Lexington Ave\nPOINT (-73.96998 40.75836)\nTheatres\n\n\n116\nDelacorte Theater\n0.0\nCentral Park - Mid-Park at 80th Street\nPOINT (-73.96882 40.78018)\nTheatres\n\n\n\n\n117 rows × 5 columns\n\n\n\n\nAggregate all types of indoor leisure space\n\nframes = [art_galleries_clean, museums_clean, library_clean, theaters_clean]\n\ntotal_indoor = pd.concat(frames)\n\ntotal_indoor\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\n\n\n\n\n0\nO'reilly William & Co Ltd\n10021.0\n52 E 76th St\nPOINT (-73.96273 40.77380)\nArt Galleries\n\n\n1\nOrganization of Independent Artists - Gallery 402\n10013.0\n19 Hudson St.\nPOINT (-74.00939 40.71647)\nArt Galleries\n\n\n2\nOwen Gallery\n10021.0\n19 E 75th St\nPOINT (-73.96435 40.77400)\nArt Galleries\n\n\n3\nP P O W Gallerie\n10001.0\n511 W 25th St\nPOINT (-74.00389 40.74959)\nArt Galleries\n\n\n4\nP P O W Inc\n10013.0\n476 Broome St\nPOINT (-74.00176 40.72291)\nArt Galleries\n\n\n...\n...\n...\n...\n...\n...\n\n\n112\nWestside Theater\n10036.0\n407 W 43rd St\nPOINT (-73.99255 40.75953)\nTheatres\n\n\n113\nWings Theatre\n10014.0\n154 Christopher St\nPOINT (-74.00889 40.73240)\nTheatres\n\n\n114\nWinter Garden Theatre\n10019.0\n1634 Broadway\nPOINT (-73.98348 40.76152)\nTheatres\n\n\n115\nYork Theatre\n10022.0\n619 Lexington Ave\nPOINT (-73.96998 40.75836)\nTheatres\n\n\n116\nDelacorte Theater\n0.0\nCentral Park - Mid-Park at 80th Street\nPOINT (-73.96882 40.78018)\nTheatres\n\n\n\n\n1380 rows × 5 columns\n\n\n\n\n\nSpatial Join\n\ngeo_total_indoor = gpd.sjoin(\n    total_indoor,  # The point data for 311 tickets\n    tracts_clean.to_crs(total_indoor.crs),  # The neighborhoods (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\ngeo_total_indoor.head()\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\nindex_right\nBoroName\nCT2020\nNTAName\nGEOID\n\n\n\n\n0\nO'reilly William & Co Ltd\n10021.0\n52 E 76th St\nPOINT (-73.96273 40.77380)\nArt Galleries\n84.0\nManhattan\n13000.0\nUpper East Side-Carnegie Hill\n3.606101e+10\n\n\n1\nOrganization of Independent Artists - Gallery 402\n10013.0\n19 Hudson St.\nPOINT (-74.00939 40.71647)\nArt Galleries\n17.0\nManhattan\n3900.0\nTribeca-Civic Center\n3.606100e+10\n\n\n2\nOwen Gallery\n10021.0\n19 E 75th St\nPOINT (-73.96435 40.77400)\nArt Galleries\n84.0\nManhattan\n13000.0\nUpper East Side-Carnegie Hill\n3.606101e+10\n\n\n3\nP P O W Gallerie\n10001.0\n511 W 25th St\nPOINT (-74.00389 40.74959)\nArt Galleries\n1134.0\nManhattan\n9901.0\nChelsea-Hudson Yards\n3.606101e+10\n\n\n4\nP P O W Inc\n10013.0\n476 Broome St\nPOINT (-74.00176 40.72291)\nArt Galleries\n1156.0\nManhattan\n4900.0\nSoHo-Little Italy-Hudson Square\n3.606100e+10\n\n\n\n\n\n\n\n\n\n\nOutdoor Leisure Space: Parks\n\n# Parks\n# establish geodataframe\nparks = gpd.read_file(\"data/Parks Properties.geojson\")\ngeo_parks = gpd.GeoDataFrame(parks, geometry='geometry')\ngeo_parks= geo_parks.set_crs(epsg=4326)\ngeo_parks\n\n\n# add type\nparks_clean = geo_parks[['signname','geometry','acres']]\nparks_clean.loc[:,\"Type\"]= \"Parks\"\nparks_clean.rename(\n    columns={\"location\": \"Address\", \"signname\": \"Name\", \"zipcode\": \"Zip\"},\n    inplace=True,)\n\nparks_clean['new_geom'] = parks_clean['geometry']\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1177565529.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  parks_clean.rename(\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n\n\n\n\n# Since many parks are in multi-polygons, which will lead to errors when doing spatial join later, a centroid of each park is generated here for smoother spatial join.\nparks_clean['geometry'] = parks_clean['geometry'].centroid\n\ngeo_parks_clean = gpd.sjoin(\n    parks_clean,  # The point data for 311 tickets\n    tracts_clean.to_crs(parks_clean.crs),  # The neighborhoods (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\ngeo_parks_clean\n\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/3082874007.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  parks_clean['geometry'] = parks_clean['geometry'].centroid\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n\n\n\n\n\n\n\n\n\nName\ngeometry\nacres\nType\nnew_geom\nindex_right\nBoroName\nCT2020\nNTAName\nGEOID\n\n\n\n\n0\nInwood Hill Park\nPOINT (-73.92544 40.87257)\n196.398\nParks\nMULTIPOLYGON (((-73.92093 40.86999, -73.92145 ...\n2191.0\nManhattan\n29700.0\nInwood Hill Park\n3.606103e+10\n\n\n1\nChallenge Playground\nPOINT (-73.72796 40.75662)\n2.035\nParks\nMULTIPOLYGON (((-73.72738 40.75605, -73.72783 ...\n936.0\nQueens\n152902.0\nDouglaston-Little Neck\n3.608115e+10\n\n\n2\nSunset Cove Park\nPOINT (-73.82300 40.59853)\n9.375\nParks\nMULTIPOLYGON (((-73.82218 40.59892, -73.82221 ...\n1129.0\nQueens\n107201.0\nBreezy Point-Belle Harbor-Rockaway Park-Broad ...\n3.608111e+10\n\n\n3\nGrand Central Parkway Extension\nPOINT (-73.85317 40.75316)\n249.389\nParks\nMULTIPOLYGON (((-73.85875 40.76741, -73.85976 ...\n625.0\nQueens\n39902.0\nNorth Corona\n3.608104e+10\n\n\n4\nIdlewild Park\nPOINT (-73.75229 40.65043)\n180.85\nParks\nMULTIPOLYGON (((-73.75809 40.65427, -73.75845 ...\n1013.0\nQueens\n66404.0\nSpringfield Gardens (South)-Brookville\n3.608107e+10\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2040\nMaria Hernandez Park\nPOINT (-73.92386 40.70317)\n6.873\nParks\nMULTIPOLYGON (((-73.92251 40.70351, -73.92381 ...\n1729.0\nBrooklyn\n42900.0\nBushwick (West)\n3.604704e+10\n\n\n2041\nCrotona Parkway Malls\nPOINT (-73.88477 40.84405)\n8.75\nParks\nMULTIPOLYGON (((-73.88496 40.84470, -73.88496 ...\n336.0\nBronx\n36300.0\nWest Farms\n3.600504e+10\n\n\n2042\nPark\nPOINT (-73.89807 40.84408)\n0.511\nParks\nMULTIPOLYGON (((-73.89759 40.84410, -73.89773 ...\n1229.0\nBronx\n16500.0\nClaremont Village-Claremont (East)\n3.600502e+10\n\n\n2043\nCunningham Park\nPOINT (-73.76880 40.73382)\n358.0\nParks\nMULTIPOLYGON (((-73.77466 40.72442, -73.77439 ...\n2241.0\nQueens\n128300.0\nCunningham Park\n3.608113e+10\n\n\n2044\nRoberto Clemente Ballfield\nPOINT (-73.96767 40.70635)\n1.93\nParks\nMULTIPOLYGON (((-73.96761 40.70581, -73.96735 ...\n2220.0\nBrooklyn\n54500.0\nSouth Williamsburg\n3.604705e+10\n\n\n\n\n2045 rows × 10 columns\n\n\n\n\n\nOutdoor Leisure Space: Open Streets\n\nopen_streets = gpd.read_file(\"data/Open Streets Locations.geojson\")\ngeo_open_streets = gpd.GeoDataFrame(open_streets, geometry='geometry', crs=2263)\ngeo_open_streets = open_streets.to_crs(epsg=4326)\n\nopen_streets_clean = geo_open_streets[['appronstre','apprtostre','apprdayswe','boroughname','reviewstat','shape_stle','geometry']]\n\n\njoin_test = gpd.sjoin(\n    open_streets_clean,  # The point data for 311 tickets\n    tracts_clean.to_crs(open_streets_clean.crs),  # The neighborhoods (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\njoin_test.head()\n\n\n\n\n\n\n\n\nappronstre\napprtostre\napprdayswe\nboroughname\nreviewstat\nshape_stle\ngeometry\nindex_right\nBoroName\nCT2020\nNTAName\nGEOID\n\n\n\n\n0\nDEISIUS STREET\nSTECHER STREET\nmon,tue,wed,thu,fri\nStaten Island\napprovedFullSchools\n264.932398036\nMULTILINESTRING ((-74.18738 40.53028, -74.1882...\n1318.0\nStaten Island\n17600.0\nAnnadale-Huguenot-Prince's Bay-Woodrow\n3.608502e+10\n\n\n1\nSUFFOLK AVENUE\nHAROLD STREET\nfri,sat,sun\nStaten Island\napprovedFull\n313.087821487\nMULTILINESTRING ((-74.12784 40.60288, -74.1277...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n\n\n2\nSUFFOLK AVENUE\nHAROLD STREET\nfri,sat,sun\nStaten Island\napprovedFull\n142.063500219\nMULTILINESTRING ((-74.12772 40.60202, -74.1276...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n\n\n3\nVERMONT COURT\nSUFFOLK AVENUE\nfri,sat,sun\nStaten Island\napprovedFull\n421.392020366\nMULTILINESTRING ((-74.12620 40.60209, -74.1277...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n\n\n4\n9 STREET\nROSE AVENUE\nfri,sat,sun\nStaten Island\napprovedFull\n448.103939286\nMULTILINESTRING ((-74.11481 40.57316, -74.1157...\n1300.0\nStaten Island\n13400.0\nNew Dorp-Midland Beach\n3.608501e+10\n\n\n\n\n\n\n\n\njoin_test['monday'] = join_test['apprdayswe'].str.count('mon')\njoin_test['tuesday'] = join_test['apprdayswe'].str.count('tue')\njoin_test['wednesday'] = join_test['apprdayswe'].str.count('wed')\njoin_test['thursday'] = join_test['apprdayswe'].str.count('thu')\njoin_test['friday'] = join_test['apprdayswe'].str.count('fri')\njoin_test['saturday'] = join_test['apprdayswe'].str.count('sat')\njoin_test['sunday'] = join_test['apprdayswe'].str.count('sun')\njoin_test['dayscount'] = join_test[['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']].sum(axis=1)\n\njoin_test\n\n\n\n\n\n\n\n\nappronstre\napprtostre\napprdayswe\nboroughname\nreviewstat\nshape_stle\ngeometry\nindex_right\nBoroName\nCT2020\nNTAName\nGEOID\nmonday\ntuesday\nwednesday\nthursday\nfriday\nsaturday\nsunday\ndayscount\n\n\n\n\n0\nDEISIUS STREET\nSTECHER STREET\nmon,tue,wed,thu,fri\nStaten Island\napprovedFullSchools\n264.932398036\nMULTILINESTRING ((-74.18738 40.53028, -74.1882...\n1318.0\nStaten Island\n17600.0\nAnnadale-Huguenot-Prince's Bay-Woodrow\n3.608502e+10\n1\n1\n1\n1\n1\n0\n0\n5\n\n\n1\nSUFFOLK AVENUE\nHAROLD STREET\nfri,sat,sun\nStaten Island\napprovedFull\n313.087821487\nMULTILINESTRING ((-74.12784 40.60288, -74.1277...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n0\n0\n0\n0\n1\n1\n1\n3\n\n\n2\nSUFFOLK AVENUE\nHAROLD STREET\nfri,sat,sun\nStaten Island\napprovedFull\n142.063500219\nMULTILINESTRING ((-74.12772 40.60202, -74.1276...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n0\n0\n0\n0\n1\n1\n1\n3\n\n\n3\nVERMONT COURT\nSUFFOLK AVENUE\nfri,sat,sun\nStaten Island\napprovedFull\n421.392020366\nMULTILINESTRING ((-74.12620 40.60209, -74.1277...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n0\n0\n0\n0\n1\n1\n1\n3\n\n\n4\n9 STREET\nROSE AVENUE\nfri,sat,sun\nStaten Island\napprovedFull\n448.103939286\nMULTILINESTRING ((-74.11481 40.57316, -74.1157...\n1300.0\nStaten Island\n13400.0\nNew Dorp-Midland Beach\n3.608501e+10\n0\n0\n0\n0\n1\n1\n1\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\nBECK STREET\nAVENUE ST JOHN\nwed\nBronx\napprovedFull\n619.314479576\nMULTILINESTRING ((-73.90222 40.81449, -73.9014...\n200.0\nBronx\n8300.0\nLongwood\n3.600501e+10\n0\n0\n1\n0\n0\n0\n0\n1\n\n\n362\nNEWKIRK AVENUE\nEAST 17 STREET\nsun\nBrooklyn\napprovedFull\n284.786787549\nMULTILINESTRING ((-73.96422 40.63510, -73.9641...\n1807.0\nBrooklyn\n52000.0\nFlatbush (West)-Ditmas Park-Parkville\n3.604705e+10\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n363\n18 STREET\n4 AVENUE\nmon,tue,wed,thu,fri\nBrooklyn\napprovedFullSchools\n768.660222317\nMULTILINESTRING ((-73.99201 40.66323, -73.9916...\n2248.0\nBrooklyn\n14300.0\nSunset Park (West)\n3.604701e+10\n1\n1\n1\n1\n1\n0\n0\n5\n\n\n364\n34 AVENUE\nJUNCTION BOULEVARD\nmon,tue,wed,thu,fri,sat,sun\nQueens\napprovedLimited\n272.995599601\nMULTILINESTRING ((-73.89716 40.75246, -73.8970...\n2168.0\nQueens\n29100.0\nJackson Heights\n3.608103e+10\n1\n1\n1\n1\n1\n1\n1\n7\n\n\n365\nDECATUR STREET\nSARATOGA AVENUE\nsat\nBrooklyn\napprovedLimited\n769.964464111\nMULTILINESTRING ((-73.92001 40.68299, -73.9176...\n1682.0\nBrooklyn\n37700.0\nBedford-Stuyvesant (East)\n3.604704e+10\n0\n0\n0\n0\n0\n1\n0\n1\n\n\n\n\n366 rows × 20 columns\n\n\n\n\nopen_streets_days = join_test[['appronstre', 'BoroName', 'monday', 'tuesday', 'wednesday','thursday', 'friday', 'saturday', 'sunday']]\nopen_streets_days\n\n\n\n\n\n\n\n\nappronstre\nBoroName\nmonday\ntuesday\nwednesday\nthursday\nfriday\nsaturday\nsunday\n\n\n\n\n0\nDEISIUS STREET\nStaten Island\n1\n1\n1\n1\n1\n0\n0\n\n\n1\nSUFFOLK AVENUE\nStaten Island\n0\n0\n0\n0\n1\n1\n1\n\n\n2\nSUFFOLK AVENUE\nStaten Island\n0\n0\n0\n0\n1\n1\n1\n\n\n3\nVERMONT COURT\nStaten Island\n0\n0\n0\n0\n1\n1\n1\n\n\n4\n9 STREET\nStaten Island\n0\n0\n0\n0\n1\n1\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\nBECK STREET\nBronx\n0\n0\n1\n0\n0\n0\n0\n\n\n362\nNEWKIRK AVENUE\nBrooklyn\n0\n0\n0\n0\n0\n0\n1\n\n\n363\n18 STREET\nBrooklyn\n1\n1\n1\n1\n1\n0\n0\n\n\n364\n34 AVENUE\nQueens\n1\n1\n1\n1\n1\n1\n1\n\n\n365\nDECATUR STREET\nBrooklyn\n0\n0\n0\n0\n0\n1\n0\n\n\n\n\n366 rows × 9 columns\n\n\n\n\n\nCensus Data\n\nPop_2020 = gpd.read_file(\"data/NYC_tracts_2020.geojson\")\nPop_2020 = Pop_2020[['GEOID', 'estimate']]\nPop_2020['GEOID']=Pop_2020['GEOID'].astype(int)\n\ntracts_pop = tracts_clean.merge(Pop_2020, on='GEOID', how='left') \nneighbor_pop = tracts_pop.groupby(['NTAName']).sum(['estimate']).reset_index()\n\n\nincome = gpd.read_file(\"data/NYC_Income.geojson\")\ngeo_income = gpd.GeoDataFrame(income, geometry='geometry')\ngeo_income = geo_income.set_crs(epsg=4326)\ngeo_income['GEOID']=geo_income['GEOID'].astype('int')\n\ntracts_income = tracts_clean.merge(geo_income.drop(columns='geometry'), on='GEOID', how='left').dropna()\nneighbor_income = tracts_income.groupby(['NTAName', 'BoroName']).median(['estimate']).reset_index()"
  },
  {
    "objectID": "analysis/Ass-2.html#chart-i-matplotlib-parks-in-neighborhoods",
    "href": "analysis/Ass-2.html#chart-i-matplotlib-parks-in-neighborhoods",
    "title": "Where to spend leisure time in NYC?",
    "section": "Chart I: Matplotlib – Parks in Neighborhoods",
    "text": "Chart I: Matplotlib – Parks in Neighborhoods\nI first hope to investigate into the distribution of Parks in different neighborhoods and boroughs in relation to population. On the one hand, higher population means more people will have need for a bigger public green space for leisure time. On the other hand, less populated neighborhoods tend to have more spaces for parks. And since the count of parks doesn’t perfectly reflect how much space is available, I am using acrage data instead of counts for this analysis.\nTo investigate, I utilized Matplotlib, which is great for making simple scatterplot charts that speaks for simple linear relationship, if there is any.\n\ngeo_parks_clean['acres'] = geo_parks_clean['acres'].astype(float)\nparks_acres_neighborhood = geo_parks_clean.groupby('NTAName').sum().drop(columns=['index_right','CT2020']).reset_index()\nparks_acres_pop = neighbor_pop.merge(parks_acres_neighborhood, on='NTAName', how='left').dropna()\nparks_pop = parks_acres_pop.merge(Boro_NTA, on='NTAName', how='left').dropna()\nparks_pop['acres'].describe()\n\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1268464147.py:2: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  parks_acres_neighborhood = geo_parks_clean.groupby('NTAName').sum().drop(columns=['index_right','CT2020']).reset_index()\n\n\ncount     237.000000\nmean      105.945812\nstd       224.643587\nmin         0.005000\n25%         7.651000\n50%        20.653000\n75%        85.527000\nmax      1930.636136\nName: acres, dtype: float64\n\n\nAfter a quick glance of the park data, I notice some extreme outliers with extremely large parks that not only serves adjacent neighborhoods but the whole city. I excluded those outliers to have a better sense of how much acrage of parks common neighborhoods get.\n\nparks_pop_filtered = parks_pop.loc[(parks_pop['acres'] &lt; 86) & (parks_pop['acres'] &gt; 7) & (parks_pop['estimate'] &gt;0)]\nparks_pop_filtered\n\n\n\n\n\n\n\n\nNTAName\nCT2020\nGEOID_x\nestimate\nacres\nGEOID_y\nBoroName\n\n\n\n\n5\nAstoria (East)-Woodside (North)\n241600\n505134241600\n34825.0\n8.642\n2.886482e+11\nQueens\n\n\n6\nAstoria (North)-Ditmars-Steinway\n214102\n613377214102\n47134.0\n11.530\n4.690532e+11\nQueens\n\n\n10\nBarren Island-Floyd Bennett Field\n70202\n36047070202\n26.0\n64.665\n3.604707e+10\nBrooklyn\n\n\n11\nBath Beach\n235800\n396517235800\n32716.0\n21.398\n1.081411e+11\nBrooklyn\n\n\n15\nBedford Park\n448115\n396055448115\n55702.0\n23.917\n1.800252e+11\nBronx\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n225\nWest Farms\n136300\n180025136300\n18206.0\n13.540\n3.960554e+11\nBronx\n\n\n230\nWhitestone-Beechhurst\n703300\n252567703300\n28353.0\n29.086\n2.164866e+11\nQueens\n\n\n232\nWilliamsburg\n695100\n468611695100\n59410.0\n81.282\n1.045365e+12\nBrooklyn\n\n\n233\nWindsor Terrace-South Slope\n363808\n288376363808\n25442.0\n8.408\n4.325650e+11\nBrooklyn\n\n\n235\nWoodside\n406004\n505134406004\n45417.0\n10.217\n5.412154e+11\nQueens\n\n\n\n\n113 rows × 7 columns\n\n\n\n\n\ncolor_map = {\"Bronx\": \"#550527\", \"Brooklyn\": \"#688E26\", \"Manhattan\": \"#FAA613\", \"Queens\": \"#F44708\", \"Staten Island\": \"#A10702\"}\n\nfig, ax = plt.subplots(figsize=(11,6))\n\nfor BoroName, group_df in parks_pop_filtered.groupby(\"BoroName\"):\n    \n    ax.scatter(\n        group_df[\"estimate\"],\n        group_df[\"acres\"],\n        marker=\"P\",\n        label=BoroName,\n        color=color_map[BoroName],\n        alpha=0.75,\n        zorder=10\n    )\n\nax.legend(loc=\"best\")\nax.set(\n    title = \"Park Space (acres) Relative to Population in Neighborhoods, by Boroughs\",\n    xlabel = \"Population in each neighborhood (2020 Census)\",\n    ylabel = \"Parks in Acres\")\nax.grid(True)\n\nplt.show()\n\n\n\n\nThis chart does not suggest a strong linear relationship between population and park acrages. Overall, Manhattan, Queens and Brooklyn host more large parks, but some neighborhoods are particularly underserved, with very high population and low park acrage (points towards lower right of the chart)."
  },
  {
    "objectID": "analysis/Ass-2.html#chart-ii-seaborn-x2",
    "href": "analysis/Ass-2.html#chart-ii-seaborn-x2",
    "title": "Where to spend leisure time in NYC?",
    "section": "Chart II: Seaborn (x2)",
    "text": "Chart II: Seaborn (x2)\nLooking at data for indoor leisure space and open street data, I utilize seaborn to create two types of charts that suite the nature of the data sets.\nThe first is a grouped bar charts. Similar to park distribution, I hope to have a quick glimpse of number of each type of leisure spaces in each borough, and hope to identify any general spatial patterns or inequalities. A grouped chart is great a revealing such pattern.\nThe second is a heatmap for open street data. The heatmap explores the number of open streets approved in different boroughs on different days of the week.\n\nIndoor Leisure Spaces: Grouped Bar Charts\n\nindoor_clean = geo_total_indoor.groupby(['Type','BoroName']).count().reset_index().drop(columns=['Zip','Address','geometry','index_right','CT2020','NTAName']).pivot(index='Type',columns='BoroName', values='Name').fillna(0).reset_index()\nindoor_melt = indoor_clean.melt(id_vars='Type', value_vars=['Bronx','Brooklyn','Manhattan', 'Queens', 'Staten Island'])\nindoor_melt\n\n\n\n\n\n\n\n\nType\nBoroName\nvalue\n\n\n\n\n0\nArt Galleries\nBronx\n6.0\n\n\n1\nLibraries\nBronx\n35.0\n\n\n2\nMuseums\nBronx\n8.0\n\n\n3\nTheatres\nBronx\n0.0\n\n\n4\nArt Galleries\nBrooklyn\n61.0\n\n\n5\nLibraries\nBrooklyn\n59.0\n\n\n6\nMuseums\nBrooklyn\n12.0\n\n\n7\nTheatres\nBrooklyn\n0.0\n\n\n8\nArt Galleries\nManhattan\n823.0\n\n\n9\nLibraries\nManhattan\n44.0\n\n\n10\nMuseums\nManhattan\n87.0\n\n\n11\nTheatres\nManhattan\n115.0\n\n\n12\nArt Galleries\nQueens\n24.0\n\n\n13\nLibraries\nQueens\n65.0\n\n\n14\nMuseums\nQueens\n12.0\n\n\n15\nTheatres\nQueens\n2.0\n\n\n16\nArt Galleries\nStaten Island\n3.0\n\n\n17\nLibraries\nStaten Island\n13.0\n\n\n18\nMuseums\nStaten Island\n9.0\n\n\n19\nTheatres\nStaten Island\n0.0\n\n\n\n\n\n\n\n\nsns.set_theme(style=\"whitegrid\")\n\ncolor_map = [\"#550527\", \"#688E26\", \"#FAA613\", \"#F44708\", \"#A10702\"]\nsns.set_palette(color_map)\n\nsns.catplot(\n    data=indoor_melt, kind=\"bar\",\n    x=\"Type\", \n    y=\"value\", \n    hue=\"BoroName\",\n    aspect=2, \n    alpha=1\n).set_axis_labels(\n    \"Type of Leisure Space\", \"Counts\"\n).set(title=\"Distribution of 4 Types of Leisure Spaces in Each Borough\")\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\nDays and Location of Open Streets in NYC: A Heatmap\n\n#heatmap for open_streets: borough x Days of the week \nopen_streets_days_melt= open_streets_days.melt(id_vars=['appronstre','BoroName'], value_vars=['monday','tuesday','wednesday', 'thursday', 'friday','saturday','sunday'])\n\n\n\n\n\n\n\n\nappronstre\nBoroName\nvariable\nvalue\n\n\n\n\n0\nDEISIUS STREET\nStaten Island\nmonday\n1\n\n\n1\nSUFFOLK AVENUE\nStaten Island\nmonday\n0\n\n\n2\nSUFFOLK AVENUE\nStaten Island\nmonday\n0\n\n\n3\nVERMONT COURT\nStaten Island\nmonday\n0\n\n\n4\n9 STREET\nStaten Island\nmonday\n0\n\n\n...\n...\n...\n...\n...\n\n\n2557\nBECK STREET\nBronx\nsunday\n0\n\n\n2558\nNEWKIRK AVENUE\nBrooklyn\nsunday\n1\n\n\n2559\n18 STREET\nBrooklyn\nsunday\n0\n\n\n2560\n34 AVENUE\nQueens\nsunday\n1\n\n\n2561\nDECATUR STREET\nBrooklyn\nsunday\n0\n\n\n\n\n2562 rows × 4 columns\n\n\n\n\nopen_streets_seaborn = open_streets_days_melt.groupby(['variable','BoroName']).sum().reset_index()\n\n# sort the order of day from monday to sunday \n\norder=['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\nopen_streets_seaborn['variable'] = pd.Categorical(open_streets_seaborn['variable'], categories=order, ordered=True)\nopen_streets_seaborn = open_streets_seaborn.sort_values(by='variable')\n\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/3045256453.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  open_streets_seaborn = open_streets_days_melt.groupby(['variable','BoroName']).sum().reset_index()\n\n\n\nimport seaborn as sns\n\nsns.set_theme()\n\n# Load the example flights dataset and convert to long-form\n\nopen_street_heatmap = (\n    open_streets_seaborn\n    .pivot(index=\"BoroName\", columns=\"variable\", values=\"value\")\n)\n\n# Draw a heatmap with the numeric values in each cell\nf, ax = plt.subplots(figsize=(9, 6))\n\nax = sns.heatmap(open_street_heatmap, annot=True, linewidths=.5)\nax.set(xlabel=\"Days in a week\", ylabel=\"Borough\", title=\"Number of Streets Open on Specific Days\")\n\n\n[Text(0.5, 33.249999999999986, 'Days in a week'),\n Text(79.75, 0.5, 'Borough'),\n Text(0.5, 1.0, 'Number of Streets Open on Specific Days')]\n\n\n\n\n\nFrom both charts, we can see Manhattan has disproportional number of art galleries and open streets in comparison to other boroughs. Museums and theatres are predominantly located on Manhattan as well. Interestingly, the distribution of libraries seems more even. However, overall, we are seeing leisure spaces are disproportional abundant and diverse in Manhattan, then brooklyn and queens, leaving Bronx and Staten Island less resourceful."
  },
  {
    "objectID": "analysis/Ass-2.html#chart-iii-altair-charts-x3",
    "href": "analysis/Ass-2.html#chart-iii-altair-charts-x3",
    "title": "Where to spend leisure time in NYC?",
    "section": "Chart III: Altair Charts (x3)",
    "text": "Chart III: Altair Charts (x3)\nFurthering the exploration on indoor leisure spaces, I created one chart on its ditribution in relation with median household income, to see if it embeds any socio-economic inequality. I then creatd a map to visualize this relationship, which can also be helpful to locate different kinds of leisure spaces in the city.\nMy third chart is an interactive bar chart on open streets in Brookylen, where audience can choose the day of the week to see all open streets approved for that day and the time they open and close. This could potentially be developed into a tool for residents and tourists to track open streets.\n\nBrush Selection: Indoor Space and Income\n\nneighbor_indoor = geo_total_indoor.groupby(['NTAName','Type']).count().reset_index().drop(['Zip', 'Address', 'geometry','index_right','BoroName', 'CT2020'], axis=1)\nneighbor_indoor = neighbor_indoor.rename(columns={'Name': 'Count'})\nneighbor_income_indoor = neighbor_income.merge(neighbor_indoor, on='NTAName', how='left')\n\nneighbor_income_indoor_pivot=neighbor_income_indoor.pivot(index=['NTAName', 'BoroName','estimate'], columns=\"Type\", values=\"Count\").reset_index().fillna(0)\nneighbor_income_indoor_pivot_1 = neighbor_income_indoor_pivot.drop(neighbor_income_indoor_pivot.columns[[3]],axis=1)\nincome_indoor_pivot = neighbor_income_indoor_pivot_1.melt(id_vars=[\"NTAName\", \"estimate\", \"BoroName\"], value_vars=[\"Art Galleries\", \"Libraries\", \"Museums\", \"Theatres\"], var_name=\"Types\",value_name=\"Count\")\nincome_indoor_pivot_filtered = income_indoor_pivot.loc[(income_indoor_pivot['Count'] &lt;20)]\nincome_indoor_pivot_filtered = income_indoor_pivot_filtered.rename(columns={'estimate': 'Median Household Income'})\n\n\nbrush = alt.selection_interval()\n\nBrush_Chart = (\nalt.Chart(income_indoor_pivot_filtered)\n   .mark_point()\n   .encode(\n       x=alt.X(\"Median Household Income:Q\", scale=alt.Scale(zero=False)),\n       y=alt.Y(\"Count:Q\", scale=alt.Scale(zero=False)),\n       color=alt.condition(brush, \"BoroName:N\", alt.value(\"lightgray\")),\n       tooltip=[\"NTAName\",\"BoroName:N\", \"Median Household Income:Q\", \"Count:Q\"])\n   .add_params(brush)\n   .properties(width=200, height=200)\n   .facet(column=\"Types:N\")\n)\n\nBrush_Chart\n\n\n\n\n\n\n\n\n\nMap: Income and Indoor Leisure Spaces - Relationships\n\nNTA = pd.read_csv(\"data/2020 Neighborhood.csv\")\nNTA['geometry'] = gpd.GeoSeries.from_wkt(NTA['the_geom'])\nNTA_geo = gpd.GeoDataFrame(NTA, geometry='geometry', crs=4326)\nNTA_geo = NTA_geo.to_crs(epsg=2263)\ngeo_total_indoor = geo_total_indoor.to_crs(epsg=2263)\n\ngeo_NTA = NTA_geo[['NTAName', 'geometry']]\n\ntracts_income_1 = tracts_clean.merge(geo_income.drop(columns='geometry'), on='GEOID', how='left').dropna()\nneighbor_income_1 = tracts_income.groupby(['NTAName', 'BoroName']).median(['estimate']).reset_index()\n\nNTA_income = geo_NTA.merge(neighbor_income_1, on='NTAName', how='left')\n\n\ngeo_total_indoor_1 = geo_total_indoor\n\ngeo_total_indoor_1['lon'] = geo_total_indoor_1['geometry'].x\ngeo_total_indoor_1['lat'] = geo_total_indoor_1['geometry'].y\n\n\nIncome = (\n    alt.Chart(NTA_income)\n    .mark_geoshape(stroke=\"white\")\n    .encode(\n        tooltip=[\"NTAName:N\", \"estimate:Q\", \"moe:Q\"],\n        color=alt.Color(\"estimate:Q\", scale=alt.Scale(scheme=\"greys\")),\n    )\n    # Important! Otherwise altair will try to re-project your data\n    .project(type=\"identity\", reflectY=True)\n    .properties(width=1000, height=800).interactive()\n)\n\nIndoorSpaces = (\n    alt.Chart(geo_total_indoor_1)\n    .mark_circle(size=10)\n    .encode(tooltip=['Name','Type','Address'],\n           longitude=\"lon\", latitude=\"lat\",\n           color=alt.Color('Type:N', scale=alt.Scale(scheme=\"lightmulti\"))\n         ).project(type=\"identity\", reflectY=True)\n)\n\n\n\nmap_1 = Income + IndoorSpaces\nmap_1\n\n\n\n\n\n\n\n\nSimilarly, libraries seem to be the least discriminatory type of leisure space. For art galleries and museums, while there are some distributed in mid to lower income neighborhood, the higher income neighborhood has higher density of such leisure space. Theatres, at the same time, is mostly located in mid to higher income neighborhood. This trend can be clearly see on the map too. Such spatial distribution means, for residents in lower to mid income neighborhood, they might have to travel further for accessing those spaces.\n\n\nOpen Street Time\n\nopen_csv = pd.read_csv(\"data/Open Streets CSV.csv\")\nopen_csv['geometry'] = gpd.GeoSeries.from_wkt(open_csv['the Geom'])\nopen_csv_geo = gpd.GeoDataFrame(open_csv, geometry='geometry', crs=2263)\nopen_csv_geo = open_csv_geo.to_crs(epsg=4326)\n\nopen_7days_time = open_csv_geo.drop(['apprDaysWe','Object ID', 'Organization Name', 'Approved From Street', 'Approved To Street', 'apprStartD', 'apprEndDat', 'Shape_STLe', 'segmentidt', 'segmentidf', 'lionversion', 'the Geom'], axis=1)\nopen_7days_time = open_7days_time.drop_duplicates(subset = \"Approved On Street\")\nopen_7days_time_melt = open_7days_time.melt(id_vars=['Approved On Street', 'Borough Name'], value_vars=['Approved Monday Open', 'Approved Monday Close', 'Approved Tuesday Open', 'Approved Tuesday Close', 'Approved Wednesday Open', 'Approved Wednesday Close', 'Approved Thursday Open', 'Approved Thursday Close', 'Approved Friday Open', 'Approved Friday Close', 'Approved Saturday Open', 'Approved Saturday Close', 'Approved Sunday Open', 'Approved Sunday Close']).dropna()\n#open_7days_time_melt['value'] =  pd.to_datetime(open_7days_time_melt['value']).dt.time\nopen_time_Brooklyn = open_7days_time_melt[(open_7days_time_melt['Borough Name'] == 'Brooklyn')]\nopen_time_Brooklyn\n\n\n\n\n\n\n\n\nApproved On Street\nBorough Name\nvariable\nvalue\n\n\n\n\n6\nRIDGE BOULEVARD\nBrooklyn\nApproved Monday Open\n10:00\n\n\n9\n82 STREET\nBrooklyn\nApproved Monday Open\n08:30\n\n\n10\n48 STREET\nBrooklyn\nApproved Monday Open\n13:30\n\n\n11\n43 STREET\nBrooklyn\nApproved Monday Open\n09:30\n\n\n12\nALBEMARLE ROAD\nBrooklyn\nApproved Monday Open\n08:00\n\n\n...\n...\n...\n...\n...\n\n\n2282\nJEFFERSON AVENUE\nBrooklyn\nApproved Sunday Close\n21:00\n\n\n2287\nSHARON STREET\nBrooklyn\nApproved Sunday Close\n20:00\n\n\n2288\nTROUTMAN STREET\nBrooklyn\nApproved Sunday Close\n22:00\n\n\n2289\nRANDOLPH STREET\nBrooklyn\nApproved Sunday Close\n23:00\n\n\n2354\nLEXINGTON AVENUE\nBrooklyn\nApproved Sunday Close\n20:00\n\n\n\n\n378 rows × 4 columns\n\n\n\n\nopen_time_Brooklyn['time'] = open_7days_time_melt['variable'].str.extract('(Open|Close)')\nopen_time_Brooklyn['dayweek'] = open_7days_time_melt['variable'].str.extract('(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)')\nopen_time_Brooklyn= open_time_Brooklyn.pivot(index=['Approved On Street', 'dayweek'], columns=\"time\", values=\"value\").reset_index()\n\norder=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nopen_time_Brooklyn['dayweek'] = pd.Categorical(open_time_Brooklyn['dayweek'], categories=order, ordered=True)\nopen_time_Brooklyn = open_time_Brooklyn.sort_values(by='dayweek').reset_index()\n\nopen_time_Brooklyn\n\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1900940832.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  open_time_Brooklyn['time'] = open_7days_time_melt['variable'].str.extract('(Open|Close)')\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1900940832.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  open_time_Brooklyn['dayweek'] = open_7days_time_melt['variable'].str.extract('(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)')\n\n\n\n\n\n\n\n\ntime\nindex\nApproved On Street\ndayweek\nClose\nOpen\n\n\n\n\n0\n139\nSUMMIT STREET\nMonday\n14:30\n12:30\n\n\n1\n151\nUNDERHILL AVENUE\nMonday\n20:00\n08:00\n\n\n2\n28\n82 STREET\nMonday\n15:30\n08:30\n\n\n3\n143\nTHATFORD AVENUE\nMonday\n18:00\n11:00\n\n\n4\n33\nAITKEN PLACE\nMonday\n15:00\n11:00\n\n\n...\n...\n...\n...\n...\n...\n\n\n184\n147\nTOMPKINS AVENUE\nSunday\n20:00\n11:00\n\n\n185\n51\nBEVERLEY ROAD\nSunday\n18:00\n10:00\n\n\n186\n112\nRANDOLPH STREET\nSunday\n23:00\n12:00\n\n\n187\n10\n4 STREET\nSunday\n22:00\n08:00\n\n\n188\n157\nVANDERBILT AVENUE\nSunday\n23:00\n11:00\n\n\n\n\n189 rows × 5 columns\n\n\n\n\nselection = alt.selection_multi(fields=['dayweek'])\ncolor = alt.condition(selection,\n                      alt.Color('dayweek:N', legend=None, \n                      scale=alt.Scale(scheme='category10')),\n                      alt.value('lightgray'))\n\nopacity = alt.condition(selection,\n                        alt.value(1), alt.value(0))\n\n\n\nbar = alt.Chart(open_time_Brooklyn).mark_bar().encode(\n        x='Open',\n        x2='Close',\n        y='Approved On Street',\n        color=color,\n        opacity=opacity,\n        tooltip=['Open', 'Close', 'Approved On Street', 'dayweek']).properties(\n        width=500,\n        height=1000).interactive()\n\nlegend = alt.Chart(open_time_Brooklyn).mark_bar().encode(\n    y=alt.Y('dayweek:N', axis=alt.Axis(orient='right')),\n    color=color\n).add_selection(\nselection\n)\n\nA_Chart = bar | legend\nA_Chart\n\n\n\n\n\n\n\n\nThis interactive bar chart on Brooklyn Open Street serves as a pilot that can be adapted for data of all five boroughs. By comparing to open and close time, we can observe that many streets are approved to be open streets with later opening and closing time on weekends. To improve this bar chart the status of each street can be added (i.e., whether they are approved to be closed fully or partially or only on school days), which is important for visitors as well."
  },
  {
    "objectID": "analysis/Ass-2.html#dashboard",
    "href": "analysis/Ass-2.html#dashboard",
    "title": "Where to spend leisure time in NYC?",
    "section": "Dashboard",
    "text": "Dashboard\nThe dashboard below is an upgrade from the indoor space and income grouped scatter plot. This dashboard makes it easier to explore the spatial distribution of indoor spaces of certain type or in neighborhoods with certain level of income. Through selecting points with higher counts, we can see neighborhoods with higher density of identified indoor leisure spaces are mostly located in Manhattan.\n\nbrush = alt.selection_interval()\n\npoints = alt.Chart(income_indoor_pivot_filtered).mark_point().encode(\n       x=alt.X(\"Median Household Income:Q\", scale=alt.Scale(zero=False)),\n       y=alt.Y(\"Count:Q\", scale=alt.Scale(zero=False)),\n       color=alt.condition(brush, \"BoroName:N\", alt.value(\"lightgray\")),\n       tooltip=[\"NTAName\",\"BoroName:N\", \"Median Household Income:Q\", \"Count:Q\"]).add_params(brush\n).properties(width=200, height=200\n).facet(column=\"Types:N\")\n\n\nbars = alt.Chart(income_indoor_pivot_filtered).mark_bar().encode(\n    y='BoroName:N',\n    color='BoroName:N',\n    x='Count:Q'\n).transform_filter(\n    brush\n)\n\nDashboard = points & bars\nDashboard"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap"
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "In-Class Projects",
    "section": "",
    "text": "In-Class Projects\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features."
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007"
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ann Zhang Python Projects",
    "section": "",
    "text": "Here exhibits course assignments completed as a part of MUSA 550 Geospatial Data Science in Python. The website is made possible using Quarto.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Ann Zhang Python Projects",
    "section": "",
    "text": "Here exhibits course assignments completed as a part of MUSA 550 Geospatial Data Science in Python. The website is made possible using Quarto.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online."
  },
  {
    "objectID": "index.html#find-out-more",
    "href": "index.html#find-out-more",
    "title": "Ann Zhang Python Projects",
    "section": "Find out more",
    "text": "Find out more\nMy personal online website can be found here: https://annannzhang.github.io/"
  },
  {
    "objectID": "analysis/assignment-1.html",
    "href": "analysis/assignment-1.html",
    "title": "1 - Philly Donut Effect",
    "section": "",
    "text": "In this assignment, we will practice our pandas skills and explore the “Donut Effect” within Philadelphia. The “Donut Effect” describes the following phenomenon: with more flexible working options and pandemic-driven density fears, people left urban dense cores and opted for more space in city suburbs, driving home and rental prices up in the suburbs relative to city centers.\nWe will be working with Zillow data for the Zillow Home Value Index (ZHVI) for Philadelphia ZIP codes. The goal will be to calculate home price appreciation in Philadelphia, comparing those ZIP codes in Center City (the central business district) to those not in Center City."
  },
  {
    "objectID": "analysis/assignment-1.html#load-the-data",
    "href": "analysis/assignment-1.html#load-the-data",
    "title": "1 - Philly Donut Effect",
    "section": "1. Load the data",
    "text": "1. Load the data\nI’ve already downloaded the relevant data file and put in the data/ folder. Let’s load it using pandas.\nNote: Be sure to use a relative file path to make it easier to load your data when grading. See this guide for more info.\n\n\nCode\nimport pandas as pd\n\n\n\n\nCode\nimport numpy as np\n\n\n\n\nCode\nzip_df = pd.read_csv(\"data/Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")"
  },
  {
    "objectID": "analysis/assignment-1.html#trim-the-data-to-just-philadelphia",
    "href": "analysis/assignment-1.html#trim-the-data-to-just-philadelphia",
    "title": "1 - Philly Donut Effect",
    "section": "2. Trim the data to just Philadelphia",
    "text": "2. Trim the data to just Philadelphia\nSelect the subset of the dataframe for Philadelphia, PA.\n\n\nCode\n# or use isin to trim data\n\nstate_trim = zip_df[\"StateName\"].isin([\"PA\"])\nstate_trim_df= zip_df.loc[state_trim]\n\nphilly_df = state_trim_df.loc[state_trim_df[\"City\"] == \"Philadelphia\"]\nphilly_df.head()\n\n\n\n\n\n\n\n\n\nRegionID\nSizeRank\nRegionName\nRegionType\nStateName\nState\nCity\nMetro\nCountyName\n2000-01-31\n...\n2021-10-31\n2021-11-30\n2021-12-31\n2022-01-31\n2022-02-28\n2022-03-31\n2022-04-30\n2022-05-31\n2022-06-30\n2022-07-31\n\n\n\n\n125\n65810\n126\n19143\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n60701.0\n...\n173114.0\n172087.0\n171445.0\n171542.0\n171680.0\n171878.0\n171607.0\n171333.0\n171771.0\n172611.0\n\n\n247\n65779\n249\n19111\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n85062.0\n...\n257911.0\n260104.0\n262257.0\n263715.0\n264809.0\n265684.0\n267222.0\n269460.0\n272201.0\n274446.0\n\n\n338\n65791\n340\n19124\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n47155.0\n...\n156225.0\n157780.0\n159029.0\n159274.0\n159886.0\n160780.0\n161929.0\n163625.0\n165020.0\n166009.0\n\n\n423\n65787\n426\n19120\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n59285.0\n...\n161167.0\n161807.0\n162634.0\n162972.0\n163597.0\n164008.0\n164887.0\n165860.0\n167321.0\n168524.0\n\n\n509\n65772\n512\n19104\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n74255.0\n...\n220270.0\n221454.0\n222006.0\n220760.0\n217933.0\n216447.0\n216424.0\n218663.0\n220453.0\n223443.0\n\n\n\n\n5 rows × 280 columns"
  },
  {
    "objectID": "analysis/assignment-1.html#melt-the-data-into-tidy-format",
    "href": "analysis/assignment-1.html#melt-the-data-into-tidy-format",
    "title": "1 - Philly Donut Effect",
    "section": "3. Melt the data into tidy format",
    "text": "3. Melt the data into tidy format\nLet’s transform the data from wide to tidy using the pd.melt() function. Create a new column in your data called “ZHVI” that holds the ZHVI values.\n\n\nCode\ndef looks_like_a_date(col):\n    \"\"\"A function that tests if a string starts with '20'\"\"\"\n    \n    return col.startswith(\"20\")\n\n\n\n\nCode\nphilly_tidy = philly_df.melt(\n    id_vars = [\"RegionName\",\"RegionID\"],\n    value_vars = list(filter(looks_like_a_date, philly_df.columns)),\n    var_name = \"Date\",\n    value_name = \"ZHVI\",\n)\n    \n\n\n\n\nCode\nphilly_tidy\n\n\n\n\n\n\n\n\n\nRegionName\nRegionID\nDate\nZHVI\n\n\n\n\n0\n19143\n65810\n2000-01-31\n60701.0\n\n\n1\n19111\n65779\n2000-01-31\n85062.0\n\n\n2\n19124\n65791\n2000-01-31\n47155.0\n\n\n3\n19120\n65787\n2000-01-31\n59285.0\n\n\n4\n19104\n65772\n2000-01-31\n74255.0\n\n\n...\n...\n...\n...\n...\n\n\n12461\n19153\n65820\n2022-07-31\n247560.0\n\n\n12462\n19118\n65785\n2022-07-31\n746009.0\n\n\n12463\n19102\n65770\n2022-07-31\n347614.0\n\n\n12464\n19127\n65794\n2022-07-31\n318732.0\n\n\n12465\n19137\n65804\n2022-07-31\n222107.0\n\n\n\n\n12466 rows × 4 columns"
  },
  {
    "objectID": "analysis/assignment-1.html#split-the-data-for-zip-codes-inoutside-center-city",
    "href": "analysis/assignment-1.html#split-the-data-for-zip-codes-inoutside-center-city",
    "title": "1 - Philly Donut Effect",
    "section": "4. Split the data for ZIP codes in/outside Center City",
    "text": "4. Split the data for ZIP codes in/outside Center City\nTo compare home appreciation in Center City vs. outside Center City, we’ll need to split the data into two dataframes, one that holds the Center City ZIP codes and one that holds the data for the rest of the ZIP codes in Philadelphia.\nTo help with this process, I’ve included a list of ZIP codes that make up the “greater Center City” region of Philadelphia. Use this list to split the melted data into two dataframes.\n\n\nCode\ngreater_center_city_zip_codes = [\n    19123,\n    19102,\n    19103,\n    19106,\n    19107,\n    19109,\n    19130,\n    19146,\n    19147,\n]\n\n\n\n\nCode\n# CENTER CITY\ncenter_city_zip = philly_tidy[\"RegionName\"].isin(greater_center_city_zip_codes)\ncenter_city_zip\ncenter_city = philly_tidy.loc[center_city_zip].copy()\ncenter_city\n\n\n\n\n\n\n\n\n\nRegionName\nRegionID\nDate\nZHVI\n\n\n\n\n9\n19146\n65813\n2000-01-31\n97460.0\n\n\n12\n19147\n65814\n2000-01-31\n119919.0\n\n\n14\n19103\n65771\n2000-01-31\n183436.0\n\n\n18\n19130\n65797\n2000-01-31\n128477.0\n\n\n33\n19107\n65775\n2000-01-31\n128049.0\n\n\n...\n...\n...\n...\n...\n\n\n12438\n19130\n65797\n2022-07-31\n431501.0\n\n\n12453\n19107\n65775\n2022-07-31\n330958.0\n\n\n12457\n19123\n65790\n2022-07-31\n443152.0\n\n\n12458\n19106\n65774\n2022-07-31\n407423.0\n\n\n12463\n19102\n65770\n2022-07-31\n347614.0\n\n\n\n\n2168 rows × 4 columns\n\n\n\n\n\nCode\n# Out_of_Center_City\nout_of_center_city_zip = ~philly_tidy[\"RegionName\"].isin(greater_center_city_zip_codes)\nout_of_center_city_zip\nout_of_center_city = philly_tidy.loc[out_of_center_city_zip].copy()\nout_of_center_city\n\n\n\n\n\n\n\n\n\nRegionName\nRegionID\nDate\nZHVI\n\n\n\n\n0\n19143\n65810\n2000-01-31\n60701.0\n\n\n1\n19111\n65779\n2000-01-31\n85062.0\n\n\n2\n19124\n65791\n2000-01-31\n47155.0\n\n\n3\n19120\n65787\n2000-01-31\n59285.0\n\n\n4\n19104\n65772\n2000-01-31\n74255.0\n\n\n...\n...\n...\n...\n...\n\n\n12460\n19129\n65796\n2022-07-31\n302177.0\n\n\n12461\n19153\n65820\n2022-07-31\n247560.0\n\n\n12462\n19118\n65785\n2022-07-31\n746009.0\n\n\n12464\n19127\n65794\n2022-07-31\n318732.0\n\n\n12465\n19137\n65804\n2022-07-31\n222107.0\n\n\n\n\n10298 rows × 4 columns"
  },
  {
    "objectID": "analysis/assignment-1.html#compare-home-value-appreciation-in-philadelpia",
    "href": "analysis/assignment-1.html#compare-home-value-appreciation-in-philadelpia",
    "title": "1 - Philly Donut Effect",
    "section": "5. Compare home value appreciation in Philadelpia",
    "text": "5. Compare home value appreciation in Philadelpia\nIn this step, we’ll calculate the average percent increase in ZHVI from March 2020 to March 2022 for ZIP codes in/out of Center City. We’ll do this by:\n\nWriting a function (see the template below) that will calculate the percent increase in ZHVI from March 31, 2020 to March 31, 2022\nGroup your data and apply this function to calculate the ZHVI percent change for each ZIP code in Philadelphia. Do this for both of your dataframes from the previous step.\nCalculate the average value across ZIP codes for both sets of ZIP codes and then compare\n\nYou should see much larger growth for ZIP codes outside of Center City…the Donut Effect!\n\n\nCode\ndef calculate_percent_increase(group_df):\n    \n    march_20sel = group_df[\"Date\"] == \"2020-03-31\"\n    march_22sel = group_df[\"Date\"] == \"2022-03-31\"\n    \n    march_20 = group_df.loc[march_20sel].squeeze()\n    march_22 = group_df.loc[march_22sel].squeeze()\n    \n    columns = [\"ZHVI\"]\n    \n    return (march_22[columns] / march_20[columns] - 1) * 100\n\n\n\n\nCode\n# Center City Grouped\ngrouped_center_city = center_city.groupby(\"RegionName\")\nresult_center_city = grouped_center_city.apply(calculate_percent_increase)\nresult_center_city\n\n\n\n\n\n\n\n\n\nZHVI\n\n\nRegionName\n\n\n\n\n\n19102\n-1.716711\n\n\n19103\n-1.696176\n\n\n19106\n2.520802\n\n\n19107\n2.883181\n\n\n19123\n5.212747\n\n\n19130\n6.673031\n\n\n19146\n6.480788\n\n\n19147\n6.139806\n\n\n\n\n\n\n\n\n\nCode\n# Outside Center City Grouped\ngrouped_outside = out_of_center_city.groupby(\"RegionName\")\nresult_outside = grouped_outside.apply(calculate_percent_increase)\nresult_outside\n\n\n\n\n\n\n\n\n\nZHVI\n\n\nRegionName\n\n\n\n\n\n19104\n14.539797\n\n\n19111\n28.690446\n\n\n19114\n21.074312\n\n\n19115\n22.455454\n\n\n19116\n23.079842\n\n\n19118\n17.585001\n\n\n19119\n17.478667\n\n\n19120\n26.927423\n\n\n19121\n26.228643\n\n\n19122\n10.275804\n\n\n19124\n28.743474\n\n\n19125\n11.007135\n\n\n19126\n20.819254\n\n\n19127\n20.023926\n\n\n19128\n21.887555\n\n\n19129\n15.598565\n\n\n19131\n23.363129\n\n\n19132\n72.218386\n\n\n19133\n36.143992\n\n\n19134\n23.936841\n\n\n19135\n28.115259\n\n\n19136\n26.487833\n\n\n19137\n23.248505\n\n\n19138\n24.662626\n\n\n19139\n37.008969\n\n\n19140\n57.150847\n\n\n19141\n26.441684\n\n\n19142\n44.564396\n\n\n19143\n23.951077\n\n\n19144\n21.094020\n\n\n19145\n7.634693\n\n\n19148\n6.963237\n\n\n19149\n24.916458\n\n\n19150\n18.735248\n\n\n19151\n19.651429\n\n\n19152\n21.993528\n\n\n19153\n38.240461\n\n\n19154\n17.930932\n\n\n\n\n\n\n\n\n\nCode\n# Center City Growth\nresult_cc_mean = result_center_city.mean().squeeze()\n\n\n\n\nCode\n# Outside of Center City Growth\nresult_ot_mean = result_outside.mean().squeeze()\n\n\n\n\nCode\nHV_growth = pd.Series([result_cc_mean,result_ot_mean])\nLocation = pd.Series([\"Center City\", \"Outside Center City\"])\nfinal_result_df = pd.DataFrame({\"Location\": Location, \"Home Value growth (%)\": HV_growth})\nfinal_result_df\n\n\n\n\n\n\n\n\n\nLocation\nHome Value growth (%)\n\n\n\n\n0\nCenter City\n3.312183\n\n\n1\nOutside Center City\n25.022864\n\n\n\n\n\n\n\nDonut Effect it is!"
  },
  {
    "objectID": "analysis/assignment-2.html",
    "href": "analysis/assignment-2.html",
    "title": "2 - Where to spend leisure time in NYC?",
    "section": "",
    "text": "This project aims to explore the spatial distribution of some leisure spaces in New York City, including indoor spaces like museums, art galleries, and theatres, and outddoor spaces like parks or open streets. The project first aims to idenfity some patterns of spatial distribution of such spaces in each borough and neighborhood, and in relation to population and median household income. Then some interactive visualization is created for both local NYC residents and tourists to access to information about some leisure spaces more easily."
  },
  {
    "objectID": "analysis/assignment-2.html#imports",
    "href": "analysis/assignment-2.html#imports",
    "title": "2 - Where to spend leisure time in NYC?",
    "section": "Imports",
    "text": "Imports\n\n\nCode\nimport altair as alt\nimport geopandas as gpd\nimport hvplot.pandas\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport datetime\nimport math\n\n%matplotlib inline"
  },
  {
    "objectID": "analysis/assignment-2.html#nyc-base-maps",
    "href": "analysis/assignment-2.html#nyc-base-maps",
    "title": "2 - Where to spend leisure time in NYC?",
    "section": "NYC Base Maps",
    "text": "NYC Base Maps\n\nTracts, Neighborhood, and Borough\nnyc_tracts = pd.read_csv(“data/2022 Census Tracts.csv”) nyc_tracts[‘geometry’] = gpd.GeoSeries.from_wkt(nyc_tracts[‘the_geom’]) geo_tracts = gpd.GeoDataFrame(nyc_tracts, geometry=‘geometry’) geo_tracts = geo_tracts.set_crs(epsg=4326) tracts = geo_tracts[[‘BoroName’, ‘CT2020’, ‘BoroCT2020’, ‘NTAName’, ‘Shape_Area’,‘geometry’,‘GEOID’]].copy() tracts_clean = tracts[[‘BoroName’, ‘CT2020’, ‘NTAName’, ‘geometry’,‘GEOID’]] Boro_NTA = tracts_clean[[‘BoroName’, ‘NTAName’]].drop_duplicates(subset = “NTAName”)"
  },
  {
    "objectID": "analysis/assignment-2.html#dataset-selection-setup",
    "href": "analysis/assignment-2.html#dataset-selection-setup",
    "title": "2 - Where to spend leisure time in NYC?",
    "section": "Dataset Selection & Setup",
    "text": "Dataset Selection & Setup\nAfter glancing through NYC Open Data portal, I have selected art galleries, museums, libraries, theatres, and parks as common leisure spaces. In addition, I explored data of open streets, a program thatt had been implemented in many global cities including New York City to transform roads into public spaces for cultural and all kinds of events on particular days (mostly on weekends).\nThis step is to bring in all datasets, clean them up, and aggregate different types of leisure spaces. The aggregated dataframe is then spacially joined with tracts, neighborhood, and boroughs, ready for further investigation on their geospatial distributions.\nSimilar data wrangling is performed on parks. Since parks come in polygon instead of points, which may result in problems with spatial joins, in the case when one park falls in two or more tracts or neighborhood. Hence, the geometry of parks’ centroids is adopted to replace the polygon geometry for further analysis.\nThe Open Street data comes with more detailed information on the approved time for each street. The days of week (e.g. Monday) and time of the day (e.g. 7:30) that it opens and closes are rearranged by melting and pivoting.\nAdditionally, 2020 Census Data on population and median household income are brought in for analysis.\n\nIndoor Leisure Spaces: Art Galleries, Museums, Libraries, and Theatres\n\n\nCode\n# establish geodataframe\nart_galleries = gpd.read_file(\"data/art galleries.geojson\")\ngeo_art_galleries = gpd.GeoDataFrame(art_galleries, geometry='geometry')\ngeo_art_galleries = art_galleries.set_crs(epsg=4326)\n\n# add type\nart_galleries_clean = geo_art_galleries[['name','zip','address1','geometry']]\nart_galleries_clean.loc[:,\"Type\"]= \"Art Galleries\"\nart_galleries_clean.rename(\n    columns={\"address1\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\n\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1429971093.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  art_galleries_clean.rename(\n\n\n\n\nCode\n# Museums \n\n# establish geodataframe\nmuseums = gpd.read_file(\"data/museums.geojson\")\ngeo_museums= gpd.GeoDataFrame(museums, geometry='geometry')\ngeo_museums = geo_museums.set_crs(epsg=4326)\ngeo_museums\n\n# add type\nmuseums_clean = geo_museums[['name','zip','adress1','geometry']]\nmuseums_clean.loc[:,\"Type\"]= \"Museums\"\nmuseums_clean.rename(\n    columns={\"adress1\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\nmuseums_clean\n\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/3071166290.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  museums_clean.rename(\n\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\n\n\n\n\n0\nAlexander Hamilton U.S. Custom House\n10004.0\n1 Bowling Grn\nPOINT (-74.01376 40.70382)\nMuseums\n\n\n1\nAlice Austen House Museum\n10305.0\n2 Hylan Blvd\nPOINT (-74.06303 40.61512)\nMuseums\n\n\n2\nAmerican Academy of Arts and Letters\n10032.0\n633 W. 155th St.\nPOINT (-73.94730 40.83385)\nMuseums\n\n\n3\nAmerican Folk Art Museum\n10019.0\n45 West 53rd Street\nPOINT (-73.97810 40.76162)\nMuseums\n\n\n4\nAmerican Immigration History Center\n0.0\nEllis Island\nPOINT (-74.03968 40.69906)\nMuseums\n\n\n...\n...\n...\n...\n...\n...\n\n\n125\nAmerican Sephardi Federation / Sephardic House\n10011.0\n15 W. 16th St.\nPOINT (-73.99389 40.73808)\nMuseums\n\n\n126\nYIVO Institute for Jewish Research\n10011.0\n15 W. 16th St.\nPOINT (-73.99379 40.73796)\nMuseums\n\n\n127\nAmerican Jewish Historical Society\n10011.0\n15 W. 16th St.\nPOINT (-73.99393 40.73802)\nMuseums\n\n\n128\nYeshiva University Museum\n10011.0\n15 W. 16th St.\nPOINT (-73.99382 40.73805)\nMuseums\n\n\n129\nCenter For Jewish History\n10011.0\n15 W. 16th St.\nPOINT (-73.99387 40.73799)\nMuseums\n\n\n\n\n130 rows × 5 columns\n\n\n\n\n\nCode\n# Libraries\n\n# establish geodataframe\nlibrary = gpd.read_file(\"data/libraries.geojson\")\ngeo_library = gpd.GeoDataFrame(library, geometry='geometry')\ngeo_library= geo_library.set_crs(epsg=4326)\ngeo_library\n\n# add type\nlibrary_clean = geo_library[['name','zip','streetname','geometry']]\nlibrary_clean.loc[:,\"Type\"]= \"Libraries\"\nlibrary_clean.rename(\n    columns={\"streetname\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\nlibrary_clean\n\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/3524914390.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  library_clean.rename(\n\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\n\n\n\n\n0\n115th Street\n10026\nWest 115th Street\nPOINT (-73.95353 40.80298)\nLibraries\n\n\n1\n125th Street\n10035\nEast 125th Street\nPOINT (-73.93485 40.80302)\nLibraries\n\n\n2\n53rd Street\n10019\nWest 53rd Street\nPOINT (-73.97736 40.76081)\nLibraries\n\n\n3\n58th Street\n10022\nEast 58th Street\nPOINT (-73.96938 40.76219)\nLibraries\n\n\n4\n67th Street\n10065\nEast 67th Street\nPOINT (-73.95955 40.76492)\nLibraries\n\n\n...\n...\n...\n...\n...\n...\n\n\n211\nSunnyside\n11104\nGreenpoint Avenue\nPOINT (-73.92167 40.74085)\nLibraries\n\n\n212\nWhitestone\n11357\n14 Road\nPOINT (-73.81070 40.78854)\nLibraries\n\n\n213\nWindsor Park\n11364\nBell Boulevard\nPOINT (-73.75562 40.73450)\nLibraries\n\n\n214\nWoodhaven\n11421\nForest Parkway\nPOINT (-73.86146 40.69453)\nLibraries\n\n\n215\nWoodside\n11377\nSkillman Avenue\nPOINT (-73.90979 40.74534)\nLibraries\n\n\n\n\n216 rows × 5 columns\n\n\n\n\n\nCode\n# Theaters\n# establish geodataframe\ntheaters = gpd.read_file(\"data/Theaters.geojson\")\ngeo_theaters = gpd.GeoDataFrame(theaters, geometry='geometry')\ngeo_theaters= geo_theaters.set_crs(epsg=4326)\ngeo_theaters\n\n\n# add type\ntheaters_clean = geo_theaters[['name','zip','address1','geometry']]\ntheaters_clean.loc[:,\"Type\"]= \"Theatres\"\ntheaters_clean.rename(\n    columns={\"address1\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\ntheaters_clean\n\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1518346627.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  theaters_clean.rename(\n\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\n\n\n\n\n0\n45th Street Theater\n10036.0\n354 West 45th Street\nPOINT (-73.99062 40.75985)\nTheatres\n\n\n1\n47th Street Theater\n10036.0\n304 West 47th Street\nPOINT (-73.98811 40.76047)\nTheatres\n\n\n2\n59E59\n10022.0\n59 East 59th Street\nPOINT (-73.97038 40.76340)\nTheatres\n\n\n3\nAcorn Theater\n10036.0\n410 West 42nd Street\nPOINT (-73.99332 40.75854)\nTheatres\n\n\n4\nAl Hirschfeld Theater\n10036.0\n302 W 45th Street\nPOINT (-73.98921 40.75926)\nTheatres\n\n\n...\n...\n...\n...\n...\n...\n\n\n112\nWestside Theater\n10036.0\n407 W 43rd St\nPOINT (-73.99255 40.75953)\nTheatres\n\n\n113\nWings Theatre\n10014.0\n154 Christopher St\nPOINT (-74.00889 40.73240)\nTheatres\n\n\n114\nWinter Garden Theatre\n10019.0\n1634 Broadway\nPOINT (-73.98348 40.76152)\nTheatres\n\n\n115\nYork Theatre\n10022.0\n619 Lexington Ave\nPOINT (-73.96998 40.75836)\nTheatres\n\n\n116\nDelacorte Theater\n0.0\nCentral Park - Mid-Park at 80th Street\nPOINT (-73.96882 40.78018)\nTheatres\n\n\n\n\n117 rows × 5 columns\n\n\n\n\nAggregate all types of indoor leisure space\n\n\nCode\nframes = [art_galleries_clean, museums_clean, library_clean, theaters_clean]\n\ntotal_indoor = pd.concat(frames)\n\ntotal_indoor\n\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\n\n\n\n\n0\nO'reilly William & Co Ltd\n10021.0\n52 E 76th St\nPOINT (-73.96273 40.77380)\nArt Galleries\n\n\n1\nOrganization of Independent Artists - Gallery 402\n10013.0\n19 Hudson St.\nPOINT (-74.00939 40.71647)\nArt Galleries\n\n\n2\nOwen Gallery\n10021.0\n19 E 75th St\nPOINT (-73.96435 40.77400)\nArt Galleries\n\n\n3\nP P O W Gallerie\n10001.0\n511 W 25th St\nPOINT (-74.00389 40.74959)\nArt Galleries\n\n\n4\nP P O W Inc\n10013.0\n476 Broome St\nPOINT (-74.00176 40.72291)\nArt Galleries\n\n\n...\n...\n...\n...\n...\n...\n\n\n112\nWestside Theater\n10036.0\n407 W 43rd St\nPOINT (-73.99255 40.75953)\nTheatres\n\n\n113\nWings Theatre\n10014.0\n154 Christopher St\nPOINT (-74.00889 40.73240)\nTheatres\n\n\n114\nWinter Garden Theatre\n10019.0\n1634 Broadway\nPOINT (-73.98348 40.76152)\nTheatres\n\n\n115\nYork Theatre\n10022.0\n619 Lexington Ave\nPOINT (-73.96998 40.75836)\nTheatres\n\n\n116\nDelacorte Theater\n0.0\nCentral Park - Mid-Park at 80th Street\nPOINT (-73.96882 40.78018)\nTheatres\n\n\n\n\n1380 rows × 5 columns\n\n\n\n\n\nSpatial Join\n\n\nCode\ngeo_total_indoor = gpd.sjoin(\n    total_indoor,  # The point data for 311 tickets\n    tracts_clean.to_crs(total_indoor.crs),  # The neighborhoods (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\ngeo_total_indoor.head()\n\n\n\n\n\n\n\n\n\nName\nZip\nAddress\ngeometry\nType\nindex_right\nBoroName\nCT2020\nNTAName\nGEOID\n\n\n\n\n0\nO'reilly William & Co Ltd\n10021.0\n52 E 76th St\nPOINT (-73.96273 40.77380)\nArt Galleries\n84.0\nManhattan\n13000.0\nUpper East Side-Carnegie Hill\n3.606101e+10\n\n\n1\nOrganization of Independent Artists - Gallery 402\n10013.0\n19 Hudson St.\nPOINT (-74.00939 40.71647)\nArt Galleries\n17.0\nManhattan\n3900.0\nTribeca-Civic Center\n3.606100e+10\n\n\n2\nOwen Gallery\n10021.0\n19 E 75th St\nPOINT (-73.96435 40.77400)\nArt Galleries\n84.0\nManhattan\n13000.0\nUpper East Side-Carnegie Hill\n3.606101e+10\n\n\n3\nP P O W Gallerie\n10001.0\n511 W 25th St\nPOINT (-74.00389 40.74959)\nArt Galleries\n1134.0\nManhattan\n9901.0\nChelsea-Hudson Yards\n3.606101e+10\n\n\n4\nP P O W Inc\n10013.0\n476 Broome St\nPOINT (-74.00176 40.72291)\nArt Galleries\n1156.0\nManhattan\n4900.0\nSoHo-Little Italy-Hudson Square\n3.606100e+10\n\n\n\n\n\n\n\n\n\n\nOutdoor Leisure Space: Parks\n\n\nCode\n# Parks\n# establish geodataframe\nparks = gpd.read_file(\"data/Parks Properties.geojson\")\ngeo_parks = gpd.GeoDataFrame(parks, geometry='geometry')\ngeo_parks= geo_parks.set_crs(epsg=4326)\ngeo_parks\n\n\n# add type\nparks_clean = geo_parks[['signname','geometry','acres']]\nparks_clean.loc[:,\"Type\"]= \"Parks\"\nparks_clean.rename(\n    columns={\"location\": \"Address\", \"signname\": \"Name\", \"zipcode\": \"Zip\"},\n    inplace=True,)\n\nparks_clean['new_geom'] = parks_clean['geometry']\n\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1177565529.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  parks_clean.rename(\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n\n\n\n\nCode\n\n# Since many parks are in multi-polygons, which will lead to errors when doing spatial join later, a centroid of each park is generated here for smoother spatial join.\nparks_clean['geometry'] = parks_clean['geometry'].centroid\n\ngeo_parks_clean = gpd.sjoin(\n    parks_clean,  # The point data for 311 tickets\n    tracts_clean.to_crs(parks_clean.crs),  # The neighborhoods (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\ngeo_parks_clean\n\n\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/3082874007.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  parks_clean['geometry'] = parks_clean['geometry'].centroid\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n\n\n\n\n\n\n\n\n\nName\ngeometry\nacres\nType\nnew_geom\nindex_right\nBoroName\nCT2020\nNTAName\nGEOID\n\n\n\n\n0\nInwood Hill Park\nPOINT (-73.92544 40.87257)\n196.398\nParks\nMULTIPOLYGON (((-73.92093 40.86999, -73.92145 ...\n2191.0\nManhattan\n29700.0\nInwood Hill Park\n3.606103e+10\n\n\n1\nChallenge Playground\nPOINT (-73.72796 40.75662)\n2.035\nParks\nMULTIPOLYGON (((-73.72738 40.75605, -73.72783 ...\n936.0\nQueens\n152902.0\nDouglaston-Little Neck\n3.608115e+10\n\n\n2\nSunset Cove Park\nPOINT (-73.82300 40.59853)\n9.375\nParks\nMULTIPOLYGON (((-73.82218 40.59892, -73.82221 ...\n1129.0\nQueens\n107201.0\nBreezy Point-Belle Harbor-Rockaway Park-Broad ...\n3.608111e+10\n\n\n3\nGrand Central Parkway Extension\nPOINT (-73.85317 40.75316)\n249.389\nParks\nMULTIPOLYGON (((-73.85875 40.76741, -73.85976 ...\n625.0\nQueens\n39902.0\nNorth Corona\n3.608104e+10\n\n\n4\nIdlewild Park\nPOINT (-73.75229 40.65043)\n180.85\nParks\nMULTIPOLYGON (((-73.75809 40.65427, -73.75845 ...\n1013.0\nQueens\n66404.0\nSpringfield Gardens (South)-Brookville\n3.608107e+10\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2040\nMaria Hernandez Park\nPOINT (-73.92386 40.70317)\n6.873\nParks\nMULTIPOLYGON (((-73.92251 40.70351, -73.92381 ...\n1729.0\nBrooklyn\n42900.0\nBushwick (West)\n3.604704e+10\n\n\n2041\nCrotona Parkway Malls\nPOINT (-73.88477 40.84405)\n8.75\nParks\nMULTIPOLYGON (((-73.88496 40.84470, -73.88496 ...\n336.0\nBronx\n36300.0\nWest Farms\n3.600504e+10\n\n\n2042\nPark\nPOINT (-73.89807 40.84408)\n0.511\nParks\nMULTIPOLYGON (((-73.89759 40.84410, -73.89773 ...\n1229.0\nBronx\n16500.0\nClaremont Village-Claremont (East)\n3.600502e+10\n\n\n2043\nCunningham Park\nPOINT (-73.76880 40.73382)\n358.0\nParks\nMULTIPOLYGON (((-73.77466 40.72442, -73.77439 ...\n2241.0\nQueens\n128300.0\nCunningham Park\n3.608113e+10\n\n\n2044\nRoberto Clemente Ballfield\nPOINT (-73.96767 40.70635)\n1.93\nParks\nMULTIPOLYGON (((-73.96761 40.70581, -73.96735 ...\n2220.0\nBrooklyn\n54500.0\nSouth Williamsburg\n3.604705e+10\n\n\n\n\n2045 rows × 10 columns\n\n\n\n\n\nOutdoor Leisure Space: Open Streets\n\n\nCode\nopen_streets = gpd.read_file(\"data/Open Streets Locations.geojson\")\ngeo_open_streets = gpd.GeoDataFrame(open_streets, geometry='geometry', crs=2263)\ngeo_open_streets = open_streets.to_crs(epsg=4326)\n\nopen_streets_clean = geo_open_streets[['appronstre','apprtostre','apprdayswe','boroughname','reviewstat','shape_stle','geometry']]\n\n\n\n\nCode\njoin_test = gpd.sjoin(\n    open_streets_clean,  # The point data for 311 tickets\n    tracts_clean.to_crs(open_streets_clean.crs),  # The neighborhoods (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\njoin_test.head()\n\n\n\n\n\n\n\n\n\nappronstre\napprtostre\napprdayswe\nboroughname\nreviewstat\nshape_stle\ngeometry\nindex_right\nBoroName\nCT2020\nNTAName\nGEOID\n\n\n\n\n0\nDEISIUS STREET\nSTECHER STREET\nmon,tue,wed,thu,fri\nStaten Island\napprovedFullSchools\n264.932398036\nMULTILINESTRING ((-74.18738 40.53028, -74.1882...\n1318.0\nStaten Island\n17600.0\nAnnadale-Huguenot-Prince's Bay-Woodrow\n3.608502e+10\n\n\n1\nSUFFOLK AVENUE\nHAROLD STREET\nfri,sat,sun\nStaten Island\napprovedFull\n313.087821487\nMULTILINESTRING ((-74.12784 40.60288, -74.1277...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n\n\n2\nSUFFOLK AVENUE\nHAROLD STREET\nfri,sat,sun\nStaten Island\napprovedFull\n142.063500219\nMULTILINESTRING ((-74.12772 40.60202, -74.1276...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n\n\n3\nVERMONT COURT\nSUFFOLK AVENUE\nfri,sat,sun\nStaten Island\napprovedFull\n421.392020366\nMULTILINESTRING ((-74.12620 40.60209, -74.1277...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n\n\n4\n9 STREET\nROSE AVENUE\nfri,sat,sun\nStaten Island\napprovedFull\n448.103939286\nMULTILINESTRING ((-74.11481 40.57316, -74.1157...\n1300.0\nStaten Island\n13400.0\nNew Dorp-Midland Beach\n3.608501e+10\n\n\n\n\n\n\n\n\n\nCode\njoin_test['monday'] = join_test['apprdayswe'].str.count('mon')\njoin_test['tuesday'] = join_test['apprdayswe'].str.count('tue')\njoin_test['wednesday'] = join_test['apprdayswe'].str.count('wed')\njoin_test['thursday'] = join_test['apprdayswe'].str.count('thu')\njoin_test['friday'] = join_test['apprdayswe'].str.count('fri')\njoin_test['saturday'] = join_test['apprdayswe'].str.count('sat')\njoin_test['sunday'] = join_test['apprdayswe'].str.count('sun')\njoin_test['dayscount'] = join_test[['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']].sum(axis=1)\n\njoin_test\n\n\n\n\n\n\n\n\n\nappronstre\napprtostre\napprdayswe\nboroughname\nreviewstat\nshape_stle\ngeometry\nindex_right\nBoroName\nCT2020\nNTAName\nGEOID\nmonday\ntuesday\nwednesday\nthursday\nfriday\nsaturday\nsunday\ndayscount\n\n\n\n\n0\nDEISIUS STREET\nSTECHER STREET\nmon,tue,wed,thu,fri\nStaten Island\napprovedFullSchools\n264.932398036\nMULTILINESTRING ((-74.18738 40.53028, -74.1882...\n1318.0\nStaten Island\n17600.0\nAnnadale-Huguenot-Prince's Bay-Woodrow\n3.608502e+10\n1\n1\n1\n1\n1\n0\n0\n5\n\n\n1\nSUFFOLK AVENUE\nHAROLD STREET\nfri,sat,sun\nStaten Island\napprovedFull\n313.087821487\nMULTILINESTRING ((-74.12784 40.60288, -74.1277...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n0\n0\n0\n0\n1\n1\n1\n3\n\n\n2\nSUFFOLK AVENUE\nHAROLD STREET\nfri,sat,sun\nStaten Island\napprovedFull\n142.063500219\nMULTILINESTRING ((-74.12772 40.60202, -74.1276...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n0\n0\n0\n0\n1\n1\n1\n3\n\n\n3\nVERMONT COURT\nSUFFOLK AVENUE\nfri,sat,sun\nStaten Island\napprovedFull\n421.392020366\nMULTILINESTRING ((-74.12620 40.60209, -74.1277...\n1323.0\nStaten Island\n18703.0\nTodt Hill-Emerson Hill-Lighthouse Hill-Manor H...\n3.608502e+10\n0\n0\n0\n0\n1\n1\n1\n3\n\n\n4\n9 STREET\nROSE AVENUE\nfri,sat,sun\nStaten Island\napprovedFull\n448.103939286\nMULTILINESTRING ((-74.11481 40.57316, -74.1157...\n1300.0\nStaten Island\n13400.0\nNew Dorp-Midland Beach\n3.608501e+10\n0\n0\n0\n0\n1\n1\n1\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\nBECK STREET\nAVENUE ST JOHN\nwed\nBronx\napprovedFull\n619.314479576\nMULTILINESTRING ((-73.90222 40.81449, -73.9014...\n200.0\nBronx\n8300.0\nLongwood\n3.600501e+10\n0\n0\n1\n0\n0\n0\n0\n1\n\n\n362\nNEWKIRK AVENUE\nEAST 17 STREET\nsun\nBrooklyn\napprovedFull\n284.786787549\nMULTILINESTRING ((-73.96422 40.63510, -73.9641...\n1807.0\nBrooklyn\n52000.0\nFlatbush (West)-Ditmas Park-Parkville\n3.604705e+10\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n363\n18 STREET\n4 AVENUE\nmon,tue,wed,thu,fri\nBrooklyn\napprovedFullSchools\n768.660222317\nMULTILINESTRING ((-73.99201 40.66323, -73.9916...\n2248.0\nBrooklyn\n14300.0\nSunset Park (West)\n3.604701e+10\n1\n1\n1\n1\n1\n0\n0\n5\n\n\n364\n34 AVENUE\nJUNCTION BOULEVARD\nmon,tue,wed,thu,fri,sat,sun\nQueens\napprovedLimited\n272.995599601\nMULTILINESTRING ((-73.89716 40.75246, -73.8970...\n2168.0\nQueens\n29100.0\nJackson Heights\n3.608103e+10\n1\n1\n1\n1\n1\n1\n1\n7\n\n\n365\nDECATUR STREET\nSARATOGA AVENUE\nsat\nBrooklyn\napprovedLimited\n769.964464111\nMULTILINESTRING ((-73.92001 40.68299, -73.9176...\n1682.0\nBrooklyn\n37700.0\nBedford-Stuyvesant (East)\n3.604704e+10\n0\n0\n0\n0\n0\n1\n0\n1\n\n\n\n\n366 rows × 20 columns\n\n\n\n\n\nCode\nopen_streets_days = join_test[['appronstre', 'BoroName', 'monday', 'tuesday', 'wednesday','thursday', 'friday', 'saturday', 'sunday']]\nopen_streets_days\n\n\n\n\n\n\n\n\n\nappronstre\nBoroName\nmonday\ntuesday\nwednesday\nthursday\nfriday\nsaturday\nsunday\n\n\n\n\n0\nDEISIUS STREET\nStaten Island\n1\n1\n1\n1\n1\n0\n0\n\n\n1\nSUFFOLK AVENUE\nStaten Island\n0\n0\n0\n0\n1\n1\n1\n\n\n2\nSUFFOLK AVENUE\nStaten Island\n0\n0\n0\n0\n1\n1\n1\n\n\n3\nVERMONT COURT\nStaten Island\n0\n0\n0\n0\n1\n1\n1\n\n\n4\n9 STREET\nStaten Island\n0\n0\n0\n0\n1\n1\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\nBECK STREET\nBronx\n0\n0\n1\n0\n0\n0\n0\n\n\n362\nNEWKIRK AVENUE\nBrooklyn\n0\n0\n0\n0\n0\n0\n1\n\n\n363\n18 STREET\nBrooklyn\n1\n1\n1\n1\n1\n0\n0\n\n\n364\n34 AVENUE\nQueens\n1\n1\n1\n1\n1\n1\n1\n\n\n365\nDECATUR STREET\nBrooklyn\n0\n0\n0\n0\n0\n1\n0\n\n\n\n\n366 rows × 9 columns\n\n\n\n\n\nCensus Data\n\n\nCode\nPop_2020 = gpd.read_file(\"data/NYC_tracts_2020.geojson\")\nPop_2020 = Pop_2020[['GEOID', 'estimate']]\nPop_2020['GEOID']=Pop_2020['GEOID'].astype(int)\n\ntracts_pop = tracts_clean.merge(Pop_2020, on='GEOID', how='left') \nneighbor_pop = tracts_pop.groupby(['NTAName']).sum(['estimate']).reset_index()\n\n\n\n\nCode\nincome = gpd.read_file(\"data/NYC_Income.geojson\")\ngeo_income = gpd.GeoDataFrame(income, geometry='geometry')\ngeo_income = geo_income.set_crs(epsg=4326)\ngeo_income['GEOID']=geo_income['GEOID'].astype('int')\n\ntracts_income = tracts_clean.merge(geo_income.drop(columns='geometry'), on='GEOID', how='left').dropna()\nneighbor_income = tracts_income.groupby(['NTAName', 'BoroName']).median(['estimate']).reset_index()"
  },
  {
    "objectID": "analysis/assignment-2.html#chart-i-matplotlib-parks-in-neighborhoods",
    "href": "analysis/assignment-2.html#chart-i-matplotlib-parks-in-neighborhoods",
    "title": "2 - Where to spend leisure time in NYC?",
    "section": "Chart I: Matplotlib – Parks in Neighborhoods",
    "text": "Chart I: Matplotlib – Parks in Neighborhoods\nI first hope to investigate into the distribution of Parks in different neighborhoods and boroughs in relation to population. On the one hand, higher population means more people will have need for a bigger public green space for leisure time. On the other hand, less populated neighborhoods tend to have more spaces for parks. And since the count of parks doesn’t perfectly reflect how much space is available, I am using acrage data instead of counts for this analysis.\nTo investigate, I utilized Matplotlib, which is great for making simple scatterplot charts that speaks for simple linear relationship, if there is any.\n\n\nCode\ngeo_parks_clean['acres'] = geo_parks_clean['acres'].astype(float)\nparks_acres_neighborhood = geo_parks_clean.groupby('NTAName').sum().drop(columns=['index_right','CT2020']).reset_index()\nparks_acres_pop = neighbor_pop.merge(parks_acres_neighborhood, on='NTAName', how='left').dropna()\nparks_pop = parks_acres_pop.merge(Boro_NTA, on='NTAName', how='left').dropna()\nparks_pop['acres'].describe()\n\n\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1268464147.py:2: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  parks_acres_neighborhood = geo_parks_clean.groupby('NTAName').sum().drop(columns=['index_right','CT2020']).reset_index()\n\n\ncount     237.000000\nmean      105.945812\nstd       224.643587\nmin         0.005000\n25%         7.651000\n50%        20.653000\n75%        85.527000\nmax      1930.636136\nName: acres, dtype: float64\n\n\nAfter a quick glance of the park data, I notice some extreme outliers with extremely large parks that not only serves adjacent neighborhoods but the whole city. I excluded those outliers to have a better sense of how much acrage of parks common neighborhoods get.\n\n\nCode\nparks_pop_filtered = parks_pop.loc[(parks_pop['acres'] &lt; 86) & (parks_pop['acres'] &gt; 7) & (parks_pop['estimate'] &gt;0)]\nparks_pop_filtered\n\n\n\n\n\n\n\n\n\nNTAName\nCT2020\nGEOID_x\nestimate\nacres\nGEOID_y\nBoroName\n\n\n\n\n5\nAstoria (East)-Woodside (North)\n241600\n505134241600\n34825.0\n8.642\n2.886482e+11\nQueens\n\n\n6\nAstoria (North)-Ditmars-Steinway\n214102\n613377214102\n47134.0\n11.530\n4.690532e+11\nQueens\n\n\n10\nBarren Island-Floyd Bennett Field\n70202\n36047070202\n26.0\n64.665\n3.604707e+10\nBrooklyn\n\n\n11\nBath Beach\n235800\n396517235800\n32716.0\n21.398\n1.081411e+11\nBrooklyn\n\n\n15\nBedford Park\n448115\n396055448115\n55702.0\n23.917\n1.800252e+11\nBronx\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n225\nWest Farms\n136300\n180025136300\n18206.0\n13.540\n3.960554e+11\nBronx\n\n\n230\nWhitestone-Beechhurst\n703300\n252567703300\n28353.0\n29.086\n2.164866e+11\nQueens\n\n\n232\nWilliamsburg\n695100\n468611695100\n59410.0\n81.282\n1.045365e+12\nBrooklyn\n\n\n233\nWindsor Terrace-South Slope\n363808\n288376363808\n25442.0\n8.408\n4.325650e+11\nBrooklyn\n\n\n235\nWoodside\n406004\n505134406004\n45417.0\n10.217\n5.412154e+11\nQueens\n\n\n\n\n113 rows × 7 columns\n\n\n\n\n\nCode\n\ncolor_map = {\"Bronx\": \"#550527\", \"Brooklyn\": \"#688E26\", \"Manhattan\": \"#FAA613\", \"Queens\": \"#F44708\", \"Staten Island\": \"#A10702\"}\n\nfig, ax = plt.subplots(figsize=(11,6))\n\nfor BoroName, group_df in parks_pop_filtered.groupby(\"BoroName\"):\n    \n    ax.scatter(\n        group_df[\"estimate\"],\n        group_df[\"acres\"],\n        marker=\"P\",\n        label=BoroName,\n        color=color_map[BoroName],\n        alpha=0.75,\n        zorder=10\n    )\n\nax.legend(loc=\"best\")\nax.set(\n    title = \"Park Space (acres) Relative to Population in Neighborhoods, by Boroughs\",\n    xlabel = \"Population in each neighborhood (2020 Census)\",\n    ylabel = \"Parks in Acres\")\nax.grid(True)\n\nplt.show()\n\n\n\n\n\nThis chart does not suggest a strong linear relationship between population and park acrages. Overall, Manhattan, Queens and Brooklyn host more large parks, but some neighborhoods are particularly underserved, with very high population and low park acrage (points towards lower right of the chart)."
  },
  {
    "objectID": "analysis/assignment-2.html#chart-ii-seaborn-x2",
    "href": "analysis/assignment-2.html#chart-ii-seaborn-x2",
    "title": "2 - Where to spend leisure time in NYC?",
    "section": "Chart II: Seaborn (x2)",
    "text": "Chart II: Seaborn (x2)\nLooking at data for indoor leisure space and open street data, I utilize seaborn to create two types of charts that suite the nature of the data sets.\nThe first is a grouped bar charts. Similar to park distribution, I hope to have a quick glimpse of number of each type of leisure spaces in each borough, and hope to identify any general spatial patterns or inequalities. A grouped chart is great a revealing such pattern.\nThe second is a heatmap for open street data. The heatmap explores the number of open streets approved in different boroughs on different days of the week.\n\nIndoor Leisure Spaces: Grouped Bar Charts\n\n\nCode\nindoor_clean = geo_total_indoor.groupby(['Type','BoroName']).count().reset_index().drop(columns=['Zip','Address','geometry','index_right','CT2020','NTAName']).pivot(index='Type',columns='BoroName', values='Name').fillna(0).reset_index()\nindoor_melt = indoor_clean.melt(id_vars='Type', value_vars=['Bronx','Brooklyn','Manhattan', 'Queens', 'Staten Island'])\nindoor_melt\n\n\n\n\n\n\n\n\n\nType\nBoroName\nvalue\n\n\n\n\n0\nArt Galleries\nBronx\n6.0\n\n\n1\nLibraries\nBronx\n35.0\n\n\n2\nMuseums\nBronx\n8.0\n\n\n3\nTheatres\nBronx\n0.0\n\n\n4\nArt Galleries\nBrooklyn\n61.0\n\n\n5\nLibraries\nBrooklyn\n59.0\n\n\n6\nMuseums\nBrooklyn\n12.0\n\n\n7\nTheatres\nBrooklyn\n0.0\n\n\n8\nArt Galleries\nManhattan\n823.0\n\n\n9\nLibraries\nManhattan\n44.0\n\n\n10\nMuseums\nManhattan\n87.0\n\n\n11\nTheatres\nManhattan\n115.0\n\n\n12\nArt Galleries\nQueens\n24.0\n\n\n13\nLibraries\nQueens\n65.0\n\n\n14\nMuseums\nQueens\n12.0\n\n\n15\nTheatres\nQueens\n2.0\n\n\n16\nArt Galleries\nStaten Island\n3.0\n\n\n17\nLibraries\nStaten Island\n13.0\n\n\n18\nMuseums\nStaten Island\n9.0\n\n\n19\nTheatres\nStaten Island\n0.0\n\n\n\n\n\n\n\n\n\nCode\nsns.set_theme(style=\"whitegrid\")\n\ncolor_map = [\"#550527\", \"#688E26\", \"#FAA613\", \"#F44708\", \"#A10702\"]\nsns.set_palette(color_map)\n\nsns.catplot(\n    data=indoor_melt, kind=\"bar\",\n    x=\"Type\", \n    y=\"value\", \n    hue=\"BoroName\",\n    aspect=2, \n    alpha=1\n).set_axis_labels(\n    \"Type of Leisure Space\", \"Counts\"\n).set(title=\"Distribution of 4 Types of Leisure Spaces in Each Borough\")\n\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\nDays and Location of Open Streets in NYC: A Heatmap\n\n\nCode\n#heatmap for open_streets: borough x Days of the week \nopen_streets_days_melt= open_streets_days.melt(id_vars=['appronstre','BoroName'], value_vars=['monday','tuesday','wednesday', 'thursday', 'friday','saturday','sunday'])\n\n\n\n\n\n\n\n\n\nappronstre\nBoroName\nvariable\nvalue\n\n\n\n\n0\nDEISIUS STREET\nStaten Island\nmonday\n1\n\n\n1\nSUFFOLK AVENUE\nStaten Island\nmonday\n0\n\n\n2\nSUFFOLK AVENUE\nStaten Island\nmonday\n0\n\n\n3\nVERMONT COURT\nStaten Island\nmonday\n0\n\n\n4\n9 STREET\nStaten Island\nmonday\n0\n\n\n...\n...\n...\n...\n...\n\n\n2557\nBECK STREET\nBronx\nsunday\n0\n\n\n2558\nNEWKIRK AVENUE\nBrooklyn\nsunday\n1\n\n\n2559\n18 STREET\nBrooklyn\nsunday\n0\n\n\n2560\n34 AVENUE\nQueens\nsunday\n1\n\n\n2561\nDECATUR STREET\nBrooklyn\nsunday\n0\n\n\n\n\n2562 rows × 4 columns\n\n\n\n\n\nCode\nopen_streets_seaborn = open_streets_days_melt.groupby(['variable','BoroName']).sum().reset_index()\n\n# sort the order of day from monday to sunday \n\norder=['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\nopen_streets_seaborn['variable'] = pd.Categorical(open_streets_seaborn['variable'], categories=order, ordered=True)\nopen_streets_seaborn = open_streets_seaborn.sort_values(by='variable')\n\n\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/3045256453.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  open_streets_seaborn = open_streets_days_melt.groupby(['variable','BoroName']).sum().reset_index()\n\n\n\n\nCode\nimport seaborn as sns\n\nsns.set_theme()\n\n# Load the example flights dataset and convert to long-form\n\nopen_street_heatmap = (\n    open_streets_seaborn\n    .pivot(index=\"BoroName\", columns=\"variable\", values=\"value\")\n)\n\n# Draw a heatmap with the numeric values in each cell\nf, ax = plt.subplots(figsize=(9, 6))\n\nax = sns.heatmap(open_street_heatmap, annot=True, linewidths=.5)\nax.set(xlabel=\"Days in a week\", ylabel=\"Borough\", title=\"Number of Streets Open on Specific Days\")\n\n\n\n[Text(0.5, 33.249999999999986, 'Days in a week'),\n Text(79.75, 0.5, 'Borough'),\n Text(0.5, 1.0, 'Number of Streets Open on Specific Days')]\n\n\n\n\n\nFrom both charts, we can see Manhattan has disproportional number of art galleries and open streets in comparison to other boroughs. Museums and theatres are predominantly located on Manhattan as well. Interestingly, the distribution of libraries seems more even. However, overall, we are seeing leisure spaces are disproportional abundant and diverse in Manhattan, then brooklyn and queens, leaving Bronx and Staten Island less resourceful."
  },
  {
    "objectID": "analysis/assignment-2.html#chart-iii-altair-charts-x3",
    "href": "analysis/assignment-2.html#chart-iii-altair-charts-x3",
    "title": "2 - Where to spend leisure time in NYC?",
    "section": "Chart III: Altair Charts (x3)",
    "text": "Chart III: Altair Charts (x3)\nFurthering the exploration on indoor leisure spaces, I created one chart on its ditribution in relation with median household income, to see if it embeds any socio-economic inequality. I then creatd a map to visualize this relationship, which can also be helpful to locate different kinds of leisure spaces in the city.\nMy third chart is an interactive bar chart on open streets in Brookylen, where audience can choose the day of the week to see all open streets approved for that day and the time they open and close. This could potentially be developed into a tool for residents and tourists to track open streets.\n\nBrush Selection: Indoor Space and Income\n\n\nCode\nneighbor_indoor = geo_total_indoor.groupby(['NTAName','Type']).count().reset_index().drop(['Zip', 'Address', 'geometry','index_right','BoroName', 'CT2020'], axis=1)\nneighbor_indoor = neighbor_indoor.rename(columns={'Name': 'Count'})\nneighbor_income_indoor = neighbor_income.merge(neighbor_indoor, on='NTAName', how='left')\n\nneighbor_income_indoor_pivot=neighbor_income_indoor.pivot(index=['NTAName', 'BoroName','estimate'], columns=\"Type\", values=\"Count\").reset_index().fillna(0)\nneighbor_income_indoor_pivot_1 = neighbor_income_indoor_pivot.drop(neighbor_income_indoor_pivot.columns[[3]],axis=1)\nincome_indoor_pivot = neighbor_income_indoor_pivot_1.melt(id_vars=[\"NTAName\", \"estimate\", \"BoroName\"], value_vars=[\"Art Galleries\", \"Libraries\", \"Museums\", \"Theatres\"], var_name=\"Types\",value_name=\"Count\")\nincome_indoor_pivot_filtered = income_indoor_pivot.loc[(income_indoor_pivot['Count'] &lt;20)]\nincome_indoor_pivot_filtered = income_indoor_pivot_filtered.rename(columns={'estimate': 'Median Household Income'})\n\n\n\n\nCode\nbrush = alt.selection_interval()\n\nBrush_Chart = (\nalt.Chart(income_indoor_pivot_filtered)\n   .mark_point()\n   .encode(\n       x=alt.X(\"Median Household Income:Q\", scale=alt.Scale(zero=False)),\n       y=alt.Y(\"Count:Q\", scale=alt.Scale(zero=False)),\n       color=alt.condition(brush, \"BoroName:N\", alt.value(\"lightgray\")),\n       tooltip=[\"NTAName\",\"BoroName:N\", \"Median Household Income:Q\", \"Count:Q\"])\n   .add_params(brush)\n   .properties(width=200, height=200)\n   .facet(column=\"Types:N\")\n)\n\nBrush_Chart\n\n\n\n\n\n\n\n\n\n\nMap: Income and Indoor Leisure Spaces - Relationships\n\n\nCode\nNTA = pd.read_csv(\"data/2020 Neighborhood.csv\")\nNTA['geometry'] = gpd.GeoSeries.from_wkt(NTA['the_geom'])\nNTA_geo = gpd.GeoDataFrame(NTA, geometry='geometry', crs=4326)\nNTA_geo = NTA_geo.to_crs(epsg=2263)\ngeo_total_indoor = geo_total_indoor.to_crs(epsg=2263)\n\ngeo_NTA = NTA_geo[['NTAName', 'geometry']]\n\ntracts_income_1 = tracts_clean.merge(geo_income.drop(columns='geometry'), on='GEOID', how='left').dropna()\nneighbor_income_1 = tracts_income.groupby(['NTAName', 'BoroName']).median(['estimate']).reset_index()\n\nNTA_income = geo_NTA.merge(neighbor_income_1, on='NTAName', how='left')\n\n\n\n\nCode\ngeo_total_indoor_1 = geo_total_indoor\n\ngeo_total_indoor_1['lon'] = geo_total_indoor_1['geometry'].x\ngeo_total_indoor_1['lat'] = geo_total_indoor_1['geometry'].y\n\n\n\n\nCode\nIncome = (\n    alt.Chart(NTA_income)\n    .mark_geoshape(stroke=\"white\")\n    .encode(\n        tooltip=[\"NTAName:N\", \"estimate:Q\", \"moe:Q\"],\n        color=alt.Color(\"estimate:Q\", scale=alt.Scale(scheme=\"greys\")),\n    )\n    # Important! Otherwise altair will try to re-project your data\n    .project(type=\"identity\", reflectY=True)\n    .properties(width=1000, height=800).interactive()\n)\n\nIndoorSpaces = (\n    alt.Chart(geo_total_indoor_1)\n    .mark_circle(size=10)\n    .encode(tooltip=['Name','Type','Address'],\n           longitude=\"lon\", latitude=\"lat\",\n           color=alt.Color('Type:N', scale=alt.Scale(scheme=\"lightmulti\"))\n         ).project(type=\"identity\", reflectY=True)\n)\n\n\n\nmap_1 = Income + IndoorSpaces\nmap_1\n\n\n\n\n\n\n\n\n\nSimilarly, libraries seem to be the least discriminatory type of leisure space. For art galleries and museums, while there are some distributed in mid to lower income neighborhood, the higher income neighborhood has higher density of such leisure space. Theatres, at the same time, is mostly located in mid to higher income neighborhood. This trend can be clearly see on the map too. Such spatial distribution means, for residents in lower to mid income neighborhood, they might have to travel further for accessing those spaces.\n\n\nOpen Street Time\n\n\nCode\nopen_csv = pd.read_csv(\"data/Open Streets CSV.csv\")\nopen_csv['geometry'] = gpd.GeoSeries.from_wkt(open_csv['the Geom'])\nopen_csv_geo = gpd.GeoDataFrame(open_csv, geometry='geometry', crs=2263)\nopen_csv_geo = open_csv_geo.to_crs(epsg=4326)\n\nopen_7days_time = open_csv_geo.drop(['apprDaysWe','Object ID', 'Organization Name', 'Approved From Street', 'Approved To Street', 'apprStartD', 'apprEndDat', 'Shape_STLe', 'segmentidt', 'segmentidf', 'lionversion', 'the Geom'], axis=1)\nopen_7days_time = open_7days_time.drop_duplicates(subset = \"Approved On Street\")\nopen_7days_time_melt = open_7days_time.melt(id_vars=['Approved On Street', 'Borough Name'], value_vars=['Approved Monday Open', 'Approved Monday Close', 'Approved Tuesday Open', 'Approved Tuesday Close', 'Approved Wednesday Open', 'Approved Wednesday Close', 'Approved Thursday Open', 'Approved Thursday Close', 'Approved Friday Open', 'Approved Friday Close', 'Approved Saturday Open', 'Approved Saturday Close', 'Approved Sunday Open', 'Approved Sunday Close']).dropna()\n#open_7days_time_melt['value'] =  pd.to_datetime(open_7days_time_melt['value']).dt.time\nopen_time_Brooklyn = open_7days_time_melt[(open_7days_time_melt['Borough Name'] == 'Brooklyn')]\nopen_time_Brooklyn\n\n\n\n\n\n\n\n\n\nApproved On Street\nBorough Name\nvariable\nvalue\n\n\n\n\n6\nRIDGE BOULEVARD\nBrooklyn\nApproved Monday Open\n10:00\n\n\n9\n82 STREET\nBrooklyn\nApproved Monday Open\n08:30\n\n\n10\n48 STREET\nBrooklyn\nApproved Monday Open\n13:30\n\n\n11\n43 STREET\nBrooklyn\nApproved Monday Open\n09:30\n\n\n12\nALBEMARLE ROAD\nBrooklyn\nApproved Monday Open\n08:00\n\n\n...\n...\n...\n...\n...\n\n\n2282\nJEFFERSON AVENUE\nBrooklyn\nApproved Sunday Close\n21:00\n\n\n2287\nSHARON STREET\nBrooklyn\nApproved Sunday Close\n20:00\n\n\n2288\nTROUTMAN STREET\nBrooklyn\nApproved Sunday Close\n22:00\n\n\n2289\nRANDOLPH STREET\nBrooklyn\nApproved Sunday Close\n23:00\n\n\n2354\nLEXINGTON AVENUE\nBrooklyn\nApproved Sunday Close\n20:00\n\n\n\n\n378 rows × 4 columns\n\n\n\n\n\nCode\nopen_time_Brooklyn['time'] = open_7days_time_melt['variable'].str.extract('(Open|Close)')\nopen_time_Brooklyn['dayweek'] = open_7days_time_melt['variable'].str.extract('(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)')\nopen_time_Brooklyn= open_time_Brooklyn.pivot(index=['Approved On Street', 'dayweek'], columns=\"time\", values=\"value\").reset_index()\n\norder=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nopen_time_Brooklyn['dayweek'] = pd.Categorical(open_time_Brooklyn['dayweek'], categories=order, ordered=True)\nopen_time_Brooklyn = open_time_Brooklyn.sort_values(by='dayweek').reset_index()\n\nopen_time_Brooklyn\n\n\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1900940832.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  open_time_Brooklyn['time'] = open_7days_time_melt['variable'].str.extract('(Open|Close)')\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_80527/1900940832.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  open_time_Brooklyn['dayweek'] = open_7days_time_melt['variable'].str.extract('(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)')\n\n\n\n\n\n\n\n\ntime\nindex\nApproved On Street\ndayweek\nClose\nOpen\n\n\n\n\n0\n139\nSUMMIT STREET\nMonday\n14:30\n12:30\n\n\n1\n151\nUNDERHILL AVENUE\nMonday\n20:00\n08:00\n\n\n2\n28\n82 STREET\nMonday\n15:30\n08:30\n\n\n3\n143\nTHATFORD AVENUE\nMonday\n18:00\n11:00\n\n\n4\n33\nAITKEN PLACE\nMonday\n15:00\n11:00\n\n\n...\n...\n...\n...\n...\n...\n\n\n184\n147\nTOMPKINS AVENUE\nSunday\n20:00\n11:00\n\n\n185\n51\nBEVERLEY ROAD\nSunday\n18:00\n10:00\n\n\n186\n112\nRANDOLPH STREET\nSunday\n23:00\n12:00\n\n\n187\n10\n4 STREET\nSunday\n22:00\n08:00\n\n\n188\n157\nVANDERBILT AVENUE\nSunday\n23:00\n11:00\n\n\n\n\n189 rows × 5 columns\n\n\n\n\n\nCode\nselection = alt.selection_multi(fields=['dayweek'])\ncolor = alt.condition(selection,\n                      alt.Color('dayweek:N', legend=None, \n                      scale=alt.Scale(scheme='category10')),\n                      alt.value('lightgray'))\n\nopacity = alt.condition(selection,\n                        alt.value(1), alt.value(0))\n\n\n\nbar = alt.Chart(open_time_Brooklyn).mark_bar().encode(\n        x='Open',\n        x2='Close',\n        y='Approved On Street',\n        color=color,\n        opacity=opacity,\n        tooltip=['Open', 'Close', 'Approved On Street', 'dayweek']).properties(\n        width=500,\n        height=1000).interactive()\n\nlegend = alt.Chart(open_time_Brooklyn).mark_bar().encode(\n    y=alt.Y('dayweek:N', axis=alt.Axis(orient='right')),\n    color=color\n).add_selection(\nselection\n)\n\nA_Chart = bar | legend\nA_Chart\n\n\n\n\n\n\n\n\n\nThis interactive bar chart on Brooklyn Open Street serves as a pilot that can be adapted for data of all five boroughs. By comparing to open and close time, we can observe that many streets are approved to be open streets with later opening and closing time on weekends. To improve this bar chart the status of each street can be added (i.e., whether they are approved to be closed fully or partially or only on school days), which is important for visitors as well."
  },
  {
    "objectID": "analysis/assignment-2.html#dashboard",
    "href": "analysis/assignment-2.html#dashboard",
    "title": "2 - Where to spend leisure time in NYC?",
    "section": "Dashboard",
    "text": "Dashboard\nThe dashboard below is an upgrade from the indoor space and income grouped scatter plot. This dashboard makes it easier to explore the spatial distribution of indoor spaces of certain type or in neighborhoods with certain level of income. Through selecting points with higher counts, we can see neighborhoods with higher density of identified indoor leisure spaces are mostly located in Manhattan.\n\n\nCode\nbrush = alt.selection_interval()\n\npoints = alt.Chart(income_indoor_pivot_filtered).mark_point().encode(\n       x=alt.X(\"Median Household Income:Q\", scale=alt.Scale(zero=False)),\n       y=alt.Y(\"Count:Q\", scale=alt.Scale(zero=False)),\n       color=alt.condition(brush, \"BoroName:N\", alt.value(\"lightgray\")),\n       tooltip=[\"NTAName\",\"BoroName:N\", \"Median Household Income:Q\", \"Count:Q\"]).add_params(brush\n).properties(width=200, height=200\n).facet(column=\"Types:N\")\n\n\nbars = alt.Chart(income_indoor_pivot_filtered).mark_bar().encode(\n    y='BoroName:N',\n    color='BoroName:N',\n    x='Count:Q'\n).transform_filter(\n    brush\n)\n\nDashboard = points & bars\nDashboard"
  },
  {
    "objectID": "analysis/assignment-4.html",
    "href": "analysis/assignment-4.html",
    "title": "4 - Street Networks & Web Scraping",
    "section": "",
    "text": "Part 1: Visualizing crash data in Philadelphia\nIn this section, you will use osmnx to analyze the crash incidence in Center City.\nPart 2: Scraping Craigslist\nIn this section, you will use Selenium and BeautifulSoup to scrape data for hundreds of apartments from Philadelphia’s Craigslist portal.\nCode\nimport pandas as pd\nimport geopandas as gpd\nimport altair as alt\nimport numpy as np\nfrom matplotlib import pyplot as plt\nnp.seterr(invalid=\"ignore\");\nfrom shapely import geometry"
  },
  {
    "objectID": "analysis/assignment-4.html#part-1-visualizing-crash-data-in-philadelphia",
    "href": "analysis/assignment-4.html#part-1-visualizing-crash-data-in-philadelphia",
    "title": "4 - Street Networks & Web Scraping",
    "section": "Part 1: Visualizing crash data in Philadelphia",
    "text": "Part 1: Visualizing crash data in Philadelphia\n\n1.1 Load the geometry for the region being analyzed\nWe’ll analyze crashes in the “Central” planning district in Philadelphia, a rough approximation for Center City. Planning districts can be loaded from Open Data Philly. Read the data into a GeoDataFrame using the following link:\nhttp://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson\nSelect the “Central” district and extract the geometry polygon for only this district. After this part, you should have a polygon variable of type shapely.geometry.polygon.Polygon.\n\n\nCode\nurl = \"http://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson\"\nphilly_districts = gpd.read_file(url).to_crs('EPSG:4326')\ncentral = philly_districts.loc[philly_districts['DIST_NAME'] == 'Central']\n\npolygon = central['geometry'].iloc[0]\nprint(type(polygon))\n\n\n&lt;class 'shapely.geometry.polygon.Polygon'&gt;\n\n\n\n\n1.2 Get the street network graph\nUse OSMnx to create a network graph (of type ‘drive’) from your polygon boundary in 1.1.\n\n\nCode\nimport osmnx as ox\ngraph = ox.graph_from_polygon(polygon, network_type='drive')\nphilly_graph = ox.project_graph(graph)\nox.plot_graph(philly_graph)\n\n\n\n\n\n(&lt;Figure size 800x800 with 1 Axes&gt;, &lt;Axes: &gt;)\n\n\n\n\n1.3 Convert your network graph edges to a GeoDataFrame\nUse OSMnx to create a GeoDataFrame of the network edges in the graph object from part 1.2. The GeoDataFrame should contain the edges but not the nodes from the network.\n\n\nCode\nedges = ox.graph_to_gdfs(philly_graph, edges=True, nodes=False).to_crs('EPSG:4326')\nedges.head()\n\n\n\n\n\n\n\n\n\n\n\nosmid\noneway\nname\nhighway\nreversed\nlength\ngeometry\nmaxspeed\nlanes\ntunnel\nbridge\nref\nwidth\nservice\naccess\njunction\n\n\nu\nv\nkey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n109727439\n109911666\n0\n132508434\nTrue\nBainbridge Street\nresidential\nFalse\n44.137\nLINESTRING (-75.17104 39.94345, -75.17053 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n109911666\n109911655\n0\n132508434\nTrue\nBainbridge Street\nresidential\nFalse\n45.459\nLINESTRING (-75.17053 39.94339, -75.17001 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n110024009\n0\n12139665\nTrue\nSouth 17th Street\ntertiary\nFalse\n109.692\nLINESTRING (-75.17053 39.94339, -75.17060 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n109727448\n109727439\n0\n12109011\nTrue\nSouth Colorado Street\nresidential\nFalse\n109.484\nLINESTRING (-75.17125 39.94248, -75.17120 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n110034229\n0\n12159387\nTrue\nFitzwater Street\nresidential\nFalse\n91.353\nLINESTRING (-75.17125 39.94248, -75.17137 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nCode\nedges.explore()\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n1.4 Load PennDOT crash data\nData for crashes (of all types) for 2020, 2021, and 2022 in Philadelphia County is available at the following path:\n./data/CRASH_PHILADELPHIA_XXXX.csv\nYou should see three separate files in the data/ folder. Use pandas to read each of the CSV files, and combine them into a single dataframe using pd.concat().\nThe data was downloaded for Philadelphia County from here.\n\n\nCode\ncrash_2020 = pd.read_csv(\"./data/CRASH_PHILADELPHIA_2020.csv\")\ncrash_2021 = pd.read_csv(\"./data/CRASH_PHILADELPHIA_2021.csv\")\ncrash_2022 = pd.read_csv(\"./data/CRASH_PHILADELPHIA_2022.csv\")\n\n\n\n\nCode\nphilly_crash = pd.concat([crash_2020, crash_2021, crash_2020])\nphilly_crash\n\n\n\n\n\n\n\n\n\nCRN\nARRIVAL_TM\nAUTOMOBILE_COUNT\nBELTED_DEATH_COUNT\nBELTED_SUSP_SERIOUS_INJ_COUNT\nBICYCLE_COUNT\nBICYCLE_DEATH_COUNT\nBICYCLE_SUSP_SERIOUS_INJ_COUNT\nBUS_COUNT\nCHLDPAS_DEATH_COUNT\n...\nWORK_ZONE_TYPE\nWORKERS_PRES\nWZ_CLOSE_DETOUR\nWZ_FLAGGER\nWZ_LAW_OFFCR_IND\nWZ_LN_CLOSURE\nWZ_MOVING\nWZ_OTHER\nWZ_SHLDER_MDN\nWZ_WORKERS_INJ_KILLED\n\n\n\n\n0\n2020036588\n1349.0\n1\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n2020036617\n1842.0\n1\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n2020035717\n2000.0\n1\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n2020034378\n1139.0\n2\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n2020025511\n345.0\n1\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10160\n2020107585\n2159.0\n1\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n10161\n2020109081\n1945.0\n2\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n10162\n2020108478\n537.0\n1\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n10163\n2020108480\n2036.0\n2\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n10164\n2020110361\n1454.0\n0\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n30877 rows × 100 columns\n\n\n\n\n\n1.5 Convert the crash data to a GeoDataFrame\nYou will need to use the DEC_LAT and DEC_LONG columns for latitude and longitude.\nThe full data dictionary for the data is available here\n\n\nCode\ncrash_geo = gpd.GeoDataFrame(philly_crash, geometry=gpd.points_from_xy(philly_crash.DEC_LONG, philly_crash.DEC_LAT), crs=\"EPSG:4326\")\n\n\n\n\n1.6 Trim the crash data to Center City\n\nGet the boundary of the edges data frame (from part 1.3). Accessing the .geometry.unary_union.convex_hull property will give you a nice outer boundary region.\nTrim the crashes using the within() function of the crash GeoDataFrame to find which crashes are within the boundary.\n\nThere should be about 3,750 crashes within the Central district.\n\n\nCode\nboundary = edges.geometry.unary_union.convex_hull\n\ncentral_crash = crash_geo[crash_geo.within(boundary)]\ncentral_crash\n\n\n\n\n\n\n\n\n\nCRN\nARRIVAL_TM\nAUTOMOBILE_COUNT\nBELTED_DEATH_COUNT\nBELTED_SUSP_SERIOUS_INJ_COUNT\nBICYCLE_COUNT\nBICYCLE_DEATH_COUNT\nBICYCLE_SUSP_SERIOUS_INJ_COUNT\nBUS_COUNT\nCHLDPAS_DEATH_COUNT\n...\nWORKERS_PRES\nWZ_CLOSE_DETOUR\nWZ_FLAGGER\nWZ_LAW_OFFCR_IND\nWZ_LN_CLOSURE\nWZ_MOVING\nWZ_OTHER\nWZ_SHLDER_MDN\nWZ_WORKERS_INJ_KILLED\ngeometry\n\n\n\n\n0\n2020036588\n1349.0\n1\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.17940 39.96010)\n\n\n7\n2020035021\n1255.0\n1\n0\n0\n0\n0\n0\n1\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.16310 39.97000)\n\n\n11\n2020021944\n805.0\n2\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.18780 39.95230)\n\n\n12\n2020024963\n1024.0\n1\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.14840 39.95580)\n\n\n18\n2020000481\n1737.0\n1\n0\n0\n0\n0\n0\n1\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.15470 39.95340)\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10132\n2020105944\n1841.0\n0\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.14090 39.95380)\n\n\n10133\n2020112963\n2052.0\n2\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.17520 39.95310)\n\n\n10148\n2020111219\n1324.0\n0\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.16110 39.96370)\n\n\n10150\n2020110378\n2025.0\n6\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.17770 39.94430)\n\n\n10161\n2020109081\n1945.0\n2\n0\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOINT (-75.18580 39.94150)\n\n\n\n\n3926 rows × 101 columns\n\n\n\n\n\n1.7 Re-project our data into an approriate CRS\nWe’ll need to find the nearest edge (street) in our graph for each crash. To do this, osmnx will calculate the distance from each crash to the graph edges. For this calculation to be accurate, we need to convert from latitude/longitude\nWe’ll convert the local state plane CRS for Philadelphia, EPSG=2272\n\nTwo steps:\n\nProject the graph object (G) using the ox.project_graph. Run ox.project_graph? to see the documentation for how to convert to a specific CRS.\nProject the crash data using the .to_crs() function.\n\n\n\nCode\nphilly_G = ox.project_graph(philly_graph, to_crs='EPSG:2272')\n\n\n\n\nCode\ncentral_crash = central_crash.to_crs('EPSG:2272')\n\n\n\n\n\n1.8 Find the nearest edge for each crash\nSee: ox.distance.nearest_edges(). It takes three arguments:\n\nthe network graph\nthe longitude of your crash data (the x attribute of the geometry column)\nthe latitude of your crash data (the y attribute of the geometry column)\n\nYou will get a numpy array with 3 columns that represent (u, v, key) where each u and v are the node IDs that the edge links together. We will ignore the key value for our analysis.\n\n\nCode\nnearest_crash = ox.distance.nearest_edges(philly_G, central_crash['geometry'].x, central_crash['geometry'].y)\n\n\n\n\n1.9 Calculate the total number of crashes per street\n\nMake a DataFrame from your data from part 1.7 with three columns, u, v, and key (we will only use the u and v columns)\nGroup by u and v and calculate the size\nReset the index and name your size() column as crash_count\n\nAfter this step you should have a DataFrame with three columns: u, v, and crash_count.\n\n\nCode\nnearest_Crash = pd.DataFrame(nearest_crash, columns=['u', 'v', 'key'])\n\n\n\n\nCode\ncrash_counts = nearest_Crash.groupby(['u', 'v']).count().reset_index()\ncrash_counts = crash_counts.rename(columns={'key': 'crash_count'})\ncrash_counts\n\n\n\n\n\n\n\n\n\nu\nv\ncrash_count\n\n\n\n\n0\n109729474\n3425014859\n2\n\n\n1\n109729486\n110342146\n4\n\n\n2\n109729699\n109811674\n10\n\n\n3\n109729709\n109729731\n3\n\n\n4\n109729731\n109729739\n6\n\n\n...\n...\n...\n...\n\n\n729\n10270051289\n5519334546\n1\n\n\n730\n10660521823\n10660521817\n2\n\n\n731\n10674041689\n10674041689\n14\n\n\n732\n11144117753\n109729699\n4\n\n\n733\n11162290432\n110329835\n1\n\n\n\n\n734 rows × 3 columns\n\n\n\n\n\n1.10 Merge your edges GeoDataFrame and crash count DataFrame\nYou can use pandas to merge them on the u and v columns. This will associate the total crash count with each edge in the street network.\nTips: - Use a left merge where the first argument of the merge is the edges GeoDataFrame. This ensures no edges are removed during the merge. - Use the fillna(0) function to fill in missing crash count values with zero.\n\n\nCode\nmerged = pd.merge(edges, crash_counts, on=['u','v'], how='left').fillna(0)\nmerged.head()\n\n\n\n\n\n\n\n\n\nu\nv\nosmid\noneway\nname\nhighway\nreversed\nlength\ngeometry\nmaxspeed\nlanes\ntunnel\nbridge\nref\nwidth\nservice\naccess\njunction\ncrash_count\n\n\n\n\n0\n109727439\n109911666\n132508434\nTrue\nBainbridge Street\nresidential\nFalse\n44.137\nLINESTRING (-75.17104 39.94345, -75.17053 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n1\n109911666\n109911655\n132508434\nTrue\nBainbridge Street\nresidential\nFalse\n45.459\nLINESTRING (-75.17053 39.94339, -75.17001 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n2\n109911666\n110024009\n12139665\nTrue\nSouth 17th Street\ntertiary\nFalse\n109.692\nLINESTRING (-75.17053 39.94339, -75.17060 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n3\n109727448\n109727439\n12109011\nTrue\nSouth Colorado Street\nresidential\nFalse\n109.484\nLINESTRING (-75.17125 39.94248, -75.17120 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n4\n109727448\n110034229\n12159387\nTrue\nFitzwater Street\nresidential\nFalse\n91.353\nLINESTRING (-75.17125 39.94248, -75.17137 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n\n\n\n\n\n\n\n1.11 Calculate a “Crash Index”\nLet’s calculate a “crash index” that provides a normalized measure of the crash frequency per street. To do this, we’ll need to:\n\nCalculate the total crash count divided by the street length, using the length column\nPerform a log transformation of the crash/length variable — use numpy’s log10() function\nNormalize the index from 0 to 1 (see the lecture notes for an example of this transformation)\n\nNote: since the crash index involves a log transformation, you should only calculate the index for streets where the crash count is greater than zero.\nAfter this step, you should have a new column in the data frame from 1.9 that includes a column called part 1.9.\n\n\nCode\nmerge_dropna = merged[merged['crash_count'] != 0]\n\n\n\n\nCode\ncalculation = np.log10(merge_dropna['crash_count']/merge_dropna['length'])\n#(np.log10(merged['crash_count']/merged['length']) - np.log10(merged['crash_count']/merged['length']).min())/ (np.log10(merged['crash_count']/merged['length']).max() - np.log10(merged['crash_count']/merged['length']).min())\nmerge_dropna['crash index'] = (calculation - calculation.min())/(calculation.max() - calculation.min())\nmerge_dropna.head(5)\n\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n\n\n\n\n\n\n\n\n\nu\nv\nosmid\noneway\nname\nhighway\nreversed\nlength\ngeometry\nmaxspeed\nlanes\ntunnel\nbridge\nref\nwidth\nservice\naccess\njunction\ncrash_count\ncrash index\n\n\n\n\n9\n110024052\n110024066\n12139665\nTrue\nSouth 17th Street\ntertiary\nFalse\n135.106\nLINESTRING (-75.17134 39.93966, -75.17152 39.9...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n9.0\n0.431858\n\n\n30\n109729474\n3425014859\n62154356\nTrue\nArch Street\nsecondary\nFalse\n126.087\nLINESTRING (-75.14847 39.95259, -75.14859 39.9...\n25 mph\n2\n0\n0\n0\n0\n0\n0\n0\n2.0\n0.258123\n\n\n31\n109729486\n110342146\n[12169305, 1052694387]\nTrue\nNorth Independence Mall East\nsecondary\nFalse\n123.116\nLINESTRING (-75.14832 39.95333, -75.14813 39.9...\n0\n[3, 2]\n0\n0\n0\n0\n0\n0\n0\n4.0\n0.344930\n\n\n33\n3425014859\n5372059859\n[12197696, 1003976882, 424804083]\nTrue\nNorth Independence Mall West\nsecondary\nFalse\n229.386\nLINESTRING (-75.14993 39.95277, -75.14995 39.9...\n0\n3\n0\n0\n0\n0\n0\n0\n0\n2.0\n0.185670\n\n\n35\n110342146\n315655546\n[424807270, 12109183]\nTrue\n0\nmotorway_link\nFalse\n153.933\nLINESTRING (-75.14809 39.95442, -75.14808 39.9...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n3.0\n0.283054\n\n\n\n\n\n\n\n\n\n1.12 Plot a histogram of the crash index values\nUse matplotlib’s hist() function to plot the crash index values from the previous step.\nYou should see that the index values are Gaussian-distributed, providing justification for why we log-transformed!\n\n\nCode\nplt.hist(merge_dropna['crash index'], color='pink', bins=20)\n\nplt.xlabel('Crash Index')\nplt.ylabel('Frequency')\nplt.title('Crash Index Values Histogram')\nplt.show()\n\n\n\n\n\n\n\n1.13 Plot an interactive map of the street networks, colored by the crash index\nYou can use GeoPandas to make an interactive Folium map, coloring the streets by the crash index column.\nTip: if you use the viridis color map, try using a “dark” tile set for better constrast of the colors.\n\n\nCode\ncrash_index = merge_dropna[['u', 'v', 'crash index']]\nmerged_index = pd.merge(merged, crash_index, on=['u','v'], how='left').fillna(0)\n\n\n\n\nCode\nmerged_index.explore(\n    column=\"crash index\",\n    cmap=\"YlGnBu\",\n    tiles=\"cartodbpositron\",)\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/assignment-4.html#part-2-scraping-craigslist",
    "href": "analysis/assignment-4.html#part-2-scraping-craigslist",
    "title": "4 - Street Networks & Web Scraping",
    "section": "Part 2: Scraping Craigslist",
    "text": "Part 2: Scraping Craigslist\nIn this part, we’ll be extracting information on apartments from Craigslist search results. You’ll be using Selenium and BeautifulSoup to extract the relevant information from the HTML text.\nFor reference on CSS selectors, please see the notes from Week 6.\n\nPrimer: the Craigslist website URL\nWe’ll start with the Philadelphia region. First we need to figure out how to submit a query to Craigslist. As with many websites, one way you can do this is simply by constructing the proper URL and sending it to Craigslist.\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1gallery0~0\nThere are three components to this URL.\n\nThe base URL: http://philadelphia.craigslist.org/search/apa\nThe user’s search parameters: ?min_price=1&min_bedrooms=1&minSqft=1\n\n\nWe will send nonzero defaults for some parameters (bedrooms, size, price) in order to exclude results that have empty values for these parameters.\n\n\nThe URL hash: #search=1~gallery~0~0\n\n\nAs we will see later, this part will be important because it contains the search page result number.\n\nThe Craigslist website requires Javascript, so we’ll need to use Selenium to load the page, and then use BeautifulSoup to extract the information we want.\n\n\n2.1 Initialize a selenium driver and open Craigslist\nAs discussed in lecture, you can use Chrome, Firefox, or Edge as your selenium driver. In this part, you should do two things:\n\nInitialize the selenium driver\nUse the driver.get() function to open the following URL:\n\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1gallery0~0\nThis will give you the search results for 1-bedroom apartments in Philadelphia.\n\n\nCode\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nimport requests\n\n\n\n\nCode\ndriver = webdriver.Chrome()\nurl = \"https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1~gallery~0~0\"\ndriver.get(url)\n\n\n\n\n2.2 Initialize your “soup”\nOnce selenium has the page open, we can get the page source from the driver and use BeautifulSoup to parse it. In this part, initialize a BeautifulSoup object with the driver’s page source\n\n\nCode\nsoup = BeautifulSoup(driver.page_source, \"html.parser\")\n\n\n\n\n2.3 Parsing the HTML\nNow that we have our “soup” object, we can use BeautifulSoup to extract out the elements we need:\n\nUse the Web Inspector to identify the HTML element that holds the information on each apartment listing.\nUse BeautifulSoup to extract these elements from the HTML.\n\nAt the end of this part, you should have a list of 120 elements, where each element is the listing for a specific apartment on the search page.\n\n\nCode\nselector = \".cl-search-result.cl-search-view-mode-gallery\"\n\ntables = soup.select(selector)\n\n\n\n\nCode\nlen(tables)\n\n\n120\n\n\n\n\n2.4 Find the relevant pieces of information\nWe will now focus on the first element in the list of 120 apartments. Use the prettify() function to print out the HTML for this first element.\nFrom this HTML, identify the HTML elements that hold:\n\nThe apartment price\nThe number of bedrooms\nThe square footage\nThe apartment title\n\nFor the first apartment, print out each of these pieces of information, using BeautifulSoup to select the proper elements.\n\n\nCode\nprint(tables[1].prettify())\n\n\n&lt;li class=\"cl-search-result cl-search-view-mode-gallery\" data-pid=\"7681605868\" title=\"Apartment by the arts!\"&gt;\n &lt;div class=\"gallery-card\"&gt;\n  &lt;div class=\"cl-gallery\"&gt;\n   &lt;div class=\"gallery-inner\"&gt;\n    &lt;a class=\"main\" href=\"https://philadelphia.craigslist.org/apa/d/philadelphia-apartment-by-the-arts/7681605868.html\"&gt;\n     &lt;div class=\"swipe\" style=\"visibility: visible;\"&gt;\n      &lt;div class=\"swipe-wrap\" style=\"width: 4032px;\"&gt;\n       &lt;div data-index=\"0\" style=\"width: 336px; left: 0px; transition-duration: 0ms; transform: translateX(0px);\"&gt;\n        &lt;span class=\"loading icom-\"&gt;\n        &lt;/span&gt;\n        &lt;img alt=\"Apartment by the arts! 1\" src=\"https://images.craigslist.org/00j0j_bWLeLp3Yzpi_0x20m2_300x300.jpg\"/&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"1\" style=\"width: 336px; left: -336px; transition-duration: 0ms; transform: translateX(336px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"2\" style=\"width: 336px; left: -672px; transition-duration: 0ms; transform: translateX(336px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"3\" style=\"width: 336px; left: -1008px; transition-duration: 0ms; transform: translateX(336px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"4\" style=\"width: 336px; left: -1344px; transition-duration: 0ms; transform: translateX(336px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"5\" style=\"width: 336px; left: -1680px; transition-duration: 0ms; transform: translateX(-336px);\"&gt;\n       &lt;/div&gt;\n      &lt;/div&gt;\n     &lt;/div&gt;\n     &lt;div class=\"slider-back-arrow icom-\"&gt;\n     &lt;/div&gt;\n     &lt;div class=\"slider-forward-arrow icom-\"&gt;\n     &lt;/div&gt;\n    &lt;/a&gt;\n   &lt;/div&gt;\n   &lt;div class=\"dots\"&gt;\n    &lt;span class=\"dot selected\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n   &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;a class=\"cl-app-anchor text-only posting-title\" href=\"https://philadelphia.craigslist.org/apa/d/philadelphia-apartment-by-the-arts/7681605868.html\" tabindex=\"0\"&gt;\n   &lt;span class=\"label\"&gt;\n    Apartment by the arts!\n   &lt;/span&gt;\n  &lt;/a&gt;\n  &lt;div class=\"meta\"&gt;\n   14 mins ago\n   &lt;span class=\"separator\"&gt;\n    ·\n   &lt;/span&gt;\n   &lt;span class=\"housing-meta\"&gt;\n    &lt;span class=\"post-bedrooms\"&gt;\n     2br\n    &lt;/span&gt;\n    &lt;span class=\"post-sqft\"&gt;\n     1390ft\n     &lt;span class=\"exponent\"&gt;\n      2\n     &lt;/span&gt;\n    &lt;/span&gt;\n   &lt;/span&gt;\n   &lt;span class=\"separator\"&gt;\n    ·\n   &lt;/span&gt;\n   Philadelphia\n  &lt;/div&gt;\n  &lt;span class=\"priceinfo\"&gt;\n   $2,850\n  &lt;/span&gt;\n  &lt;button class=\"bd-button cl-favorite-button icon-only\" tabindex=\"0\" title=\"add to favorites list\" type=\"button\"&gt;\n   &lt;span class=\"icon icom-\"&gt;\n   &lt;/span&gt;\n   &lt;span class=\"label\"&gt;\n   &lt;/span&gt;\n  &lt;/button&gt;\n  &lt;button class=\"bd-button cl-banish-button icon-only\" tabindex=\"0\" title=\"hide posting\" type=\"button\"&gt;\n   &lt;span class=\"icon icom-\"&gt;\n   &lt;/span&gt;\n   &lt;span class=\"label\"&gt;\n    hide\n   &lt;/span&gt;\n  &lt;/button&gt;\n &lt;/div&gt;\n&lt;/li&gt;\n\n\n\n\n\nCode\ntable = tables[1]\n\n\n\n\nCode\ndata = []\n\n#price\nprice = table.select_one(\".priceinfo\").text\n\n#no. of bedrooms\nbedrooms = table.select_one(\".post-bedrooms\").text\n    \n#square footage\nsize = table.select_one(\".post-sqft\").text\n    \n#apt name\nname = table.select_one(\".label\").text\n\ndata.append({\"Price\": price, \"Bedrooms\": bedrooms, \"Size\": size, \"Name\": name})\n\ndata = pd.DataFrame(data)\n\ndata\n\n\n\n\n\n\n\n\n\nPrice\nBedrooms\nSize\nName\n\n\n\n\n0\n$2,850\n2br\n1390ft2\nApartment by the arts!\n\n\n\n\n\n\n\n\n\n2.5 Functions to format the results\nIn this section, you’ll create functions that take in the raw string elements for price, size, and number of bedrooms and returns them formatted as numbers.\nI’ve started the functions to format the values. You should finish theses functions in this section.\nHints - You can use string formatting functions like string.replace() and string.strip() - The int() and float() functions can convert strings to numbers\n\n\nCode\ndef format_bedrooms(bedrooms_string):\n    \n    bedrooms = int(bedrooms_string.replace(\"br\", \"\"))\n    \n    return bedrooms\n\n\n\n\nCode\ndef format_size(size_string):\n    \n    size = int(size_string.replace(\"ft\", \"\"))\n    \n    return size \n\n\n\n\nCode\ndef format_price(price_string):\n    \n    price = int(price_string.replace(\"$\", \"\").replace(\",\" , \"\"))\n    \n    return price\n\n\n\n\n2.6 Putting it all together\nIn this part, you’ll complete the code block below using results from previous parts. The code will loop over 5 pages of search results and scrape data for 600 apartments.\nWe can get a specific page by changing the search=PAGE part of the URL hash. For example, to get page 2 instead of page 1, we will navigate to:\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=2gallery0~0\nIn the code below, the outer for loop will loop over 5 pages of search results. The inner for loop will loop over the 120 apartments listed on each search page.\nFill in the missing pieces of the inner loop using the code from the previous section. We will be able to extract out the relevant pieces of info for each apartment.\nAfter filling in the missing pieces and executing the code cell, you should have a Data Frame called results that holds the data for 600 apartment listings.\n\nNotes\nBe careful if you try to scrape more listings. Craigslist will temporarily ban your IP address (for a very short time) if you scrape too much at once. I’ve added a sleep() function to the for loop to wait 30 seconds between scraping requests.\nIf the for loop gets stuck at the “Processing page X…” step for more than a minute or so, your IP address is probably banned temporarily, and you’ll have to wait a few minutes before trying again.\n\n\nCode\nfrom time import sleep\n\n\n\n\nCode\nresults = []\n\n# search in batches of 120 for 5 pages\n# NOTE: you will get temporarily banned if running more than ~5 pages or so\n# the API limits are more leninient during off-peak times, and you can try\n# experimenting with more pages\nmax_pages = 5\n\n# The base URL we will be using\nbase_url = \"https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1\"\n\n# loop over each page of search results\nfor page_num in range(1, max_pages + 1):\n    print(f\"Processing page {page_num}...\")\n\n    # Update the URL hash for this page number and make the combined URL\n    url_hash = f\"#search={page_num}~gallery~0~0\"\n    url = base_url + url_hash\n\n    # Go to the driver and wait for 5 seconds\n    driver.get(url)\n    sleep(5)\n\n    # YOUR CODE: get the list of all apartments\n    # This is the same code from Part 1.2 and 1.3\n    # It should be a list of 120 apartments\n    soup = soup\n    apts = tables\n    print(\"Number of apartments = \", len(apts))\n\n    # loop over each apartment in the list\n    page_results = []\n    for apt in apts:\n\n        # YOUR CODE: the bedrooms string\n        bedrooms = apt.select_one(\".post-bedrooms\").text\n\n        # YOUR CODE: the size string\n        size = apt.select_one(\".post-sqft\").text\n\n        # YOUR : the title string\n        title = apt.select_one(\".label\").text\n\n        # YOUR CODE: the price string\n        price = apt.select_one(\".priceinfo\").text\n\n\n        # Format using functions from Part 1.5\n        bedrooms = format_bedrooms(bedrooms)\n        size = format_size(size)\n        price = format_price(price)\n\n        # Save the result\n        page_results.append([price, size, bedrooms, title])\n\n    # Create a dataframe and save\n    col_names = [\"price\", \"size\", \"bedrooms\", \"title\"]\n    df = pd.DataFrame(page_results, columns=col_names)\n    results.append(df)\n\n    print(\"sleeping for 10 seconds between calls\")\n    sleep(10)\n\n# Finally, concatenate all the results\nresults = pd.concat(results, axis=0).reset_index(drop=True)\n\n\nProcessing page 1...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 2...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 3...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 4...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 5...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\n\n\n\n\nCode\nresults\n\n\n\n\n\n\n\n\n\nprice\nsize\nbedrooms\ntitle\npricepersq\n\n\n\n\n0\n2474\n13942\n3\n3bd 2ba, Serene Wooded View, West Chester PA\n0.177449\n\n\n1\n2850\n13902\n2\nApartment by the arts!\n0.205006\n\n\n2\n1800\n7592\n1\nA Cozy Living Space In Rittenhouse Square.\n0.237092\n\n\n3\n1750\n5002\n1\nPets welcome in this Beautifully renovated apa...\n0.349860\n\n\n4\n1950\n8502\n2\nEnjoy a fantastic living space with tons of na...\n0.229358\n\n\n...\n...\n...\n...\n...\n...\n\n\n595\n2075\n12712\n2\nResort-Style Swimming Pool, Extra Storage, 2/BD\n0.163232\n\n\n596\n1876\n5072\n1\nOn-demand car wash/detailing, Handyman and mai...\n0.369874\n\n\n597\n1680\n5092\n1\nPenthouse Hideaway, LVT Flooring, Bike Storage\n0.329929\n\n\n598\n2199\n6922\n1\nFire Pit, On-site Management/Maintenance, Flex...\n0.317683\n\n\n599\n1544\n5182\n1\n1 BD, Bike Storage, Washer/Dryer\n0.297954\n\n\n\n\n600 rows × 5 columns\n\n\n\n\n\n\n2.7 Plotting the distribution of prices\nUse matplotlib’s hist() function to make two histograms for:\n\nApartment prices\nApartment prices per square foot (price / size)\n\nMake sure to add labels to the respective axes and a title describing the plot.\n\n\nCode\nplt.title('Apartment Price Distributions')\nplt.hist(results['price'], color='pink', edgecolor='grey')\nplt.xlabel('Price ($)')\nplt.ylabel('Counts')\nplt.show()\n\n\n\n\n\n\n\nCode\nresults['pricepersq'] = results['price'] / results['size']\n\nplt.title('Apartment Price Per Square foot')\nplt.hist(results['pricepersq'], color='lightgrey', edgecolor='grey')\nplt.xlabel('Price per sqft($)')\nplt.ylabel('Counts')\nplt.show()\n\n\n\n\n\n\nSide note: rental prices per sq. ft. from Craigslist\nThe histogram of price per sq ft should be centered around ~1.5. Here is a plot of how Philadelphia’s rents compare to the other most populous cities:\n\nSource\n\n\n\n2.8 Comparing prices for different sizes\nUse altair to explore the relationship between price, size, and number of bedrooms. Make an interactive scatter plot of price (x-axis) vs. size (y-axis), with the points colored by the number of bedrooms.\nMake sure the plot is interactive (zoom-able and pan-able) and add a tooltip with all of the columns in our scraped data frame.\nWith this sort of plot, you can quickly see the outlier apartments in terms of size and price.\n\n\nCode\nchart = alt.Chart(results)\nchart = chart.mark_circle(size=40)\nchart = chart.encode(\n    x = \"price:Q\",\n    y = \"size:Q\",\n    color = \"bedrooms:N\",\n    tooltip = [\"title\", \"price\", \"size\", \"bedrooms\"],\n)\n\nchart.interactive()"
  },
  {
    "objectID": "analysis/assignment-5.html",
    "href": "analysis/assignment-5.html",
    "title": "5 - Exploring Yelp Reviews in Philly",
    "section": "",
    "text": "In this assignment, we’ll explore restaurant review data available through the Yelp Dataset Challenge. The dataset includes Yelp data for user reviews and business information for many metropolitan areas. I’ve already downloaded this dataset (8 GB total!) and extracted out the data files for reviews and restaurants in Philadelphia. I’ve placed these data files into the data directory in this repository.\nThis assignment is broken into two parts:\nPart 1: Analyzing correlations between restaurant reviews and census data\nWe’ll explore the relationship between restaurant reviews and the income levels of the restaurant’s surrounding area.\nPart 2: Exploring the impact of fast food restaurants\nWe’ll run a sentiment analysis on reviews of fast food restaurants and estimate income levels in neighborhoods with fast food restaurants. We’ll test how well our sentiment analysis works by comparing the number of stars to the sentiment of reviews.\nBackground readings - Does sentiment analysis work? - The Geography of Taste: Using Yelp to Study Urban Culture\nCode\nimport pandas as pd\nimport geopandas as gpd\nimport altair as alt\nimport numpy as np"
  },
  {
    "objectID": "analysis/assignment-5.html#correlating-restaurant-ratings-and-income-levels",
    "href": "analysis/assignment-5.html#correlating-restaurant-ratings-and-income-levels",
    "title": "5 - Exploring Yelp Reviews in Philly",
    "section": "1. Correlating restaurant ratings and income levels",
    "text": "1. Correlating restaurant ratings and income levels\nIn this part, we’ll use the census API to download household income data and explore how it correlates with restaurant review data.\n\n1.1 Query the Census API\nUse the cenpy package to download median household income in the past 12 months by census tract from the 2021 ACS 5-year data set for your county of interest.\nYou have two options to find the correct variable names: - Search through: https://api.census.gov/data/2021/acs/acs5/variables.html - Initialize an API connection and use the .varslike() function to search for the proper keywords\nAt the end of this step, you should have a pandas DataFrame holding the income data for all census tracts within the county being analyzed. Feel free to rename your variable from the ACS so it has a more meaningful name!\n\n\n\n\n\n\nCaution\n\n\n\nSome census tracts won’t have any value because there are not enough households in that tract. The census will use a negative number as a default value for those tracts. You can safely remove those tracts from the analysis!\n\n\n\n\nCode\nimport cenpy\n\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/libpysal/cg/alpha_shapes.py:39: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def nb_dist(x, y):\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/libpysal/cg/alpha_shapes.py:165: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def get_faces(triangle):\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/libpysal/cg/alpha_shapes.py:199: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def build_faces(faces, triangles_is, num_triangles, num_faces_single):\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/libpysal/cg/alpha_shapes.py:261: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def nb_mask_faces(mask, faces):\n\n\n\n\nCode\nacs = cenpy.remote.APIConnection(\"ACSDT5Y2021\")\n\n\n\n\nCode\nmatches = acs.varslike(\n    pattern=\"household income\",\n    by=\"label\",\n).sort_index()\n\n\n\n\nCode\npd.set_option('display.max_colwidth', None)\nmatches.head(10)\n\n\n\n\n\n\n\n\n\nlabel\nconcept\npredicateType\ngroup\nlimit\npredicateOnly\nhasGeoCollectionSupport\nattributes\nrequired\n\n\n\n\nB19013A_001E\nEstimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS) (WHITE ALONE HOUSEHOLDER)\nint\nB19013A\n0\nNaN\nNaN\nB19013A_001EA,B19013A_001M,B19013A_001MA\nNaN\n\n\nB19013B_001E\nEstimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS) (BLACK OR AFRICAN AMERICAN ALONE HOUSEHOLDER)\nint\nB19013B\n0\nNaN\nNaN\nB19013B_001EA,B19013B_001M,B19013B_001MA\nNaN\n\n\nB19013C_001E\nEstimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS) (AMERICAN INDIAN AND ALASKA NATIVE ALONE HOUSEHOLDER)\nint\nB19013C\n0\nNaN\nNaN\nB19013C_001EA,B19013C_001M,B19013C_001MA\nNaN\n\n\nB19013D_001E\nEstimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS) (ASIAN ALONE HOUSEHOLDER)\nint\nB19013D\n0\nNaN\nNaN\nB19013D_001EA,B19013D_001M,B19013D_001MA\nNaN\n\n\nB19013E_001E\nEstimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS) (NATIVE HAWAIIAN AND OTHER PACIFIC ISLANDER ALONE HOUSEHOLDER)\nint\nB19013E\n0\nNaN\nNaN\nB19013E_001EA,B19013E_001M,B19013E_001MA\nNaN\n\n\nB19013F_001E\nEstimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS) (SOME OTHER RACE ALONE HOUSEHOLDER)\nint\nB19013F\n0\nNaN\nNaN\nB19013F_001EA,B19013F_001M,B19013F_001MA\nNaN\n\n\nB19013G_001E\nEstimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS) (TWO OR MORE RACES HOUSEHOLDER)\nint\nB19013G\n0\nNaN\nNaN\nB19013G_001EA,B19013G_001M,B19013G_001MA\nNaN\n\n\nB19013H_001E\nEstimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS) (WHITE ALONE, NOT HISPANIC OR LATINO HOUSEHOLDER)\nint\nB19013H\n0\nNaN\nNaN\nB19013H_001EA,B19013H_001M,B19013H_001MA\nNaN\n\n\nB19013I_001E\nEstimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS) (HISPANIC OR LATINO HOUSEHOLDER)\nint\nB19013I\n0\nNaN\nNaN\nB19013I_001EA,B19013I_001M,B19013I_001MA\nNaN\n\n\nB19013_001E\nEstimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS)\nint\nB19013\n0\nNaN\nNaN\nB19013_001EA,B19013_001M,B19013_001MA\nNaN\n\n\n\n\n\n\n\n\n\nCode\nvariables = [\n    \"NAME\",\n    \"B19013_001E\" #median income in the past 12 month\n]\n\n\n\n\nCode\nphilly_county_code = \"101\"\npa_state_code = \"42\"\n\nphilly_income = acs.query(cols=variables, geo_unit=\"block group:*\", geo_filter={\"state\": pa_state_code, \"county\": philly_county_code, \"tract\": \"*\"}, apikey='b3abcecc231fa30ccaa18cb5e854c30f1982fe3f')\nphilly_income.head(10)\n\nphilly_income['B19013_001E'] = philly_income['B19013_001E'].replace('-666666666', '0').astype('int32')\n\n\n\n\nCode\nphilly_income.head()\n\n\n\n\n\n\n\n\n\nNAME\nB19013_001E\nstate\ncounty\ntract\nblock group\n\n\n\n\n0\nBlock Group 1, Census Tract 1.01, Philadelphia County, Pennsylvania\n0\n42\n101\n000101\n1\n\n\n1\nBlock Group 2, Census Tract 1.01, Philadelphia County, Pennsylvania\n0\n42\n101\n000101\n2\n\n\n2\nBlock Group 3, Census Tract 1.01, Philadelphia County, Pennsylvania\n0\n42\n101\n000101\n3\n\n\n3\nBlock Group 4, Census Tract 1.01, Philadelphia County, Pennsylvania\n97210\n42\n101\n000101\n4\n\n\n4\nBlock Group 5, Census Tract 1.01, Philadelphia County, Pennsylvania\n109269\n42\n101\n000101\n5\n\n\n\n\n\n\n\n\n\n1.2 Download census tracts from the Census and merge the data from part 1.1\n\nDownload census tracts for the desired geography using the pygris package\nMerge the downloaded census tracts with the household income DataFrame\n\n\n\nCode\nimport pygris\nphilly_blocks = pygris.block_groups(state=pa_state_code, county=philly_county_code, year=2021)\n\n\n\n\nCode\nphilly_merged = philly_blocks.merge(\n    philly_income,\n    left_on=[\"STATEFP\", \"COUNTYFP\", \"TRACTCE\", \"BLKGRPCE\"],\n    right_on=[\"state\", \"county\", \"tract\", \"block group\"],)\n\n\n\n\nCode\nphilly_merged.head(1)\n\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nTRACTCE\nBLKGRPCE\nGEOID\nNAMELSAD\nMTFCC\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\ngeometry\nNAME\nB19013_001E\nstate\ncounty\ntract\nblock group\n\n\n\n\n0\n42\n101\n989100\n2\n421019891002\nBlock Group 2\nG5030\nS\n373653\n7060\n+40.0373207\n-075.0177378\nPOLYGON ((-75.02195 40.03435, -75.02191 40.03451, -75.02173 40.03482, -75.02151 40.03515, -75.02135 40.03544, -75.02129 40.03562, -75.02130 40.03582, -75.02134 40.03607, -75.02150 40.03638, -75.02163 40.03663, -75.02181 40.03702, -75.02168 40.03709, -75.01967 40.03813, -75.01902 40.03847, -75.01843 40.03877, -75.01783 40.03908, -75.01703 40.03950, -75.01621 40.03991, -75.01535 40.03894, -75.01500 40.03910, -75.01432 40.03945, -75.01254 40.03803, -75.01216 40.03772, -75.01195 40.03755, -75.01131 40.03704, -75.01225 40.03654, -75.01530 40.03485, -75.01790 40.03340, -75.01797 40.03336, -75.01817 40.03350, -75.01862 40.03366, -75.01959 40.03374, -75.01986 40.03376, -75.02000 40.03377, -75.02011 40.03377, -75.02022 40.03378, -75.02043 40.03379, -75.02083 40.03378, -75.02125 40.03374, -75.02142 40.03375, -75.02157 40.03381, -75.02173 40.03392, -75.02180 40.03401, -75.02189 40.03411, -75.02195 40.03422, -75.02195 40.03435))\nBlock Group 2, Census Tract 9891, Philadelphia County, Pennsylvania\n0\n42\n101\n989100\n2\n\n\n\n\n\n\n\n\n\n1.3 Load the restaurants data\nThe Yelp dataset includes data for 7,350 restaurants across the city. Load the data from the data/ folder and use the latitude and longitude columns to create a GeoDataFrame after loading the JSON data. Be sure to set the right CRS on when initializing the GeoDataFrame!\nNotes\nThe JSON data is in a “records” format. To load it, you’ll need to pass the following keywords:\n\norient='records'\nlines=True\n\n\n\nCode\ndata = pd.read_json(\"data/restaurants_philly.json.gz\", orient='records', lines=True)\ngpd_Data = gpd.GeoDataFrame(data, geometry=gpd.points_from_xy(data.longitude, data.latitude), crs=\"EPSG:4326\")\n\n\n\n\nCode\ngpd_Data.head()\n\n\n\n\n\n\n\n\n\nbusiness_id\nlatitude\nlongitude\nname\nreview_count\nstars\ncategories\ngeometry\n\n\n\n\n0\nMTSW4McQd7CbVtyjqoe9mw\n39.955505\n-75.155564\nSt Honore Pastries\n80\n4.0\nRestaurants, Food, Bubble Tea, Coffee & Tea, Bakeries\nPOINT (-75.15556 39.95551)\n\n\n1\nMUTTqe8uqyMdBl186RmNeA\n39.953949\n-75.143226\nTuna Bar\n245\n4.0\nSushi Bars, Restaurants, Japanese\nPOINT (-75.14323 39.95395)\n\n\n2\nROeacJQwBeh05Rqg7F6TCg\n39.943223\n-75.162568\nBAP\n205\n4.5\nKorean, Restaurants\nPOINT (-75.16257 39.94322)\n\n\n3\nQdN72BWoyFypdGJhhI5r7g\n39.939825\n-75.157447\nBar One\n65\n4.0\nCocktail Bars, Bars, Italian, Nightlife, Restaurants\nPOINT (-75.15745 39.93982)\n\n\n4\nMjboz24M9NlBeiOJKLEd_Q\n40.022466\n-75.218314\nDeSandro on Main\n41\n3.0\nPizza, Restaurants, Salad, Soup\nPOINT (-75.21831 40.02247)\n\n\n\n\n\n\n\n\n\n1.4 Add tract info for each restaurant\nDo a spatial join to identify which census tract each restaurant is within. Make sure each dataframe has the same CRS!\nAt the end of this step, you should have a new dataframe with a column identifying the tract number for each restaurant.\n\n\nCode\ntract_restaurant = gpd.sjoin(gpd_Data, philly_merged.loc[:,[\"geometry\", \"tract\", \"B19013_001E\"]], how=\"left\", op=\"within\")\ntract_restaurant = tract_restaurant.rename(columns={'B19013_001E': 'Median Household Income'})\ntract_restaurant.head()\n\n\n/Users/annzhang/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n/var/folders/q3/y0zpvj752qg3_3nvpkx6v2300000gn/T/ipykernel_14881/1177895681.py:1: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\nUse `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n\nLeft CRS: EPSG:4326\nRight CRS: EPSG:4269\n\n  tract_restaurant = gpd.sjoin(gpd_Data, philly_merged.loc[:,[\"geometry\", \"tract\", \"B19013_001E\"]], how=\"left\", op=\"within\")\n\n\n\n\n\n\n\n\n\nbusiness_id\nlatitude\nlongitude\nname\nreview_count\nstars\ncategories\ngeometry\nindex_right\ntract\nMedian Household Income\n\n\n\n\n0\nMTSW4McQd7CbVtyjqoe9mw\n39.955505\n-75.155564\nSt Honore Pastries\n80\n4.0\nRestaurants, Food, Bubble Tea, Coffee & Tea, Bakeries\nPOINT (-75.15556 39.95551)\n988.0\n000200\n42308.0\n\n\n1\nMUTTqe8uqyMdBl186RmNeA\n39.953949\n-75.143226\nTuna Bar\n245\n4.0\nSushi Bars, Restaurants, Japanese\nPOINT (-75.14323 39.95395)\n658.0\n000102\n198125.0\n\n\n2\nROeacJQwBeh05Rqg7F6TCg\n39.943223\n-75.162568\nBAP\n205\n4.5\nKorean, Restaurants\nPOINT (-75.16257 39.94322)\n689.0\n001500\n107292.0\n\n\n3\nQdN72BWoyFypdGJhhI5r7g\n39.939825\n-75.157447\nBar One\n65\n4.0\nCocktail Bars, Bars, Italian, Nightlife, Restaurants\nPOINT (-75.15745 39.93982)\n774.0\n001800\n103125.0\n\n\n4\nMjboz24M9NlBeiOJKLEd_Q\n40.022466\n-75.218314\nDeSandro on Main\n41\n3.0\nPizza, Restaurants, Salad, Soup\nPOINT (-75.21831 40.02247)\n715.0\n021000\n86146.0\n\n\n\n\n\n\n\n\n\n1.5 Add income data to your restaurant data\nAdd the income data to your dataframe from the previous step, merging the census data based on the tract that each restaurant is within.\n\n\nCode\n# this step is completed in 1.4\n\n\n\n\n1.6 Make a plot of median household income vs. Yelp stars\nOur dataset has the number of stars for each restaurant, rounded to the nearest 0.5 star. In this step, create a line plot that shows the average income value for each stars category (e.g., all restaurants with 1 star, 1.5 stars, 2 stars, etc.)\nWhile their are multiple ways to do this, the seaborn.lineplot() is a great option. This can show the average value in each category as well as 95% uncertainty intervals. Use this function to plot the stars (“x”) vs. average income (“y”) for all of our restaurants, using the dataframe from last step. Be sure to format your figure to make it look nice!\nQuestion: Is there a correlation between a restaurant’s ratings and the income levels of its surrounding neighborhood?\n\n\nCode\nstars = tract_restaurant.loc[:, ['stars','Median Household Income']].groupby(['stars']).median(['Median Household Income']).reset_index()\n\n\n\n\nCode\nimport seaborn as sns\n\n\n\n\nCode\nsns.set_theme()\nsns.lineplot(data=stars, x=\"stars\", y=\"Median Household Income\")\n\n\n&lt;Axes: xlabel='stars', ylabel='Median Household Income'&gt;\n\n\n\n\n\nAs suggested by the graph above, there is an overall trend of higher ratings of restaurants associated with higeher median household income in the tracts that restaurants are located in."
  },
  {
    "objectID": "analysis/assignment-5.html#fast-food-trends-in-philadelphia",
    "href": "analysis/assignment-5.html#fast-food-trends-in-philadelphia",
    "title": "5 - Exploring Yelp Reviews in Philly",
    "section": "2. Fast food trends in Philadelphia",
    "text": "2. Fast food trends in Philadelphia\nAt the end of part 1, you should have seen a strong trend where higher income tracts generally had restaurants with better reviews. In this section, we’ll explore the impact of fast food restaurants and how they might be impacting this trend.\nHypothesis\n\nFast food restaurants are predominantly located in areas with lower median income levels.\nFast food restaurants have worse reviews compared to typical restaurants.\n\nIf true, these two hypotheses could help to explain the trend we found in part 1. Let’s dive in and test our hypotheses!\n\n2.1 Identify fast food restaurants\nThe “categories” column in our dataset contains multiple classifications for each restaurant. One such category is “Fast Food”. In this step, add a new column called “is_fast_food” that is True if the “categories” column contains the term “Fast Food” and False otherwise\n\n\nCode\ntract_restaurant['is_fast_food'] = tract_restaurant['categories'].str.contains('Fast Food')\n\n\n\n\n2.2 Calculate the median income for fast food and otherwise\nGroup by the “is_fast_food” column and calculate the median income for restaurants that are and are not fast food. You should find that income levels are lower in tracts with fast food.\nNote: this is just an estimate, since we are calculating a median of median income values.\n\n\nCode\nfastfood_income = tract_restaurant.loc[:, ['is_fast_food','Median Household Income']].groupby(['is_fast_food']).median(['Median Household Income']).reset_index()\nfastfood_income\n\n\n\n\n\n\n\n\n\nis_fast_food\nMedian Household Income\n\n\n\n\n0\nFalse\n66290.0\n\n\n1\nTrue\n46668.0\n\n\n\n\n\n\n\n\n\n2.3 Load fast food review data\nIn the rest of part 2, we’re going to run a sentiment analysis on the reviews for fast food restaurants. The review data for all fast food restaurants identified in part 2.1 is already stored in the data/ folder. The data is stored as a JSON file and you can use pandas.read_json to load it.\nNotes\nThe JSON data is in a “records” format. To load it, you’ll need to pass the following keywords:\n\norient='records'\nlines=True\n\n\n\nCode\nreviews = pd.read_json(\"data/reviews_philly_fast_food.json.gz\", orient='records',lines=True)\nreviews.head()\n\n\n\n\n\n\n\n\n\nbusiness_id\nreview_id\nstars\ntext\n\n\n\n\n0\nkgMEBZG6rjkGeFzPaIM4MQ\nE-yGr1OhsUBxNeUVLDVouA\n1\nI know I shouldn't expect much but everything I asked for that was on the drive thru menu was not available. I was actually afraid of what I was going to get once I did get it. I saw the movie \"waiting\". Word of advice stay clear of this arch. Just so you know I was only trying to order a beverage how pathetic is that.\n\n\n1\nFKrP06TDAKtxNG1vrRQcQQ\n0IpFZoaY_RRNjha8Q_Wz6w\n2\nPerfect place to go if you like waiting 20 minutes at the counter and getting dirty looks from the waiters while you're waiting for service. My friend and I made the mistake of coming here after being famished after a long day. Hey Johnny Rockets, I understand that you guys can get busy, but a quick acknowledgment that we existed would have sufficed. After finally waving down a waitress, we finally got a menu with a heavy sigh on the side as if we were troubling the staff. I urged my friend that we should probably leave, but we had already waited long enough, so I reluctantly ordered.\\nI got the original burger and fries as did my friend as per usual. Don't get me wrong, the burgers and fries are decent but definitely not worth the wait or hassle. Even the chocolate shake that I ordered was pretty good with a good consistency, but the taste of bad service still lingered and eventually overpowered the chocolate flavored concoction. Needless to say, I will definitely not be coming back to this location if it continues to be managed so rudely.\n\n\n2\nw9hS5x1F52Id-G1KTrAOZg\n0KlwfaHZyvao41_3S47dyg\n2\nWas not a fan of their cheesesteak. Their wiz sauce was mustard based and it was not terrible as a sandwich itself but mustard is not the flavor one expects or wants in a cheesesteak. It was overwhelming and not good.\n\n\n3\nfr2qDm_mY1afIGMvqsKUCg\noKSUOq7pCQzyypFDSa1HoA\n3\nOk this is an aberration from my city foodie reviews but I figured I'll take the time and review some of the local establishments that I order from on a consistent base while working in Bensalem, P.A. Without further adieu...\\nGeorge's Chix is good. I'm always satisfied with the food and value each and every time. I haven't branched out seeking much more than a good deal each time and trust me I'm not looking to find the healthiest option on the menu (usually I get the nugget/fries/soda deal, the chix fingers/fries/soda deal, or Ceasar wrap/fries/soda deal- $8 or $9 after a nice tip) but the food is alright.\\nGive George's a try if you're in town and looking to get down on some fried grub!\n\n\n4\nfr2qDm_mY1afIGMvqsKUCg\n6SMUmb7Npwnq6AusxqOXzQ\n5\nMy family has been customers of George's for years. You pay for what you get. The quality of the food is premier and they don't hold back on portion size. Their cheesesteaks are amazing (chopped steak) and their chicken cheesesteaks are made from the same chicken as their Char platters, which is legit. Their Char platters come with some of the best and juciest grilled chicken I've ever had. They make their own sauce, so just that alone puts their wings above the other restaurants in the neighborhood that just use Franks Red Hot. I have yet to have something on their menu that I didn't like.\n\n\n\n\n\n\n\n\n\n2.4 Trim to the most popular fast food restaurants\nThere’s too many reviews to run a sentiment analysis on all of them in a reasonable time. Let’s trim our reviews dataset to the most popular fast food restaurants, using the list provided below.\nYou will need to get the “business_id” values for each of these restaurants from the restaurants data loaded in part 1.3. Then, trim the reviews data to include reviews only for those business IDs.\n\n\nCode\npopular_fast_food = [\n    \"McDonald's\",\n    \"Wendy's\",\n    \"Subway\",\n    \"Popeyes Louisiana Kitchen\",\n    \"Taco Bell\",\n    \"KFC\",\n    \"Burger King\",\n]\n\n\n\n\nCode\npop_fast_food = tract_restaurant[tract_restaurant['name'].isin(popular_fast_food)]\nselected_reviews = reviews[reviews['business_id'].isin(pop_fast_food['business_id'])]\nselected_reviews.head()\n\n\n\n\n\n\n\n\n\nbusiness_id\nreview_id\nstars\ntext\n\n\n\n\n0\nkgMEBZG6rjkGeFzPaIM4MQ\nE-yGr1OhsUBxNeUVLDVouA\n1\nI know I shouldn't expect much but everything I asked for that was on the drive thru menu was not available. I was actually afraid of what I was going to get once I did get it. I saw the movie \"waiting\". Word of advice stay clear of this arch. Just so you know I was only trying to order a beverage how pathetic is that.\n\n\n8\nPjknD8uD_0tisZQbomiYoQ\n6TqKBa-HDiq2_W_ip2AItA\n5\nI am only giving 5 stars because the Shamrock Shake is back and delicious!! Too bad it's around only once a year ;(\n\n\n13\nkgMEBZG6rjkGeFzPaIM4MQ\nNGaXI03qbtBLshjfJV4pbQ\n3\nDirty bathrooms and very slow service, but I was pleased because they had a TV on with subtitles and the volume on, and it was turned to the news! A good place to pass some time with a tasty Mc-snack and a hot coffee while in Philly during a chilly day! We stopped here on the way to a football game and found it a very pleasant and relaxing place to hang out for a while.\n\n\n17\nLACylKxImNI29DKUQpWuHw\nHHy9yIjW07VHUE6nXVbsVA\n3\nBurger King is an okay alternative to Mcdonalds. 6/10 Would recommend but idk about coming again.\n\n\n21\ngq4zw-ru_rkZ2UBIanaZFQ\nyMZTK5B_0SAdUXSrIkXrmA\n1\nive tried going here four times with no success because the drive tru takes so long that i have pulled away every single time. great job idiots!\n\n\n\n\n\n\n\n\n\n2.5 Run the emotions classifier on fast food reviews\nRun a sentiment analysis on the reviews data from the previous step. Use the DistilBERT model that can predict emotion labels (anger, fear, sadness, joy, love, and surprise). Transform the result from the classifier into a DataFrame so that you have a column for each of the emotion labels.\n\n\nCode\nfrom transformers import pipeline\n\n\n2023-11-11 21:15:57.312117: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\n\nCode\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\nmodel = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n\ninputs = tokenizer(\"Hello, my daog is cute\", return_tensors=\"pt\")\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npredicted_class_id = logits.argmax().item()\nmodel.config.id2label[predicted_class_id]\n\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n'LABEL_0'\n\n\n\n\nCode\n# The model\nmodel = \"bhadresh-savani/distilbert-base-uncased-emotion\"\n\n# Initialize our sentiment analyzer\nemotion_classifier = pipeline(\n    task=\"text-classification\",  # The task we are doing\n    model=model,  # The specific model name\n    top_k=None,  # Predict all labels, not just top ones\n    tokenizer=model,  # Tokenize inputs using model tokenizer\n    truncation=True,  # Truncate text if we need to\n)\n\n\n\n\n\n\n\nCode\nreview = selected_reviews['text'].str.strip().tolist()\n\n\n\n\nCode\n%%time\nscores = emotion_classifier(review)\n\n\nCPU times: user 11min 18s, sys: 19.7 s, total: 11min 38s\nWall time: 2min 56s\n\n\n\n\nCode\nscores[0]\n\n\n[{'label': 'sadness', 'score': 0.7338692545890808},\n {'label': 'fear', 'score': 0.2506760060787201},\n {'label': 'anger', 'score': 0.011038999073207378},\n {'label': 'joy', 'score': 0.0027580487076193094},\n {'label': 'surprise', 'score': 0.0010148979490622878},\n {'label': 'love', 'score': 0.0006427845801226795}]\n\n\n\n\nCode\nemo = pd.DataFrame([{d[\"label\"]: d[\"score\"] for d in dd} for dd in scores]).assign(\n    text=review\n)\nemo.head()\n\n\n\n\n\n\n\n\n\nsadness\nfear\nanger\njoy\nsurprise\nlove\ntext\n\n\n\n\n0\n0.733869\n0.250676\n0.011039\n0.002758\n0.001015\n0.000643\nI know I shouldn't expect much but everything I asked for that was on the drive thru menu was not available. I was actually afraid of what I was going to get once I did get it. I saw the movie \"waiting\". Word of advice stay clear of this arch. Just so you know I was only trying to order a beverage how pathetic is that.\n\n\n1\n0.000230\n0.000126\n0.000165\n0.998759\n0.000246\n0.000475\nI am only giving 5 stars because the Shamrock Shake is back and delicious!! Too bad it's around only once a year ;(\n\n\n2\n0.000216\n0.000088\n0.000153\n0.998563\n0.000161\n0.000819\nDirty bathrooms and very slow service, but I was pleased because they had a TV on with subtitles and the volume on, and it was turned to the news! A good place to pass some time with a tasty Mc-snack and a hot coffee while in Philly during a chilly day! We stopped here on the way to a football game and found it a very pleasant and relaxing place to hang out for a while.\n\n\n3\n0.000838\n0.000403\n0.000811\n0.996928\n0.000140\n0.000880\nBurger King is an okay alternative to Mcdonalds. 6/10 Would recommend but idk about coming again.\n\n\n4\n0.005284\n0.000753\n0.006195\n0.985421\n0.001620\n0.000726\nive tried going here four times with no success because the drive tru takes so long that i have pulled away every single time. great job idiots!\n\n\n\n\n\n\n\n\n\n2.6 Identify the predicted emotion for each text\nUse the pandas idxmax() to identify the predicted emotion for each review, and add this value to a new column called “prediction”\nThe predicted emotion has the highest confidence score across all emotion labels for a particular label.\n\n\nCode\nemo_labels = [\"anger\", \"fear\", \"sadness\", \"joy\", \"surprise\", \"love\"]\nemo['prediction'] = emo[emo_labels].idxmax(axis=1)\nemo.head()\n\n\n\n\n\n\n\n\n\nsadness\nfear\nanger\njoy\nsurprise\nlove\ntext\nprediction\n\n\n\n\n0\n0.733869\n0.250676\n0.011039\n0.002758\n0.001015\n0.000643\nI know I shouldn't expect much but everything I asked for that was on the drive thru menu was not available. I was actually afraid of what I was going to get once I did get it. I saw the movie \"waiting\". Word of advice stay clear of this arch. Just so you know I was only trying to order a beverage how pathetic is that.\nsadness\n\n\n1\n0.000230\n0.000126\n0.000165\n0.998759\n0.000246\n0.000475\nI am only giving 5 stars because the Shamrock Shake is back and delicious!! Too bad it's around only once a year ;(\njoy\n\n\n2\n0.000216\n0.000088\n0.000153\n0.998563\n0.000161\n0.000819\nDirty bathrooms and very slow service, but I was pleased because they had a TV on with subtitles and the volume on, and it was turned to the news! A good place to pass some time with a tasty Mc-snack and a hot coffee while in Philly during a chilly day! We stopped here on the way to a football game and found it a very pleasant and relaxing place to hang out for a while.\njoy\n\n\n3\n0.000838\n0.000403\n0.000811\n0.996928\n0.000140\n0.000880\nBurger King is an okay alternative to Mcdonalds. 6/10 Would recommend but idk about coming again.\njoy\n\n\n4\n0.005284\n0.000753\n0.006195\n0.985421\n0.001620\n0.000726\nive tried going here four times with no success because the drive tru takes so long that i have pulled away every single time. great job idiots!\njoy\n\n\n\n\n\n\n\n\n\n2.7 Combine the ratings and sentiment data\nCombine the data from part 2.4 (reviews data) and part 2.6 (emotion data). Use the pd.concat() function and combine along the column axis.\nNote: You’ll need to reset the index of your reviews data frame so it matches the emotion data index (it should run from 0 to the length of the data - 1).\n\n\nCode\ncombined = pd.concat([selected_reviews.reset_index(), emo], axis=1)\ncombined.head()\n\n\n\n\n\n\n\n\n\nindex\nbusiness_id\nreview_id\nstars\ntext\nsadness\nfear\nanger\njoy\nsurprise\nlove\ntext\nprediction\n\n\n\n\n0\n0\nkgMEBZG6rjkGeFzPaIM4MQ\nE-yGr1OhsUBxNeUVLDVouA\n1\nI know I shouldn't expect much but everything I asked for that was on the drive thru menu was not available. I was actually afraid of what I was going to get once I did get it. I saw the movie \"waiting\". Word of advice stay clear of this arch. Just so you know I was only trying to order a beverage how pathetic is that.\n0.733869\n0.250676\n0.011039\n0.002758\n0.001015\n0.000643\nI know I shouldn't expect much but everything I asked for that was on the drive thru menu was not available. I was actually afraid of what I was going to get once I did get it. I saw the movie \"waiting\". Word of advice stay clear of this arch. Just so you know I was only trying to order a beverage how pathetic is that.\nsadness\n\n\n1\n8\nPjknD8uD_0tisZQbomiYoQ\n6TqKBa-HDiq2_W_ip2AItA\n5\nI am only giving 5 stars because the Shamrock Shake is back and delicious!! Too bad it's around only once a year ;(\n0.000230\n0.000126\n0.000165\n0.998759\n0.000246\n0.000475\nI am only giving 5 stars because the Shamrock Shake is back and delicious!! Too bad it's around only once a year ;(\njoy\n\n\n2\n13\nkgMEBZG6rjkGeFzPaIM4MQ\nNGaXI03qbtBLshjfJV4pbQ\n3\nDirty bathrooms and very slow service, but I was pleased because they had a TV on with subtitles and the volume on, and it was turned to the news! A good place to pass some time with a tasty Mc-snack and a hot coffee while in Philly during a chilly day! We stopped here on the way to a football game and found it a very pleasant and relaxing place to hang out for a while.\n0.000216\n0.000088\n0.000153\n0.998563\n0.000161\n0.000819\nDirty bathrooms and very slow service, but I was pleased because they had a TV on with subtitles and the volume on, and it was turned to the news! A good place to pass some time with a tasty Mc-snack and a hot coffee while in Philly during a chilly day! We stopped here on the way to a football game and found it a very pleasant and relaxing place to hang out for a while.\njoy\n\n\n3\n17\nLACylKxImNI29DKUQpWuHw\nHHy9yIjW07VHUE6nXVbsVA\n3\nBurger King is an okay alternative to Mcdonalds. 6/10 Would recommend but idk about coming again.\n0.000838\n0.000403\n0.000811\n0.996928\n0.000140\n0.000880\nBurger King is an okay alternative to Mcdonalds. 6/10 Would recommend but idk about coming again.\njoy\n\n\n4\n21\ngq4zw-ru_rkZ2UBIanaZFQ\nyMZTK5B_0SAdUXSrIkXrmA\n1\nive tried going here four times with no success because the drive tru takes so long that i have pulled away every single time. great job idiots!\n0.005284\n0.000753\n0.006195\n0.985421\n0.001620\n0.000726\nive tried going here four times with no success because the drive tru takes so long that i have pulled away every single time. great job idiots!\njoy\n\n\n\n\n\n\n\n\n\n2.8 Plot sentiment vs. stars\nWe now have a dataframe with the predicted primary emotion for each review and the associated number of stars for each review. Let’s explore two questions:\n\nDoes sentiment analysis work? Do reviews with fewer stars have negative emotions?\nFor our fast food restaurants, are reviews generally positive or negative?\n\nUse seaborn’s histplot() to make a stacked bar chart showing the breakdown of each emotion for each stars category (1 star, 2 stars, etc.). A few notes:\n\nTo stack multiple emotion labels in one bar, use the multiple=\"stack\" keyword\nThe discrete=True can be helpful to tell seaborn our stars values are discrete categories\n\n\n\nCode\nsns.set_theme()\nsns.histplot(data=combined, x=\"stars\", hue=\"prediction\", multiple=\"stack\", discrete=True)\n\n\n&lt;Axes: xlabel='stars', ylabel='Count'&gt;\n\n\n\n\n\nQuestion: What does your chart indicate for the effectiveness of our sentiment analysis? Does our original hypothesis about fast food restaurants seem plausible?\nThe chart indicates two important pieces of information – 1) disproportionally large number of reviews on fast food restaurants are 1-star; 2) based on results from sentiment analysis, highly rated reviews usually reflects joy, while 1-star reviews largely reflects sadness and anger, despite a small portion reflecting joy."
  },
  {
    "objectID": "analysis/assignment-6.html",
    "href": "analysis/assignment-6.html",
    "title": "Assignment 6: Predictive Modeling of Housing Prices in Philadelphia",
    "section": "",
    "text": "Due date: Wednesday, 12/6 by the end of the day\nLectures 12B and 13A will cover predictive modeling of housing prices in Philadelphia. We’ll extend that analysis in this section by:"
  },
  {
    "objectID": "analysis/assignment-6.html#part-2-modeling-philadelphias-housing-prices-and-algorithmic-fairness",
    "href": "analysis/assignment-6.html#part-2-modeling-philadelphias-housing-prices-and-algorithmic-fairness",
    "title": "Assignment 6: Predictive Modeling of Housing Prices in Philadelphia",
    "section": "Part 2: Modeling Philadelphia’s Housing Prices and Algorithmic Fairness",
    "text": "Part 2: Modeling Philadelphia’s Housing Prices and Algorithmic Fairness\n\n2.1 Load data from the Office of Property Assessment\nUse the requests package to query the CARTO API for single-family property assessment data in Philadelphia for properties that had their last sale during 2022.\nSources: - OpenDataPhilly - Metadata\n\nimport requests\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\n\n\ncarto_url = \"https://phl.carto.com/api/v2/sql\"\nwhere = \"sale_date &gt;= '2022-01-01' and sale_date &lt;= '2022-12-31'\"\nwhere = where + \" and category_code_description IN ('SINGLE FAMILY', 'Single Family')\"\n\n\n# Create the query\nquery = f\"SELECT * FROM opa_properties_public WHERE {where}\"\n\n# Make the request\nparams = {\"q\": query, \"format\": \"geojson\", \"where\": where}\nresponse = requests.get(carto_url, params=params)\n\n# Make the GeoDataFrame\nsales2022 = gpd.GeoDataFrame.from_features(response.json(), crs=\"EPSG:4326\")\n\n\n\n2.2 Load data for census tracts and neighborhoods\nLoad various Philadelphia-based regions that we will use in our analysis.\n\nCensus tracts can be downloaded from: https://opendata.arcgis.com/datasets/8bc0786524a4486bb3cf0f9862ad0fbf_0.geojson\nNeighborhoods can be downloaded from: https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson\n\n\ntracts = gpd.read_file('2010_Tracts.geojson')\n\n\nurl = 'https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson'\nneighbor = gpd.read_file(url).to_crs('EPSG:4326')\n\n\n\n2.3 Spatially join the sales data and neighborhoods/census tracts.\nPerform a spatial join, such that each sale has an associated neighborhood and census tract.\nNote: After performing the first spatial join, you will need to use the drop() function to remove the index_right column; otherwise an error will be raised on the second spatial join about duplicate columns.\n\njoined = gpd.sjoin(sales2022, tracts, predicate=\"within\", how=\"left\",)\njoined = gpd.sjoin(joined.drop('index_right', axis=1), neighbor, predicate = \"within\", how = \"left\")\n\n\n\n2.4 Train a Random Forest on the sales data\nIn this step, you should follow the steps outlined in lecture to preprocess and train your model. We’ll extend our analysis to do a hyperparameter grid search to find the best model configuration. As you train your model, follow the following steps:\nPreprocessing Requirements - Trim the sales data to those sales with prices between $3,000 and $1 million - Set up a pipeline that includes both numerical columns and categorical columns - Include one-hot encoded variable for the neighborhood of the sale, instead of ZIP code. We don’t want to include multiple location based categories, since they encode much of the same information.\nTraining requirements - Use a 70/30% training/test split and predict the log of the sales price. - Use GridSearchCV to perform a k-fold cross validation that optimize at least 2 hyperparameters of the RandomForestRegressor - After fitting your model and finding the optimal hyperparameters, you should evaluate the score (R-squared) on the test set (the original 30% sample withheld)\nNote: You don’t need to include additional features (such as spatial distance features) or perform any extra feature engineering beyond what is required above to receive full credit. Of course, you are always welcome to experiment!\n\n# model\nfrom sklearn.ensemble import RandomForestRegressor\n\n# pipeline\nfrom sklearn.pipeline import make_pipeline\n\n# model selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\n\n\n# trim data to between $3000 and $1 million\nvalid = (joined['sale_price'] &gt; 3000) & (joined['sale_price'] &lt; 1e6)\njoined = joined.loc[valid]\n\n\npd.set_option('display.max_columns', None)\njoined.head()\n\n\n\n\n\n\n\n\ngeometry\ncartodb_id_left\nassessment_date\nbasements\nbeginning_point\nbook_and_page\nbuilding_code\nbuilding_code_description\ncategory_code\ncategory_code_description\ncensus_tract\ncentral_air\ncross_reference\ndate_exterior_condition\ndepth\nexempt_building\nexempt_land\nexterior_condition\nfireplaces\nfrontage\nfuel\ngarage_spaces\ngarage_type\ngeneral_construction\ngeographic_ward\nhomestead_exemption\nhouse_extension\nhouse_number\ninterior_condition\nlocation\nmailing_address_1\nmailing_address_2\nmailing_care_of\nmailing_city_state\nmailing_street\nmailing_zip\nmarket_value\nmarket_value_date\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_of_rooms\nnumber_stories\noff_street_open\nother_building\nowner_1\nowner_2\nparcel_number\nparcel_shape\nquality_grade\nrecording_date\nregistry_number\nsale_date\nsale_price\nseparate_utilities\nsewer\nsite_type\nstate_code\nstreet_code\nstreet_designation\nstreet_direction\nstreet_name\nsuffix\ntaxable_building\ntaxable_land\ntopography\ntotal_area\ntotal_livable_area\ntype_heater\nunfinished\nunit\nutility\nview_type\nyear_built\nyear_built_estimate\nzip_code\nzoning\npin\nbuilding_code_new\nbuilding_code_description_new\nobjectid\nOBJECTID\nSTATEFP10\nCOUNTYFP10\nTRACTCE10\nGEOID10\nNAME10\nNAMELSAD10\nMTFCC10\nFUNCSTAT10\nALAND10\nAWATER10\nINTPTLAT10\nINTPTLON10\nLOGRECNO\nindex_right\nname\nlistname\nmapname\nshape_leng\nshape_area\ncartodb_id_right\ncreated_at\nupdated_at\n\n\n\n\n0\nPOINT (-75.18212 39.97558)\n1699\n2022-05-24T00:00:00Z\nNone\n139'10 15/16\" N STILES\n54234223\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n137\nNone\nNone\nNone\n36.0\n0.0\n0.0\n4\n0.0\n13.0\nNone\n0.0\nNone\nA\n29\n0\nNone\n1245\n4\n1245 N NEWKIRK ST\nCSC INGEO\nNone\nNone\nPHILADELPHIA PA\n1245 N NEWKIRK ST\n19121-4526\n131100\nNone\n1.0\n2.0\nNaN\n2.0\n1125.0\nNone\nAUTUMN CAPITAL GROUP INC\nNone\n292108701\nE\nC\n2023-10-17T00:00:00Z\n010N010260\n2022-10-24T00:00:00Z\n213000\nNone\nNone\nNone\nPA\n59640\nST\nN\nNEWKIRK\nNone\n104880.0\n26220.0\nF\n468.0\n650.0\nNone\nNone\nNone\nNone\nI\n1920\nY\n19121\nRSA5\n1001388772\n22\nROW TYPICAL\n401673742\n238.0\n42\n101\n013700\n42101013700\n137\nCensus Tract 137\nG5020\nS\n589738.0\n0.0\n+39.9772752\n-075.1842323\n10467\n100.0\nBREWERYTOWN\nBrewerytown\nBrewerytown\n14790.673736\n1.125654e+07\n80.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n2\nPOINT (-75.16524 40.00137)\n2261\n2022-05-24T00:00:00Z\nD\n120' W 21ST ST\n54222844\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n173\nN\nNone\nNone\n79.0\n0.0\n0.0\n4\n0.0\n20.0\nNone\n0.0\nNone\nF\n11\n0\nNone\n2116\n3\n2116 W CLEARFIELD ST\nNone\nNone\nNone\nPHILADELPHIA PA\n2116 W CLEARFIELD ST\n19132-1517\n139700\nNone\n1.0\n4.0\nNaN\n2.0\n929.0\nNone\nMILLIONIQUE LLC\nNone\n111112200\nE\nC\n2023-09-13T00:00:00Z\n38N5 289\n2022-03-03T00:00:00Z\n10000\nNone\nNone\nNone\nPA\n23640\nST\nW\nCLEARFIELD\nNone\n111760.0\n27940.0\nF\n1583.0\n1960.0\nH\nNone\nNone\nNone\nI\n1925\nY\n19132\nRSA5\n1001149040\n22\nROW TYPICAL\n401675592\n241.0\n42\n101\n017300\n42101017300\n173\nCensus Tract 173\nG5020\nS\n874586.0\n0.0\n+39.9985707\n-075.1609337\n10503\n12.0\nGLENWOOD\nGlenwood\nGlenwood\n12350.882116\n9.652808e+06\n65.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n4\nPOINT (-75.14075 39.97514)\n2645\n2023-05-11T00:00:00Z\nA\nNWC BODINE TO NEC\n54224335\nSR\nVACANT LAND RESIDE &lt; ACRE\n1\nSINGLE FAMILY\n156\nY\nNone\nNone\n61.0\n0.0\n0.0\n1\n0.0\n21.0\nA\n1.0\n1\nB\n18\n0\nNone\n249\n1\n249 W OXFORD ST\nSIMPLIFILE LC E-RECORDING\nNone\nNone\nPHILADELPHIA PA\n249 W OXFORD ST # A\n19122-3742\n500000\nNone\nNaN\n3.0\nNaN\n3.0\nNaN\nNone\nNone\nNone\n182000012\nE\nNone\n2023-09-18T00:00:00Z\n012N230250\n2022-03-29T00:00:00Z\n192000\nNone\n2\nNone\nPA\n62120\nST\nW\nOXFORD\nNone\n400000.0\n100000.0\nF\n1294.0\n2199.0\nA\nNone\nA\nNone\nI\n2021\nNone\n19122\nRSA5\n1001681249\n25\nROW MODERN\n401675399\n18.0\n42\n101\n015600\n42101015600\n156\nCensus Tract 156\nG5020\nS\n429567.0\n0.0\n+39.9790066\n-075.1418910\n10484\n87.0\nOLD_KENSINGTON\nKensington, Old\nOld Kensington\n10913.733421\n7.476740e+06\n88.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n6\nPOINT (-75.13389 40.03928)\n2839\n2022-05-24T00:00:00Z\nH\n241' N OF CHEW ST\n54230133\nR30\nROW B/GAR 2 STY MASONRY\n1\nSINGLE FAMILY\n275\nN\nNone\nNone\n95.0\n0.0\n0.0\n7\n0.0\n15.0\nNone\n1.0\nNone\nB\n61\n0\nNone\n5732\n4\n5732 N 7TH ST\nWALKER MICHAEL\nNone\nNone\nSICKLERVILLE NJ\n44 FARMHOUSE RD\n08081\n133400\nNone\n1.0\n3.0\nNaN\n2.0\n1920.0\nNone\nWALKER MICHAEL\nNone\n612234600\nE\nC\n2023-10-04T00:00:00Z\n135N7 61\n2022-08-21T00:00:00Z\n21000\nNone\nNone\nNone\nNJ\n87930\nST\nN\n7TH\nNone\n106720.0\n26680.0\nF\n1425.0\n1164.0\nH\nNone\nNone\nNone\nI\n1925\nY\n19120\nRSA5\n1001602509\n24\nROW PORCH FRONT\n401674376\n350.0\n42\n101\n027500\n42101027500\n275\nCensus Tract 275\nG5020\nS\n606825.0\n0.0\n+40.0400497\n-075.1322707\n10588\n42.0\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n7\nPOINT (-75.14337 40.00957)\n2890\n2022-05-24T00:00:00Z\nD\n415' N OF ERIE AVE\n54230032\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n198\nN\nNone\nNone\n45.0\n0.0\n0.0\n4\n0.0\n16.0\nNone\n0.0\nNone\nA\n43\n0\nNone\n3753\n4\n3753 N DELHI ST\nNone\nNone\nNone\nDELRAY BEACH FL\n4899 NW 6TH STREET\n33445\n73800\nNone\n1.0\n3.0\nNaN\n2.0\n1683.0\nNone\nRJ SIMPLE SOLUTION LLC\nNone\n432345900\nE\nC\n2023-10-04T00:00:00Z\n100N040379\n2022-06-13T00:00:00Z\n35000\nNone\nNone\nNone\nFL\n28040\nST\nN\nDELHI\nNone\n59040.0\n14760.0\nF\n720.0\n960.0\nH\nNone\nNone\nNone\nI\n1942\nY\n19140\nRM1\n1001175031\n24\nROW PORCH FRONT\n401674385\n23.0\n42\n101\n019800\n42101019800\n198\nCensus Tract 198\nG5020\nS\n541006.0\n0.0\n+40.0107245\n-075.1421472\n10523\n98.0\nHUNTING_PARK\nHunting Park\nHunting Park\n32920.799360\n3.902450e+07\n73.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n\n\n\n\n\n\n\n\n#categorical\ncat_cols = ['exterior_condition', 'listname']\n \n# numeric\nnum_cols = ['fireplaces', 'garage_spaces', 'number_of_bathrooms', 'number_of_bedrooms', 'number_stories', 'total_area', 'total_livable_area']\n\nfeatures = cat_cols + num_cols + ['sale_price']\n\n\njoined_trimmed = joined[features].dropna()\njoined_trimmed\n\n\n\n\n\n\n\n\nexterior_condition\ninterior_condition\nquality_grade\nparcel_shape\nlistname\nview_type\nyear_built\nfireplaces\ngarage_spaces\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_stories\ntotal_area\ntotal_livable_area\nsale_price\n\n\n\n\n0\n4\n4\nC\nE\nBrewerytown\nI\n1920\n0.0\n0.0\n1.0\n2.0\n2.0\n468.0\n650.0\n213000\n\n\n2\n4\n3\nC\nE\nGlenwood\nI\n1925\n0.0\n0.0\n1.0\n4.0\n2.0\n1583.0\n1960.0\n10000\n\n\n6\n7\n4\nC\nE\nOlney\nI\n1925\n0.0\n1.0\n1.0\n3.0\n2.0\n1425.0\n1164.0\n21000\n\n\n7\n4\n4\nC\nE\nHunting Park\nI\n1942\n0.0\n0.0\n1.0\n3.0\n2.0\n720.0\n960.0\n35000\n\n\n9\n4\n4\nC\nE\nHarrowgate\nI\n1920\n0.0\n0.0\n1.0\n3.0\n1.0\n602.0\n840.0\n39600\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n24472\n4\n4\nC\nE\nHolmesburg\nI\n1950\n0.0\n1.0\n1.0\n3.0\n2.0\n1147.0\n1034.0\n163000\n\n\n24473\n4\n4\nC+\nE\nMayfair\nH\n1940\n0.0\n0.0\n1.0\n3.0\n2.0\n1600.0\n1360.0\n139900\n\n\n24474\n4\n4\nC\nE\nOxford Circle\nI\n1950\n0.0\n1.0\n1.0\n3.0\n1.0\n2813.0\n1656.0\n280000\n\n\n24477\n4\n3\nC\nE\nMayfair\nA\n1950\n0.0\n1.0\n2.0\n3.0\n1.0\n1424.0\n1292.0\n280000\n\n\n24479\n4\n4\nC\nE\nWissinoming\nI\n1920\n0.0\n0.0\n1.0\n3.0\n2.0\n1109.0\n1076.0\n150000\n\n\n\n\n17430 rows × 15 columns\n\n\n\n\njoined_trimmed['sale_price'] = pd.Series(joined_trimmed['sale_price'], dtype='int32')\n\n\njoined_trimmed[\"log_price\"] = np.log(joined_trimmed[\"sale_price\"])\n\n\njoined_trimmed\n\n\n\n\n\n\n\n\nexterior_condition\ninterior_condition\nquality_grade\nparcel_shape\nlistname\nfireplaces\ngarage_spaces\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_stories\ntotal_area\ntotal_livable_area\nsale_price\nlog_price\n\n\n\n\n0\n4\n4\nC\nE\nHunting Park\n0.0\n0.0\n1.0\n3.0\n2.0\n720.0\n960.0\n35000\n10.463103\n\n\n1\n7\n4\nC\nE\nOlney\n0.0\n1.0\n1.0\n3.0\n2.0\n1425.0\n1164.0\n21000\n9.952278\n\n\n6\n4\n3\nC\nE\nGlenwood\n0.0\n0.0\n1.0\n4.0\n2.0\n1583.0\n1960.0\n10000\n9.210340\n\n\n7\n4\n4\nC+\nB\nChestnut Hill\n0.0\n0.0\n1.0\n4.0\n2.0\n2457.0\n2110.0\n350000\n12.765688\n\n\n8\n4\n4\nC\nE\nWest Oak Lane\n0.0\n0.0\n1.0\n3.0\n2.0\n1161.0\n1120.0\n70000\n11.156251\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n24432\n2\n2\nC\nE\nWhitman\n0.0\n0.0\n1.0\n3.0\n2.0\n736.0\n951.0\n150000\n11.918391\n\n\n24434\n4\n4\nC+\nE\nMayfair\n0.0\n0.0\n1.0\n3.0\n2.0\n1600.0\n1360.0\n139900\n11.848683\n\n\n24436\n4\n4\nC\nE\nStadium District\n0.0\n0.0\n1.0\n3.0\n2.0\n1053.0\n960.0\n190000\n12.154779\n\n\n24440\n4\n4\nC\nE\nRichmond\n0.0\n0.0\n1.0\n3.0\n2.0\n741.0\n1020.0\n46000\n10.736397\n\n\n24442\n4\n4\nC\nE\nFox Chase\n0.0\n1.0\n0.0\n0.0\n2.0\n9000.0\n1388.0\n150000\n11.918391\n\n\n\n\n17461 rows × 14 columns\n\n\n\n\n# 70/30 training / test split \ntrain_set, test_set = train_test_split(joined_trimmed, test_size=0.3, random_state=42)\n\n# the target labels\ny_train = train_set[\"log_price\"]\ny_test = test_set[\"log_price\"]\n\n# features\nfeature_cols = cat_cols + num_cols\nX_train = train_set[feature_cols]\nX_test = test_set[feature_cols]\n\n\n# column transformers + pipeline for random forest\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder()\n\ntransformer = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), num_cols),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n    ]\n)\n\npipe = make_pipeline(transformer, RandomForestRegressor(n_estimators=100,\n                                       random_state=42)\n)\n\n\n# Fit the training set\npipe.fit(X_train, y_train);\n\n\n# Test Score\npipe.score(X_test, y_test)\n\n0.5646447325116657\n\n\n\n# Setup for GridSearchCV\n\nmodel_step = \"randomforestregressor\"\nparam_grid = {\n    f\"{model_step}__n_estimators\": [5, 10, 15, 20, 30, 50, 100, 200],\n    f\"{model_step}__max_depth\": [9, 13, 21, 33, 51],\n}\n\n\n\n# Create the grid and use 3-fold CV\ngrid = GridSearchCV(pipe, param_grid, cv=3, verbose=1)\n\n# Run the search\ngrid.fit(X_train, y_train)\n\n\nFitting 3 folds for each of 40 candidates, totalling 120 fits\n\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['fireplaces',\n                                                                          'garage_spaces',\n                                                                          'number_of_bathrooms',\n                                                                          'number_of_bedrooms',\n                                                                          'number_stories',\n                                                                          'total_area',\n                                                                          'total_livable_area']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['exterior_condition',\n                                                                          'listname'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid={'randomforestregressor__max_depth': [9, 13, 21, 33,\n                                                              51],\n                         'randomforestregressor__n_estimators': [5, 10, 15, 20,\n                                                                 30, 50, 100,\n                                                                 200]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['fireplaces',\n                                                                          'garage_spaces',\n                                                                          'number_of_bathrooms',\n                                                                          'number_of_bedrooms',\n                                                                          'number_stories',\n                                                                          'total_area',\n                                                                          'total_livable_area']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['exterior_condition',\n                                                                          'listname'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid={'randomforestregressor__max_depth': [9, 13, 21, 33,\n                                                              51],\n                         'randomforestregressor__n_estimators': [5, 10, 15, 20,\n                                                                 30, 50, 100,\n                                                                 200]},\n             verbose=1)estimator: PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['fireplaces',\n                                                   'garage_spaces',\n                                                   'number_of_bathrooms',\n                                                   'number_of_bedrooms',\n                                                   'number_stories',\n                                                   'total_area',\n                                                   'total_livable_area']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['exterior_condition',\n                                                   'listname'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=42))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['fireplaces', 'garage_spaces',\n                                  'number_of_bathrooms', 'number_of_bedrooms',\n                                  'number_stories', 'total_area',\n                                  'total_livable_area']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['exterior_condition', 'listname'])])num['fireplaces', 'garage_spaces', 'number_of_bathrooms', 'number_of_bedrooms', 'number_stories', 'total_area', 'total_livable_area']StandardScalerStandardScaler()cat['exterior_condition', 'listname']OneHotEncoderOneHotEncoder(handle_unknown='ignore')RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\n# The best estimator\ngrid.best_estimator_\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['fireplaces',\n                                                   'garage_spaces',\n                                                   'number_of_bathrooms',\n                                                   'number_of_bedrooms',\n                                                   'number_stories',\n                                                   'total_area',\n                                                   'total_livable_area']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['exterior_condition',\n                                                   'listname'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(max_depth=33, n_estimators=200,\n                                       random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['fireplaces',\n                                                   'garage_spaces',\n                                                   'number_of_bathrooms',\n                                                   'number_of_bedrooms',\n                                                   'number_stories',\n                                                   'total_area',\n                                                   'total_livable_area']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['exterior_condition',\n                                                   'listname'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(max_depth=33, n_estimators=200,\n                                       random_state=42))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['fireplaces', 'garage_spaces',\n                                  'number_of_bathrooms', 'number_of_bedrooms',\n                                  'number_stories', 'total_area',\n                                  'total_livable_area']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['exterior_condition', 'listname'])])num['fireplaces', 'garage_spaces', 'number_of_bathrooms', 'number_of_bedrooms', 'number_stories', 'total_area', 'total_livable_area']StandardScalerStandardScaler()cat['exterior_condition', 'listname']OneHotEncoderOneHotEncoder(handle_unknown='ignore')RandomForestRegressorRandomForestRegressor(max_depth=33, n_estimators=200, random_state=42)\n\n\n\nnew_pipe = make_pipeline(transformer, RandomForestRegressor(max_depth=33, n_estimators=200,\n                                       random_state=42))\n\n\n# Fit the training set\nnew_pipe.fit(train_set, y_train);\n\n\n# Test Score\nnew_pipe.score(test_set, y_test)\n\n0.5675150349542478\n\n\n\n\n2.5 Calculate the percent error of your model predictions for each sale in the test set\nFit your best model and use it to make predictions on the test set.\nNote: This should be the percent error in terms of sale price. You’ll need to convert if your model predicted the log of sales price!\n\n# this step is not necessary, just curious what the MAPE is. \n# X_test = test_set[feature_cols]\n# y_test = test_set[\"log_price\"].values\n# y_test_no_log = np.exp(y_test)\n\ndef evaluate_mape(model, X_test, y_test):\n    \"\"\"\n    Given a model and test features/targets, print out the \n    mean absolute error and accuracy\n    \"\"\"\n    # Make the predictions\n    predictions = model.predict(X_test)\n\n    # Absolute Percentage error\n    abs_errors = abs(np.exp(predictions) - y_test)\n    avg_error = np.mean(abs_errors)\n\n    # Mean absolute percentage error\n    mape = 100 * np.mean(abs_errors / y_test)\n\n\n    # Accuracy\n    accuracy = 100 - mape\n\n    print(\"Model Performance\")\n    print(f\"Average Absolute Error: {avg_error:0.4f}\")\n    print(f\"Accuracy = {accuracy:0.2f}%.\")\n\n    return accuracy\n\n\ny_test_no_log = np.exp(y_test)\n\n\nbase_accuracy = evaluate_mape(pipe, X_test, y_test_no_log)\n\nModel Performance\nAverage Absolute Error: 69104.9786\nAccuracy = 51.06%.\n\n\n\n# This is the actual percent error calculation part \n\ndef percent_error(model, X_test, y_test):\n    \n    predictions = np.exp(model.predict(X_test))\n    \n    #percentage error\n    p_errors = (predictions - y_test) / y_test\n    \n    return p_errors\n\n\np_errors = percent_error(pipe, X_test, y_test_no_log)\n\n\npipe.predict(X_test)\n\narray([10.79687142, 12.01202048, 12.44467831, ..., 11.82508308,\n       11.50655521, 12.76111153])\n\n\n\ny_test\n\n8682     11.245046\n45       11.336188\n6987     11.580584\n16121    12.449019\n8673     11.695247\n           ...    \n2252     12.254863\n13927    13.422468\n17434    11.695247\n12885    11.289782\n3753     13.060488\nName: log_price, Length: 5230, dtype: float64\n\n\n\np_errors\n\n7209     0.095504\n52      -0.068308\n10900    3.547680\n22227   -0.448610\n18079    8.437723\n           ...   \n12894   -0.101753\n15565   -0.107182\n21955   -0.495361\n3723    -0.141809\n18913    0.245689\nName: log_price, Length: 5229, dtype: float64\n\n\n\n\n2.6 Make a data frame with percent errors and census tract info for each sale in the test set\nCreate a data frame that has the property geometries, census tract data, and percent errors for all of the sales in the test set.\nNotes\n\nWhen using the “train_test_split()” function, the index of the test data frame includes the labels from the original sales data frame\nYou can use this index to slice out the test data from the original sales data frame, which should include the census tract info and geometries\nAdd a new column to this data frame holding the percent error data\nMake sure to use the percent error and not the absolute percent error\n\n\ntest_df = joined.loc[test_set.index]\n\n\ntest_df['percent_error'] = p_errors\n\n\n\n2.8 Plot a map of the median percent error by census tract\n\nYou’ll want to group your data frame of test sales by the GEOID10 column and take the median of you percent error column\nMerge the census tract geometries back in and use geopandas to plot.\n\n\nmedian_p_errors = test_df.groupby('GEOID10')['percent_error'].median()\n\n\nmedian_p_errors = pd.DataFrame(median_p_errors).reset_index()\n\n\ngeo_median = tracts.merge(median_p_errors, on='GEOID10')\n\n\ngeo_median.explore(\n    column=\"percent_error\",\n    tooltip=\"percent_error\",\n    popup=True,\n    cmap=\"coolwarm\",\n    tiles=\"CartoDB positron\",\n    legend=True\n    )\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n2.9 Compare the percent errors in Qualifying Census Tracts and other tracts\nQualifying Census Tracts are a poverty designation that HUD uses to allocate housing tax credits\n\nI’ve included a list of the census tract names that qualify in Philadelphia\nAdd a new column to your dataframe of test set sales that is True/False depending on if the tract is a QCT\nThen, group by this new column and calculate the median percent error\n\nYou should find that the algorithm’s accuracy is significantly worse in these low-income, qualifying census tracts\n\nqct = ['5',\n '20',\n '22',\n '28.01',\n '30.01',\n '30.02',\n '31',\n '32',\n '33',\n '36',\n '37.01',\n '37.02',\n '39.01',\n '41.01',\n '41.02',\n '56',\n '60',\n '61',\n '62',\n '63',\n '64',\n '65',\n '66',\n '67',\n '69',\n '70',\n '71.01',\n '71.02',\n '72',\n '73',\n '74',\n '77',\n '78',\n '80',\n '81.01',\n '81.02',\n '82',\n '83.01',\n '83.02',\n '84',\n '85',\n '86.01',\n '86.02',\n '87.01',\n '87.02',\n '88.01',\n '88.02',\n '90',\n '91',\n '92',\n '93',\n '94',\n '95',\n '96',\n '98.01',\n '100',\n '101',\n '102',\n '103',\n '104',\n '105',\n '106',\n '107',\n '108',\n '109',\n '110',\n '111',\n '112',\n '113',\n '119',\n '121',\n '122.01',\n '122.03',\n '131',\n '132',\n '137',\n '138',\n '139',\n '140',\n '141',\n '144',\n '145',\n '146',\n '147',\n '148',\n '149',\n '151.01',\n '151.02',\n '152',\n '153',\n '156',\n '157',\n '161',\n '162',\n '163',\n '164',\n '165',\n '167.01',\n '167.02',\n '168',\n '169.01',\n '169.02',\n '170',\n '171',\n '172.01',\n '172.02',\n '173',\n '174',\n '175',\n '176.01',\n '176.02',\n '177.01',\n '177.02',\n '178',\n '179',\n '180.02',\n '188',\n '190',\n '191',\n '192',\n '195.01',\n '195.02',\n '197',\n '198',\n '199',\n '200',\n '201.01',\n '201.02',\n '202',\n '203',\n '204',\n '205',\n '206',\n '208',\n '239',\n '240',\n '241',\n '242',\n '243',\n '244',\n '245',\n '246',\n '247',\n '249',\n '252',\n '253',\n '265',\n '267',\n '268',\n '271',\n '274.01',\n '274.02',\n '275',\n '276',\n '277',\n '278',\n '279.01',\n '279.02',\n '280',\n '281',\n '282',\n '283',\n '284',\n '285',\n '286',\n '287',\n '288',\n '289.01',\n '289.02',\n '290',\n '291',\n '293',\n '294',\n '298',\n '299',\n '300',\n '301',\n '302',\n '305.01',\n '305.02',\n '309',\n '311.01',\n '312',\n '313',\n '314.01',\n '314.02',\n '316',\n '318',\n '319',\n '321',\n '325',\n '329',\n '330',\n '337.01',\n '345.01',\n '357.01',\n '376',\n '377',\n '380',\n '381',\n '382',\n '383',\n '389',\n '390']\n\n\ngeo_median['qct'] = geo_median['NAME10'].isin(qct)\n\ngeo_median.groupby('qct')['percent_error'].median()\n\nqct\nFalse   -0.059781\nTrue     0.039906\nName: percent_error, dtype: float64"
  },
  {
    "objectID": "analysis/Final-1.html",
    "href": "analysis/Final-1.html",
    "title": "Part I - Arts & Culture in New York & London",
    "section": "",
    "text": "The first part of this project aims to provide a city-level overview of art and cultural infrastructure in two of the largest cities in the world – New York City and London. Besides thriving arts and cultural scene in the two cities, another reason for choosing them is their effort in creating programs that support local artists and dedicate urban space for producing, performing, and displaying creative work. From basic spatial distribution to equity in access to arts and cultural infrastructure, this section aims to highlight cultural hubs and show the potential discriminations lower income communities are facing. Art has historically been a bougie and privileged field, but it doesn’t have to be anymore.\nAll data in this section is acquired through NYC and London’s city data portal.\nCode\nimport pandas as pd\nimport numpy as np\nimport altair as alt\nimport geopandas as gpd\nimport hvplot.pandas\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport datetime\nimport math\n\n%matplotlib inline\n\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots"
  },
  {
    "objectID": "analysis/Final-1.html#load-the-data",
    "href": "analysis/Final-1.html#load-the-data",
    "title": "Part I - Arts & Culture in New York",
    "section": "",
    "text": "I’ve already downloaded the relevant data file and put in the data/ folder. Let’s load it using pandas.\nNote: Be sure to use a relative file path to make it easier to load your data when grading. See this guide for more info.\n\n\nCode\nimport pandas as pd\n\n\n\n\nCode\nimport numpy as np\n\n\n\n\nCode\nzip_df = pd.read_csv(\"data/Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")"
  },
  {
    "objectID": "analysis/Final-1.html#trim-the-data-to-just-philadelphia",
    "href": "analysis/Final-1.html#trim-the-data-to-just-philadelphia",
    "title": "Part I - Arts & Culture in New York",
    "section": "2. Trim the data to just Philadelphia",
    "text": "2. Trim the data to just Philadelphia\nSelect the subset of the dataframe for Philadelphia, PA.\n\n\nCode\n# or use isin to trim data\n\nstate_trim = zip_df[\"StateName\"].isin([\"PA\"])\nstate_trim_df= zip_df.loc[state_trim]\n\nphilly_df = state_trim_df.loc[state_trim_df[\"City\"] == \"Philadelphia\"]\nphilly_df.head()\n\n\n\n\n\n\n\n\n\nRegionID\nSizeRank\nRegionName\nRegionType\nStateName\nState\nCity\nMetro\nCountyName\n2000-01-31\n...\n2021-10-31\n2021-11-30\n2021-12-31\n2022-01-31\n2022-02-28\n2022-03-31\n2022-04-30\n2022-05-31\n2022-06-30\n2022-07-31\n\n\n\n\n125\n65810\n126\n19143\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n60701.0\n...\n173114.0\n172087.0\n171445.0\n171542.0\n171680.0\n171878.0\n171607.0\n171333.0\n171771.0\n172611.0\n\n\n247\n65779\n249\n19111\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n85062.0\n...\n257911.0\n260104.0\n262257.0\n263715.0\n264809.0\n265684.0\n267222.0\n269460.0\n272201.0\n274446.0\n\n\n338\n65791\n340\n19124\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n47155.0\n...\n156225.0\n157780.0\n159029.0\n159274.0\n159886.0\n160780.0\n161929.0\n163625.0\n165020.0\n166009.0\n\n\n423\n65787\n426\n19120\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n59285.0\n...\n161167.0\n161807.0\n162634.0\n162972.0\n163597.0\n164008.0\n164887.0\n165860.0\n167321.0\n168524.0\n\n\n509\n65772\n512\n19104\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n74255.0\n...\n220270.0\n221454.0\n222006.0\n220760.0\n217933.0\n216447.0\n216424.0\n218663.0\n220453.0\n223443.0\n\n\n\n\n5 rows × 280 columns"
  },
  {
    "objectID": "analysis/Final-1.html#melt-the-data-into-tidy-format",
    "href": "analysis/Final-1.html#melt-the-data-into-tidy-format",
    "title": "Part I - Arts & Culture in New York",
    "section": "3. Melt the data into tidy format",
    "text": "3. Melt the data into tidy format\nLet’s transform the data from wide to tidy using the pd.melt() function. Create a new column in your data called “ZHVI” that holds the ZHVI values.\n\n\nCode\ndef looks_like_a_date(col):\n    \"\"\"A function that tests if a string starts with '20'\"\"\"\n    \n    return col.startswith(\"20\")\n\n\n\n\nCode\nphilly_tidy = philly_df.melt(\n    id_vars = [\"RegionName\",\"RegionID\"],\n    value_vars = list(filter(looks_like_a_date, philly_df.columns)),\n    var_name = \"Date\",\n    value_name = \"ZHVI\",\n)\n    \n\n\n\n\nCode\nphilly_tidy\n\n\n\n\n\n\n\n\n\nRegionName\nRegionID\nDate\nZHVI\n\n\n\n\n0\n19143\n65810\n2000-01-31\n60701.0\n\n\n1\n19111\n65779\n2000-01-31\n85062.0\n\n\n2\n19124\n65791\n2000-01-31\n47155.0\n\n\n3\n19120\n65787\n2000-01-31\n59285.0\n\n\n4\n19104\n65772\n2000-01-31\n74255.0\n\n\n...\n...\n...\n...\n...\n\n\n12461\n19153\n65820\n2022-07-31\n247560.0\n\n\n12462\n19118\n65785\n2022-07-31\n746009.0\n\n\n12463\n19102\n65770\n2022-07-31\n347614.0\n\n\n12464\n19127\n65794\n2022-07-31\n318732.0\n\n\n12465\n19137\n65804\n2022-07-31\n222107.0\n\n\n\n\n12466 rows × 4 columns"
  },
  {
    "objectID": "analysis/Final-1.html#split-the-data-for-zip-codes-inoutside-center-city",
    "href": "analysis/Final-1.html#split-the-data-for-zip-codes-inoutside-center-city",
    "title": "Part I - Arts & Culture in New York",
    "section": "4. Split the data for ZIP codes in/outside Center City",
    "text": "4. Split the data for ZIP codes in/outside Center City\nTo compare home appreciation in Center City vs. outside Center City, we’ll need to split the data into two dataframes, one that holds the Center City ZIP codes and one that holds the data for the rest of the ZIP codes in Philadelphia.\nTo help with this process, I’ve included a list of ZIP codes that make up the “greater Center City” region of Philadelphia. Use this list to split the melted data into two dataframes.\n\n\nCode\ngreater_center_city_zip_codes = [\n    19123,\n    19102,\n    19103,\n    19106,\n    19107,\n    19109,\n    19130,\n    19146,\n    19147,\n]\n\n\n\n\nCode\n# CENTER CITY\ncenter_city_zip = philly_tidy[\"RegionName\"].isin(greater_center_city_zip_codes)\ncenter_city_zip\ncenter_city = philly_tidy.loc[center_city_zip].copy()\ncenter_city\n\n\n\n\n\n\n\n\n\nRegionName\nRegionID\nDate\nZHVI\n\n\n\n\n9\n19146\n65813\n2000-01-31\n97460.0\n\n\n12\n19147\n65814\n2000-01-31\n119919.0\n\n\n14\n19103\n65771\n2000-01-31\n183436.0\n\n\n18\n19130\n65797\n2000-01-31\n128477.0\n\n\n33\n19107\n65775\n2000-01-31\n128049.0\n\n\n...\n...\n...\n...\n...\n\n\n12438\n19130\n65797\n2022-07-31\n431501.0\n\n\n12453\n19107\n65775\n2022-07-31\n330958.0\n\n\n12457\n19123\n65790\n2022-07-31\n443152.0\n\n\n12458\n19106\n65774\n2022-07-31\n407423.0\n\n\n12463\n19102\n65770\n2022-07-31\n347614.0\n\n\n\n\n2168 rows × 4 columns\n\n\n\n\n\nCode\n# Out_of_Center_City\nout_of_center_city_zip = ~philly_tidy[\"RegionName\"].isin(greater_center_city_zip_codes)\nout_of_center_city_zip\nout_of_center_city = philly_tidy.loc[out_of_center_city_zip].copy()\nout_of_center_city\n\n\n\n\n\n\n\n\n\nRegionName\nRegionID\nDate\nZHVI\n\n\n\n\n0\n19143\n65810\n2000-01-31\n60701.0\n\n\n1\n19111\n65779\n2000-01-31\n85062.0\n\n\n2\n19124\n65791\n2000-01-31\n47155.0\n\n\n3\n19120\n65787\n2000-01-31\n59285.0\n\n\n4\n19104\n65772\n2000-01-31\n74255.0\n\n\n...\n...\n...\n...\n...\n\n\n12460\n19129\n65796\n2022-07-31\n302177.0\n\n\n12461\n19153\n65820\n2022-07-31\n247560.0\n\n\n12462\n19118\n65785\n2022-07-31\n746009.0\n\n\n12464\n19127\n65794\n2022-07-31\n318732.0\n\n\n12465\n19137\n65804\n2022-07-31\n222107.0\n\n\n\n\n10298 rows × 4 columns"
  },
  {
    "objectID": "analysis/Final-1.html#compare-home-value-appreciation-in-philadelpia",
    "href": "analysis/Final-1.html#compare-home-value-appreciation-in-philadelpia",
    "title": "Part I - Arts & Culture in New York",
    "section": "5. Compare home value appreciation in Philadelpia",
    "text": "5. Compare home value appreciation in Philadelpia\nIn this step, we’ll calculate the average percent increase in ZHVI from March 2020 to March 2022 for ZIP codes in/out of Center City. We’ll do this by:\n\nWriting a function (see the template below) that will calculate the percent increase in ZHVI from March 31, 2020 to March 31, 2022\nGroup your data and apply this function to calculate the ZHVI percent change for each ZIP code in Philadelphia. Do this for both of your dataframes from the previous step.\nCalculate the average value across ZIP codes for both sets of ZIP codes and then compare\n\nYou should see much larger growth for ZIP codes outside of Center City…the Donut Effect!\n\n\nCode\ndef calculate_percent_increase(group_df):\n    \n    march_20sel = group_df[\"Date\"] == \"2020-03-31\"\n    march_22sel = group_df[\"Date\"] == \"2022-03-31\"\n    \n    march_20 = group_df.loc[march_20sel].squeeze()\n    march_22 = group_df.loc[march_22sel].squeeze()\n    \n    columns = [\"ZHVI\"]\n    \n    return (march_22[columns] / march_20[columns] - 1) * 100\n\n\n\n\nCode\n# Center City Grouped\ngrouped_center_city = center_city.groupby(\"RegionName\")\nresult_center_city = grouped_center_city.apply(calculate_percent_increase)\nresult_center_city\n\n\n\n\n\n\n\n\n\nZHVI\n\n\nRegionName\n\n\n\n\n\n19102\n-1.716711\n\n\n19103\n-1.696176\n\n\n19106\n2.520802\n\n\n19107\n2.883181\n\n\n19123\n5.212747\n\n\n19130\n6.673031\n\n\n19146\n6.480788\n\n\n19147\n6.139806\n\n\n\n\n\n\n\n\n\nCode\n# Outside Center City Grouped\ngrouped_outside = out_of_center_city.groupby(\"RegionName\")\nresult_outside = grouped_outside.apply(calculate_percent_increase)\nresult_outside\n\n\n\n\n\n\n\n\n\nZHVI\n\n\nRegionName\n\n\n\n\n\n19104\n14.539797\n\n\n19111\n28.690446\n\n\n19114\n21.074312\n\n\n19115\n22.455454\n\n\n19116\n23.079842\n\n\n19118\n17.585001\n\n\n19119\n17.478667\n\n\n19120\n26.927423\n\n\n19121\n26.228643\n\n\n19122\n10.275804\n\n\n19124\n28.743474\n\n\n19125\n11.007135\n\n\n19126\n20.819254\n\n\n19127\n20.023926\n\n\n19128\n21.887555\n\n\n19129\n15.598565\n\n\n19131\n23.363129\n\n\n19132\n72.218386\n\n\n19133\n36.143992\n\n\n19134\n23.936841\n\n\n19135\n28.115259\n\n\n19136\n26.487833\n\n\n19137\n23.248505\n\n\n19138\n24.662626\n\n\n19139\n37.008969\n\n\n19140\n57.150847\n\n\n19141\n26.441684\n\n\n19142\n44.564396\n\n\n19143\n23.951077\n\n\n19144\n21.094020\n\n\n19145\n7.634693\n\n\n19148\n6.963237\n\n\n19149\n24.916458\n\n\n19150\n18.735248\n\n\n19151\n19.651429\n\n\n19152\n21.993528\n\n\n19153\n38.240461\n\n\n19154\n17.930932\n\n\n\n\n\n\n\n\n\nCode\n# Center City Growth\nresult_cc_mean = result_center_city.mean().squeeze()\n\n\n\n\nCode\n# Outside of Center City Growth\nresult_ot_mean = result_outside.mean().squeeze()\n\n\n\n\nCode\nHV_growth = pd.Series([result_cc_mean,result_ot_mean])\nLocation = pd.Series([\"Center City\", \"Outside Center City\"])\nfinal_result_df = pd.DataFrame({\"Location\": Location, \"Home Value growth (%)\": HV_growth})\nfinal_result_df\n\n\n\n\n\n\n\n\n\nLocation\nHome Value growth (%)\n\n\n\n\n0\nCenter City\n3.312183\n\n\n1\nOutside Center City\n25.022864\n\n\n\n\n\n\n\nDonut Effect it is!"
  },
  {
    "objectID": "analysis/Final-2.html",
    "href": "analysis/Final-2.html",
    "title": "Part II - Street Networks and Accessibility",
    "section": "",
    "text": "This part aims to push further with the analysis of spatial distribution of arts and cultural amenities and look at their accessibility. Due to the limited scope of this project, the analysis may not involve the whole existing dataset, but rather, a small part of data is used for demonstrating purpose.\nCode\n# Packages\n\nimport xarray as xr\nimport hvplot.pandas  # noqa\nimport hvplot.xarray  # noqa\nimport cartopy.crs as ccrs\n\nimport pandas as pd\nimport numpy as np\nimport altair as alt\nimport geopandas as gpd\nimport hvplot.pandas\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport datetime\nimport math\n\n%matplotlib inline\n\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport osmnx as ox\nfrom shapely.ops import unary_union\nimport panel as pn\nCode\n# Load Data\n\n# NYC\n\n# Neighborhoods Geo\nNYC_Neighborhood = pd.read_csv(\"./Final_Data/1/2020 Neighborhood.csv\")\nNYC_Neighborhood['geometry'] = gpd.GeoSeries.from_wkt(NYC_Neighborhood['the_geom'])\nNYC_Neighborhood = gpd.GeoDataFrame(NYC_Neighborhood, geometry='geometry').set_crs(epsg=4326)\n\n# Cultural Amenities Load Data\nNYC_art_galleries = gpd.read_file(\"./Final_Data/1/art galleries.geojson\").set_crs(epsg=4326)\nNYC_museums = gpd.read_file(\"./Final_Data/1/museums.geojson\").set_crs(epsg=4326)\nNYC_library = gpd.read_file(\"./Final_Data/1/libraries.geojson\").set_crs(epsg=4326)\nNYC_theaters = gpd.read_file(\"./Final_Data/1/Theaters.geojson\").set_crs(epsg=4326)\n\nNYC_art_galleries = NYC_art_galleries[['name','zip','address1','geometry']]\nNYC_art_galleries.rename(\n    columns={\"address1\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\n\nNYC_museums = NYC_museums[['name','zip','adress1','geometry']]\nNYC_museums.rename(\n    columns={\"adress1\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\n\nNYC_library = NYC_library[['name','zip','streetname','geometry']]\nNYC_library.rename(\n    columns={\"streetname\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\n\nNYC_theaters = NYC_theaters[['name','zip','address1','geometry']]\nNYC_theaters.rename(\n    columns={\"address1\": \"Address\", \"name\": \"Name\", \"zip\": \"Zip\"},\n    inplace=True,)\n\n\n# Aggregate all \nNYC_art_galleries.loc[:,\"Type\"]= \"Art Gallery\"\nNYC_museums.loc[:,\"Type\"]= \"Museums\"\nNYC_library.loc[:,\"Type\"]= \"Libraries\"\nNYC_theaters.loc[:,\"Type\"]= \"Theatre\"\n\n# Cultural Amenities\nNYC_amenities = pd.concat([NYC_art_galleries, NYC_museums, NYC_library, NYC_theaters])"
  },
  {
    "objectID": "analysis/Final-2.html#load-the-data",
    "href": "analysis/Final-2.html#load-the-data",
    "title": "Part II - The MET and MOMA from a Data Lens",
    "section": "",
    "text": "I’ve already downloaded the relevant data file and put in the data/ folder. Let’s load it using pandas.\nNote: Be sure to use a relative file path to make it easier to load your data when grading. See this guide for more info.\n\n\nCode\nimport pandas as pd\n\n\n\n\nCode\nimport numpy as np\n\n\n\n\nCode\nzip_df = pd.read_csv(\"data/Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")"
  },
  {
    "objectID": "analysis/Final-2.html#trim-the-data-to-just-philadelphia",
    "href": "analysis/Final-2.html#trim-the-data-to-just-philadelphia",
    "title": "Part II - The MET and MOMA from a Data Lens",
    "section": "2. Trim the data to just Philadelphia",
    "text": "2. Trim the data to just Philadelphia\nSelect the subset of the dataframe for Philadelphia, PA.\n\n\nCode\n# or use isin to trim data\n\nstate_trim = zip_df[\"StateName\"].isin([\"PA\"])\nstate_trim_df= zip_df.loc[state_trim]\n\nphilly_df = state_trim_df.loc[state_trim_df[\"City\"] == \"Philadelphia\"]\nphilly_df.head()\n\n\n\n\n\n\n\n\n\nRegionID\nSizeRank\nRegionName\nRegionType\nStateName\nState\nCity\nMetro\nCountyName\n2000-01-31\n...\n2021-10-31\n2021-11-30\n2021-12-31\n2022-01-31\n2022-02-28\n2022-03-31\n2022-04-30\n2022-05-31\n2022-06-30\n2022-07-31\n\n\n\n\n125\n65810\n126\n19143\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n60701.0\n...\n173114.0\n172087.0\n171445.0\n171542.0\n171680.0\n171878.0\n171607.0\n171333.0\n171771.0\n172611.0\n\n\n247\n65779\n249\n19111\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n85062.0\n...\n257911.0\n260104.0\n262257.0\n263715.0\n264809.0\n265684.0\n267222.0\n269460.0\n272201.0\n274446.0\n\n\n338\n65791\n340\n19124\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n47155.0\n...\n156225.0\n157780.0\n159029.0\n159274.0\n159886.0\n160780.0\n161929.0\n163625.0\n165020.0\n166009.0\n\n\n423\n65787\n426\n19120\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n59285.0\n...\n161167.0\n161807.0\n162634.0\n162972.0\n163597.0\n164008.0\n164887.0\n165860.0\n167321.0\n168524.0\n\n\n509\n65772\n512\n19104\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n74255.0\n...\n220270.0\n221454.0\n222006.0\n220760.0\n217933.0\n216447.0\n216424.0\n218663.0\n220453.0\n223443.0\n\n\n\n\n5 rows × 280 columns"
  },
  {
    "objectID": "analysis/Final-2.html#melt-the-data-into-tidy-format",
    "href": "analysis/Final-2.html#melt-the-data-into-tidy-format",
    "title": "Part II - The MET and MOMA from a Data Lens",
    "section": "3. Melt the data into tidy format",
    "text": "3. Melt the data into tidy format\nLet’s transform the data from wide to tidy using the pd.melt() function. Create a new column in your data called “ZHVI” that holds the ZHVI values.\n\n\nCode\ndef looks_like_a_date(col):\n    \"\"\"A function that tests if a string starts with '20'\"\"\"\n    \n    return col.startswith(\"20\")\n\n\n\n\nCode\nphilly_tidy = philly_df.melt(\n    id_vars = [\"RegionName\",\"RegionID\"],\n    value_vars = list(filter(looks_like_a_date, philly_df.columns)),\n    var_name = \"Date\",\n    value_name = \"ZHVI\",\n)\n    \n\n\n\n\nCode\nphilly_tidy\n\n\n\n\n\n\n\n\n\nRegionName\nRegionID\nDate\nZHVI\n\n\n\n\n0\n19143\n65810\n2000-01-31\n60701.0\n\n\n1\n19111\n65779\n2000-01-31\n85062.0\n\n\n2\n19124\n65791\n2000-01-31\n47155.0\n\n\n3\n19120\n65787\n2000-01-31\n59285.0\n\n\n4\n19104\n65772\n2000-01-31\n74255.0\n\n\n...\n...\n...\n...\n...\n\n\n12461\n19153\n65820\n2022-07-31\n247560.0\n\n\n12462\n19118\n65785\n2022-07-31\n746009.0\n\n\n12463\n19102\n65770\n2022-07-31\n347614.0\n\n\n12464\n19127\n65794\n2022-07-31\n318732.0\n\n\n12465\n19137\n65804\n2022-07-31\n222107.0\n\n\n\n\n12466 rows × 4 columns"
  },
  {
    "objectID": "analysis/Final-2.html#split-the-data-for-zip-codes-inoutside-center-city",
    "href": "analysis/Final-2.html#split-the-data-for-zip-codes-inoutside-center-city",
    "title": "Part II - The MET and MOMA from a Data Lens",
    "section": "4. Split the data for ZIP codes in/outside Center City",
    "text": "4. Split the data for ZIP codes in/outside Center City\nTo compare home appreciation in Center City vs. outside Center City, we’ll need to split the data into two dataframes, one that holds the Center City ZIP codes and one that holds the data for the rest of the ZIP codes in Philadelphia.\nTo help with this process, I’ve included a list of ZIP codes that make up the “greater Center City” region of Philadelphia. Use this list to split the melted data into two dataframes.\n\n\nCode\ngreater_center_city_zip_codes = [\n    19123,\n    19102,\n    19103,\n    19106,\n    19107,\n    19109,\n    19130,\n    19146,\n    19147,\n]\n\n\n\n\nCode\n# CENTER CITY\ncenter_city_zip = philly_tidy[\"RegionName\"].isin(greater_center_city_zip_codes)\ncenter_city_zip\ncenter_city = philly_tidy.loc[center_city_zip].copy()\ncenter_city\n\n\n\n\n\n\n\n\n\nRegionName\nRegionID\nDate\nZHVI\n\n\n\n\n9\n19146\n65813\n2000-01-31\n97460.0\n\n\n12\n19147\n65814\n2000-01-31\n119919.0\n\n\n14\n19103\n65771\n2000-01-31\n183436.0\n\n\n18\n19130\n65797\n2000-01-31\n128477.0\n\n\n33\n19107\n65775\n2000-01-31\n128049.0\n\n\n...\n...\n...\n...\n...\n\n\n12438\n19130\n65797\n2022-07-31\n431501.0\n\n\n12453\n19107\n65775\n2022-07-31\n330958.0\n\n\n12457\n19123\n65790\n2022-07-31\n443152.0\n\n\n12458\n19106\n65774\n2022-07-31\n407423.0\n\n\n12463\n19102\n65770\n2022-07-31\n347614.0\n\n\n\n\n2168 rows × 4 columns\n\n\n\n\n\nCode\n# Out_of_Center_City\nout_of_center_city_zip = ~philly_tidy[\"RegionName\"].isin(greater_center_city_zip_codes)\nout_of_center_city_zip\nout_of_center_city = philly_tidy.loc[out_of_center_city_zip].copy()\nout_of_center_city\n\n\n\n\n\n\n\n\n\nRegionName\nRegionID\nDate\nZHVI\n\n\n\n\n0\n19143\n65810\n2000-01-31\n60701.0\n\n\n1\n19111\n65779\n2000-01-31\n85062.0\n\n\n2\n19124\n65791\n2000-01-31\n47155.0\n\n\n3\n19120\n65787\n2000-01-31\n59285.0\n\n\n4\n19104\n65772\n2000-01-31\n74255.0\n\n\n...\n...\n...\n...\n...\n\n\n12460\n19129\n65796\n2022-07-31\n302177.0\n\n\n12461\n19153\n65820\n2022-07-31\n247560.0\n\n\n12462\n19118\n65785\n2022-07-31\n746009.0\n\n\n12464\n19127\n65794\n2022-07-31\n318732.0\n\n\n12465\n19137\n65804\n2022-07-31\n222107.0\n\n\n\n\n10298 rows × 4 columns"
  },
  {
    "objectID": "analysis/Final-2.html#compare-home-value-appreciation-in-philadelpia",
    "href": "analysis/Final-2.html#compare-home-value-appreciation-in-philadelpia",
    "title": "Part II - The MET and MOMA from a Data Lens",
    "section": "5. Compare home value appreciation in Philadelpia",
    "text": "5. Compare home value appreciation in Philadelpia\nIn this step, we’ll calculate the average percent increase in ZHVI from March 2020 to March 2022 for ZIP codes in/out of Center City. We’ll do this by:\n\nWriting a function (see the template below) that will calculate the percent increase in ZHVI from March 31, 2020 to March 31, 2022\nGroup your data and apply this function to calculate the ZHVI percent change for each ZIP code in Philadelphia. Do this for both of your dataframes from the previous step.\nCalculate the average value across ZIP codes for both sets of ZIP codes and then compare\n\nYou should see much larger growth for ZIP codes outside of Center City…the Donut Effect!\n\n\nCode\ndef calculate_percent_increase(group_df):\n    \n    march_20sel = group_df[\"Date\"] == \"2020-03-31\"\n    march_22sel = group_df[\"Date\"] == \"2022-03-31\"\n    \n    march_20 = group_df.loc[march_20sel].squeeze()\n    march_22 = group_df.loc[march_22sel].squeeze()\n    \n    columns = [\"ZHVI\"]\n    \n    return (march_22[columns] / march_20[columns] - 1) * 100\n\n\n\n\nCode\n# Center City Grouped\ngrouped_center_city = center_city.groupby(\"RegionName\")\nresult_center_city = grouped_center_city.apply(calculate_percent_increase)\nresult_center_city\n\n\n\n\n\n\n\n\n\nZHVI\n\n\nRegionName\n\n\n\n\n\n19102\n-1.716711\n\n\n19103\n-1.696176\n\n\n19106\n2.520802\n\n\n19107\n2.883181\n\n\n19123\n5.212747\n\n\n19130\n6.673031\n\n\n19146\n6.480788\n\n\n19147\n6.139806\n\n\n\n\n\n\n\n\n\nCode\n# Outside Center City Grouped\ngrouped_outside = out_of_center_city.groupby(\"RegionName\")\nresult_outside = grouped_outside.apply(calculate_percent_increase)\nresult_outside\n\n\n\n\n\n\n\n\n\nZHVI\n\n\nRegionName\n\n\n\n\n\n19104\n14.539797\n\n\n19111\n28.690446\n\n\n19114\n21.074312\n\n\n19115\n22.455454\n\n\n19116\n23.079842\n\n\n19118\n17.585001\n\n\n19119\n17.478667\n\n\n19120\n26.927423\n\n\n19121\n26.228643\n\n\n19122\n10.275804\n\n\n19124\n28.743474\n\n\n19125\n11.007135\n\n\n19126\n20.819254\n\n\n19127\n20.023926\n\n\n19128\n21.887555\n\n\n19129\n15.598565\n\n\n19131\n23.363129\n\n\n19132\n72.218386\n\n\n19133\n36.143992\n\n\n19134\n23.936841\n\n\n19135\n28.115259\n\n\n19136\n26.487833\n\n\n19137\n23.248505\n\n\n19138\n24.662626\n\n\n19139\n37.008969\n\n\n19140\n57.150847\n\n\n19141\n26.441684\n\n\n19142\n44.564396\n\n\n19143\n23.951077\n\n\n19144\n21.094020\n\n\n19145\n7.634693\n\n\n19148\n6.963237\n\n\n19149\n24.916458\n\n\n19150\n18.735248\n\n\n19151\n19.651429\n\n\n19152\n21.993528\n\n\n19153\n38.240461\n\n\n19154\n17.930932\n\n\n\n\n\n\n\n\n\nCode\n# Center City Growth\nresult_cc_mean = result_center_city.mean().squeeze()\n\n\n\n\nCode\n# Outside of Center City Growth\nresult_ot_mean = result_outside.mean().squeeze()\n\n\n\n\nCode\nHV_growth = pd.Series([result_cc_mean,result_ot_mean])\nLocation = pd.Series([\"Center City\", \"Outside Center City\"])\nfinal_result_df = pd.DataFrame({\"Location\": Location, \"Home Value growth (%)\": HV_growth})\nfinal_result_df\n\n\n\n\n\n\n\n\n\nLocation\nHome Value growth (%)\n\n\n\n\n0\nCenter City\n3.312183\n\n\n1\nOutside Center City\n25.022864\n\n\n\n\n\n\n\nDonut Effect it is!"
  },
  {
    "objectID": "analysis/Final-3.html",
    "href": "analysis/Final-3.html",
    "title": "Part III - Museum Collections - MET, MoMA, British Museum",
    "section": "",
    "text": "The last part of this project zooms into museum collections at famous worldwide museums, including the MET, MoMA, British Museum, V&AM, and Philadelphia Museum of Art (used as an example for web scrapping, though it Is not located in New York or London). The part is broken into four sections – starting with an overview of large museums worldwide, then moving into three different ways of acquiring information about collections at museums online.\nThe first group is MET and MoMA, which have the most well-constructed digital galleries and open-access data hosted on GitHub. With the existing dataset, it is very easy to take a deep dive into the analysis.\nThe second group is British Museum, which does not have GitHub page but has a mature database for their abundant objects in the collections. Their websites have the function of downloading the search results in csv, if the results are under 20,000 items. In comparison to web-scrapping, this query and open access system allows visitors to download a way more detailed table of all the search results, despite the capacity is capped at 20,000 (so if you want to download more than 20,000, got to find a way to parse and download two times separately).\nThe last group is museums that has online digital collections showcasing on website but not yet given open data access to the general public. With their digital gallery, it is possible for users to web scrap and acquire basic information of objects. But two downsides are 1) it is very slow to scrap many items, and 2) the information acquired is too basic (i.e., usually only the name of the artwork, name of the artist, year, and geography is displayed on the search result page). As you will see in section 2 in this document, there is a clear comparison in terms of the level of details available about each item when comparing the three approaches.\nUltimately, this part of the project is to advocate for more resources given to impactful large museums to establish their online open access database. The public will undoubtedly enjoy it and may be able to generate some important insights for Museum’s future curations.\nCode\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objects as go\nimport numpy as np\npd.set_option('display.max_columns', None)"
  },
  {
    "objectID": "analysis/Final-3.html#load-the-data",
    "href": "analysis/Final-3.html#load-the-data",
    "title": "Part III - Cultural Infrastructure Street Networks",
    "section": "",
    "text": "I’ve already downloaded the relevant data file and put in the data/ folder. Let’s load it using pandas.\nNote: Be sure to use a relative file path to make it easier to load your data when grading. See this guide for more info.\n\n\nCode\nimport pandas as pd\n\n\n\n\nCode\nimport numpy as np\n\n\n\n\nCode\nzip_df = pd.read_csv(\"data/Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")"
  },
  {
    "objectID": "analysis/Final-3.html#trim-the-data-to-just-philadelphia",
    "href": "analysis/Final-3.html#trim-the-data-to-just-philadelphia",
    "title": "Part III - Cultural Infrastructure Street Networks",
    "section": "2. Trim the data to just Philadelphia",
    "text": "2. Trim the data to just Philadelphia\nSelect the subset of the dataframe for Philadelphia, PA.\n\n\nCode\n# or use isin to trim data\n\nstate_trim = zip_df[\"StateName\"].isin([\"PA\"])\nstate_trim_df= zip_df.loc[state_trim]\n\nphilly_df = state_trim_df.loc[state_trim_df[\"City\"] == \"Philadelphia\"]\nphilly_df.head()\n\n\n\n\n\n\n\n\n\nRegionID\nSizeRank\nRegionName\nRegionType\nStateName\nState\nCity\nMetro\nCountyName\n2000-01-31\n...\n2021-10-31\n2021-11-30\n2021-12-31\n2022-01-31\n2022-02-28\n2022-03-31\n2022-04-30\n2022-05-31\n2022-06-30\n2022-07-31\n\n\n\n\n125\n65810\n126\n19143\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n60701.0\n...\n173114.0\n172087.0\n171445.0\n171542.0\n171680.0\n171878.0\n171607.0\n171333.0\n171771.0\n172611.0\n\n\n247\n65779\n249\n19111\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n85062.0\n...\n257911.0\n260104.0\n262257.0\n263715.0\n264809.0\n265684.0\n267222.0\n269460.0\n272201.0\n274446.0\n\n\n338\n65791\n340\n19124\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n47155.0\n...\n156225.0\n157780.0\n159029.0\n159274.0\n159886.0\n160780.0\n161929.0\n163625.0\n165020.0\n166009.0\n\n\n423\n65787\n426\n19120\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n59285.0\n...\n161167.0\n161807.0\n162634.0\n162972.0\n163597.0\n164008.0\n164887.0\n165860.0\n167321.0\n168524.0\n\n\n509\n65772\n512\n19104\nZip\nPA\nPA\nPhiladelphia\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n74255.0\n...\n220270.0\n221454.0\n222006.0\n220760.0\n217933.0\n216447.0\n216424.0\n218663.0\n220453.0\n223443.0\n\n\n\n\n5 rows × 280 columns"
  },
  {
    "objectID": "analysis/Final-3.html#melt-the-data-into-tidy-format",
    "href": "analysis/Final-3.html#melt-the-data-into-tidy-format",
    "title": "Part III - Cultural Infrastructure Street Networks",
    "section": "3. Melt the data into tidy format",
    "text": "3. Melt the data into tidy format\nLet’s transform the data from wide to tidy using the pd.melt() function. Create a new column in your data called “ZHVI” that holds the ZHVI values.\n\n\nCode\ndef looks_like_a_date(col):\n    \"\"\"A function that tests if a string starts with '20'\"\"\"\n    \n    return col.startswith(\"20\")\n\n\n\n\nCode\nphilly_tidy = philly_df.melt(\n    id_vars = [\"RegionName\",\"RegionID\"],\n    value_vars = list(filter(looks_like_a_date, philly_df.columns)),\n    var_name = \"Date\",\n    value_name = \"ZHVI\",\n)\n    \n\n\n\n\nCode\nphilly_tidy\n\n\n\n\n\n\n\n\n\nRegionName\nRegionID\nDate\nZHVI\n\n\n\n\n0\n19143\n65810\n2000-01-31\n60701.0\n\n\n1\n19111\n65779\n2000-01-31\n85062.0\n\n\n2\n19124\n65791\n2000-01-31\n47155.0\n\n\n3\n19120\n65787\n2000-01-31\n59285.0\n\n\n4\n19104\n65772\n2000-01-31\n74255.0\n\n\n...\n...\n...\n...\n...\n\n\n12461\n19153\n65820\n2022-07-31\n247560.0\n\n\n12462\n19118\n65785\n2022-07-31\n746009.0\n\n\n12463\n19102\n65770\n2022-07-31\n347614.0\n\n\n12464\n19127\n65794\n2022-07-31\n318732.0\n\n\n12465\n19137\n65804\n2022-07-31\n222107.0\n\n\n\n\n12466 rows × 4 columns"
  },
  {
    "objectID": "analysis/Final-3.html#split-the-data-for-zip-codes-inoutside-center-city",
    "href": "analysis/Final-3.html#split-the-data-for-zip-codes-inoutside-center-city",
    "title": "Part III - Cultural Infrastructure Street Networks",
    "section": "4. Split the data for ZIP codes in/outside Center City",
    "text": "4. Split the data for ZIP codes in/outside Center City\nTo compare home appreciation in Center City vs. outside Center City, we’ll need to split the data into two dataframes, one that holds the Center City ZIP codes and one that holds the data for the rest of the ZIP codes in Philadelphia.\nTo help with this process, I’ve included a list of ZIP codes that make up the “greater Center City” region of Philadelphia. Use this list to split the melted data into two dataframes.\n\n\nCode\ngreater_center_city_zip_codes = [\n    19123,\n    19102,\n    19103,\n    19106,\n    19107,\n    19109,\n    19130,\n    19146,\n    19147,\n]\n\n\n\n\nCode\n# CENTER CITY\ncenter_city_zip = philly_tidy[\"RegionName\"].isin(greater_center_city_zip_codes)\ncenter_city_zip\ncenter_city = philly_tidy.loc[center_city_zip].copy()\ncenter_city\n\n\n\n\n\n\n\n\n\nRegionName\nRegionID\nDate\nZHVI\n\n\n\n\n9\n19146\n65813\n2000-01-31\n97460.0\n\n\n12\n19147\n65814\n2000-01-31\n119919.0\n\n\n14\n19103\n65771\n2000-01-31\n183436.0\n\n\n18\n19130\n65797\n2000-01-31\n128477.0\n\n\n33\n19107\n65775\n2000-01-31\n128049.0\n\n\n...\n...\n...\n...\n...\n\n\n12438\n19130\n65797\n2022-07-31\n431501.0\n\n\n12453\n19107\n65775\n2022-07-31\n330958.0\n\n\n12457\n19123\n65790\n2022-07-31\n443152.0\n\n\n12458\n19106\n65774\n2022-07-31\n407423.0\n\n\n12463\n19102\n65770\n2022-07-31\n347614.0\n\n\n\n\n2168 rows × 4 columns\n\n\n\n\n\nCode\n# Out_of_Center_City\nout_of_center_city_zip = ~philly_tidy[\"RegionName\"].isin(greater_center_city_zip_codes)\nout_of_center_city_zip\nout_of_center_city = philly_tidy.loc[out_of_center_city_zip].copy()\nout_of_center_city\n\n\n\n\n\n\n\n\n\nRegionName\nRegionID\nDate\nZHVI\n\n\n\n\n0\n19143\n65810\n2000-01-31\n60701.0\n\n\n1\n19111\n65779\n2000-01-31\n85062.0\n\n\n2\n19124\n65791\n2000-01-31\n47155.0\n\n\n3\n19120\n65787\n2000-01-31\n59285.0\n\n\n4\n19104\n65772\n2000-01-31\n74255.0\n\n\n...\n...\n...\n...\n...\n\n\n12460\n19129\n65796\n2022-07-31\n302177.0\n\n\n12461\n19153\n65820\n2022-07-31\n247560.0\n\n\n12462\n19118\n65785\n2022-07-31\n746009.0\n\n\n12464\n19127\n65794\n2022-07-31\n318732.0\n\n\n12465\n19137\n65804\n2022-07-31\n222107.0\n\n\n\n\n10298 rows × 4 columns"
  },
  {
    "objectID": "analysis/Final-3.html#compare-home-value-appreciation-in-philadelpia",
    "href": "analysis/Final-3.html#compare-home-value-appreciation-in-philadelpia",
    "title": "Part III - Cultural Infrastructure Street Networks",
    "section": "5. Compare home value appreciation in Philadelpia",
    "text": "5. Compare home value appreciation in Philadelpia\nIn this step, we’ll calculate the average percent increase in ZHVI from March 2020 to March 2022 for ZIP codes in/out of Center City. We’ll do this by:\n\nWriting a function (see the template below) that will calculate the percent increase in ZHVI from March 31, 2020 to March 31, 2022\nGroup your data and apply this function to calculate the ZHVI percent change for each ZIP code in Philadelphia. Do this for both of your dataframes from the previous step.\nCalculate the average value across ZIP codes for both sets of ZIP codes and then compare\n\nYou should see much larger growth for ZIP codes outside of Center City…the Donut Effect!\n\n\nCode\ndef calculate_percent_increase(group_df):\n    \n    march_20sel = group_df[\"Date\"] == \"2020-03-31\"\n    march_22sel = group_df[\"Date\"] == \"2022-03-31\"\n    \n    march_20 = group_df.loc[march_20sel].squeeze()\n    march_22 = group_df.loc[march_22sel].squeeze()\n    \n    columns = [\"ZHVI\"]\n    \n    return (march_22[columns] / march_20[columns] - 1) * 100\n\n\n\n\nCode\n# Center City Grouped\ngrouped_center_city = center_city.groupby(\"RegionName\")\nresult_center_city = grouped_center_city.apply(calculate_percent_increase)\nresult_center_city\n\n\n\n\n\n\n\n\n\nZHVI\n\n\nRegionName\n\n\n\n\n\n19102\n-1.716711\n\n\n19103\n-1.696176\n\n\n19106\n2.520802\n\n\n19107\n2.883181\n\n\n19123\n5.212747\n\n\n19130\n6.673031\n\n\n19146\n6.480788\n\n\n19147\n6.139806\n\n\n\n\n\n\n\n\n\nCode\n# Outside Center City Grouped\ngrouped_outside = out_of_center_city.groupby(\"RegionName\")\nresult_outside = grouped_outside.apply(calculate_percent_increase)\nresult_outside\n\n\n\n\n\n\n\n\n\nZHVI\n\n\nRegionName\n\n\n\n\n\n19104\n14.539797\n\n\n19111\n28.690446\n\n\n19114\n21.074312\n\n\n19115\n22.455454\n\n\n19116\n23.079842\n\n\n19118\n17.585001\n\n\n19119\n17.478667\n\n\n19120\n26.927423\n\n\n19121\n26.228643\n\n\n19122\n10.275804\n\n\n19124\n28.743474\n\n\n19125\n11.007135\n\n\n19126\n20.819254\n\n\n19127\n20.023926\n\n\n19128\n21.887555\n\n\n19129\n15.598565\n\n\n19131\n23.363129\n\n\n19132\n72.218386\n\n\n19133\n36.143992\n\n\n19134\n23.936841\n\n\n19135\n28.115259\n\n\n19136\n26.487833\n\n\n19137\n23.248505\n\n\n19138\n24.662626\n\n\n19139\n37.008969\n\n\n19140\n57.150847\n\n\n19141\n26.441684\n\n\n19142\n44.564396\n\n\n19143\n23.951077\n\n\n19144\n21.094020\n\n\n19145\n7.634693\n\n\n19148\n6.963237\n\n\n19149\n24.916458\n\n\n19150\n18.735248\n\n\n19151\n19.651429\n\n\n19152\n21.993528\n\n\n19153\n38.240461\n\n\n19154\n17.930932\n\n\n\n\n\n\n\n\n\nCode\n# Center City Growth\nresult_cc_mean = result_center_city.mean().squeeze()\n\n\n\n\nCode\n# Outside of Center City Growth\nresult_ot_mean = result_outside.mean().squeeze()\n\n\n\n\nCode\nHV_growth = pd.Series([result_cc_mean,result_ot_mean])\nLocation = pd.Series([\"Center City\", \"Outside Center City\"])\nfinal_result_df = pd.DataFrame({\"Location\": Location, \"Home Value growth (%)\": HV_growth})\nfinal_result_df\n\n\n\n\n\n\n\n\n\nLocation\nHome Value growth (%)\n\n\n\n\n0\nCenter City\n3.312183\n\n\n1\nOutside Center City\n25.022864\n\n\n\n\n\n\n\nDonut Effect it is!"
  },
  {
    "objectID": "analysis/final_background.html",
    "href": "analysis/final_background.html",
    "title": "ALL ABOUT ART",
    "section": "",
    "text": "ALL ABOUT ART\nA Cultural Tale of Two Cities (Final Project)\n(Except it’s NYC not Paris this time since I speak zero French…)\nComing from an art history background, I’ve always been interested in public accessibility to art exhibition, galleries, and museums. This project aims to take a broader range of cultural infrastructure, which also includes theatres and libraries, and explore its spatial distribution and accessbility in New York City. The project consists of three parts:\nPart I - Basic Info - Culture, Economics, and … in NYC & London Part II - Street Networks & accessibility Part III - Highlight Collections at MET, MOMA, Tate, and The British\nImportant Takeaways / 1. As digital humanities become more prevelant in arts and humanities fields, more museumms starts to digitize their existing collection. I am very impressed by the MET and MOMA’s effort for showing"
  },
  {
    "objectID": "analysis/Final-1.html#cultural-amenities-x-boroughs",
    "href": "analysis/Final-1.html#cultural-amenities-x-boroughs",
    "title": "Part I - Arts & Culture in New York & London",
    "section": "1.1 Cultural Amenities x Boroughs",
    "text": "1.1 Cultural Amenities x Boroughs\nFor NYC, I have acquired data for four predominant types of cultural infrastructure – art galleries, museums, libraries, and theaters. The bar plot and heatmap below shows the distribution of each type of amenities across different boroughs. The number of art galleries far exceeds any other type of amenity, potentially since it includes private commercial galleries that’s for-profit, while other types of amenities receive government funding or support. Art galleries also tend to be smaller in size, making it easier to open. Most of the amenities, particularly art galleries and theaters, are located in Manhattan, where all other types of resources (e.g., public transportation) are also abundant. Libraries, nevertheless, seem to be more equally distributed across each borough, which aligns with most cities’ agenda of making public libraries accessible and equitable.\n\n\nCode\n# Spatial Join\namenities_geo = gpd.sjoin(\n    NYC_amenities,  # The point data for 311 tickets\n    NYC_Neighborhood.to_crs(NYC_amenities.crs),  # The neighborhoods (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\n\n\n\n\nCode\namenities_boro = amenities_geo.groupby(['Type','BoroName']).size().reset_index()\namenities_boro = amenities_boro.rename(columns={amenities_boro.columns[2]: 'Counts'})\n\n\n\n\nCode\nfig = px.bar(amenities_boro, x='Type', y='Counts', color='BoroName')\nfig.update_layout(template='plotly_white', height=700, title='Number of Different Types of Cultural Amenities in Each Borough')\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\namenities_boro_heatmap = amenities_boro.pivot(index=['Type'], columns='BoroName', values='Counts').fillna(0)\n\nfig = px.imshow(amenities_boro_heatmap)\nfig.update_layout(template='plotly_white', height=1000, width=1000, title='Heatmap showing similar trend')\n\nfig.show()"
  },
  {
    "objectID": "analysis/Final-1.html#distributions-across-neighborhoods",
    "href": "analysis/Final-1.html#distributions-across-neighborhoods",
    "title": "Part I - Arts & Culture in New York & London",
    "section": "1.2 Distributions Across Neighborhoods",
    "text": "1.2 Distributions Across Neighborhoods\nIf we treat all types of amenities as a whole and look at their spatial distribution across neighborhoods, we get the map below. The map is normalized by population of each neighborhood (2020 Census), while the bar chart shows the absolute counts of amenities.\nTogether, they show similar trend of Manhattan being the predominant hub for arts and cultural activities, in particular, SoHo, Chealsea, Midtown-Time Sq., Midtown-South-Flatiron-Union Sq., and Upper East-Carnegie Hall are homes to significantly more cultural amenities. This matches the public perception of those neighborhoods being more ‘artsy’ and are usually popular tourists’ destinations.\n\n\nCode\nNYC_NTA = NYC_Neighborhood[['NTAName', 'geometry']]\namenities_NTA = amenities_geo.groupby(['NTAName']).size().reset_index()\namenities_NTA = amenities_NTA.rename(columns={amenities_NTA.columns[1]: 'Counts'})\n\namenities_NTA = NYC_NTA.merge(\n    amenities_NTA, on='NTAName'\n)\namenities_NTA_pop = amenities_NTA.merge(NYC_NTA_pop, on='NTAName')\namenities_NTA_pop['Normalized'] = amenities_NTA_pop.apply(\n    lambda row: row['Counts'] / row['estimate'] if (row['estimate'] != 0 and row['estimate'] &gt; 100) else 0, axis=1\n)\n\n\n\n\n\nCode\namenities_NTA_pop.explore(column='Normalized', tiles=\"cartodbpositron\")\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nCode\namenities_NTA_types = amenities_geo.groupby(['Type', 'NTAName']).size().reset_index()\namenities_NTA_types = amenities_NTA_types.rename(columns={amenities_NTA_types.columns[2]: 'Counts'})\namenities_NTA_types = amenities_NTA_types.merge(NYC_NTA, on='NTAName').drop(columns='geometry')\n\n\n\n\nCode\n\n\nfig = px.bar(amenities_NTA_types.sort_values(by='NTAName'), x='NTAName', y='Counts', color='Type')\nfig.update_layout(height=1000, title='Distribution of Different Types of Cultural Amenities, by Neighborhood')\nfig.show()"
  },
  {
    "objectID": "analysis/Final-1.html#cultural-amenities-x-income",
    "href": "analysis/Final-1.html#cultural-amenities-x-income",
    "title": "Part I - Arts & Culture in New York & London",
    "section": "1.3 Cultural Amenities x Income",
    "text": "1.3 Cultural Amenities x Income\nUtilizing census data of median household income, I plotted the income vs. number of arts and cultural amenities relationship in the following scatter plot. Aligning with trends identified in the previous two sections, neighborhoods in Manhattan host more cultural amenities, and they tend to be home to people that are more well-off. Interestingly, for Bronx, Brooklyn, Queens, and Staten Island, we are seeing a relatively low number of amenities irrespective of the median household income. This suggests that income is not the only factor and should not be the restraint for development of art and cultural amenities. Communities with lower incomes have hope and should deserve more arts and cultural amenities in their close proximity, too.\n\n\nCode\n# NYC Population\n\nNYC_income = gpd.read_file(\"./Final_Data/1/NYC_Income.geojson\")\nNYC_income = NYC_income[['GEOID', 'estimate']]\nNYC_income['GEOID']=NYC_income['GEOID'].astype(int)\n\nNYC_income = NYC_Tracts.merge(NYC_income, on='GEOID', how='left') \nNYC_NTA_income = NYC_income.groupby(['NTAName']).median(['estimate']).reset_index().fillna(0)\n\n\n\n\nCode\nNTA_Boro_Name = NYC_Neighborhood[['NTAName', 'BoroName']]\n\n\n\n\nCode\namenities_NTA_income = amenities_NTA.merge(NYC_NTA_income, on='NTAName')\namenities_NTA_income = amenities_NTA_income.merge(NTA_Boro_Name, on='NTAName')\n\n\n\n\nCode\nfig = px.scatter(amenities_NTA_income, x='estimate', y='Counts', color='BoroName')\nfig.update_traces(marker_size=10)\nfig.update_layout(height=700, template='plotly_white', title=\"Count - Income Relation in New York Citys\")\nfig.show()"
  },
  {
    "objectID": "analysis/Final-1.html#creative-enterprise-zone",
    "href": "analysis/Final-1.html#creative-enterprise-zone",
    "title": "Part I - Arts & Culture in New York & London",
    "section": "2.1 Creative Enterprise Zone",
    "text": "2.1 Creative Enterprise Zone\nTo start off my digital exploration of Greater London, I mapped the Creative Enterprise Zones (CEZ), which was launched in 2018. According to Mayor’s office, CEZ is a Mayoral initiative “to designate areas of London where artists and creative businesses can find permanent affordable space to work; are supported to start-up and grow, and where local people are helped to learn creative sector skills and access pathways to employment.” [1] As plotted on map, CEZ is distributed across the city. Though still growing, this initiative can be a potential successful prototype for many other cities to deploy to support local art and businesses. (Feels like the Business Improvement District of arts…)\n[1] https://www.london.gov.uk/programmes-strategies/arts-and-culture/space-culture/explore-creative-enterprise-zones/about-creative-enterprise-zones\n\n\nCode\nLondon_CEZ.explore(column='sitename', tiles=\"cartodbpositron\")\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/Final-1.html#spatial-distribution-of-amenities-in-london",
    "href": "analysis/Final-1.html#spatial-distribution-of-amenities-in-london",
    "title": "Part I - Arts & Culture in New York & London",
    "section": "2.2 Spatial Distribution of Amenities in London",
    "text": "2.2 Spatial Distribution of Amenities in London\nAs expected, CEZ seems to be filling the spatial gaps between existing hubs of art and cultural activities in the city center. One important characteristic is that Greater London Authority has a very complete dataset of ALL types of cultural infrastructure, not only including performing or exhibiting spaces, but also creative spaces (e.g., studios or creative coworking space) and rehearsal space for performing arts. In comparison, NYC’s Open Data portal only highlights the more prominent types, which are particularly helpful for visitors. London’s dataset also incorporates creative workers, producers, and artists, which are fundamental for offering support and evaluating the effectiveness of their policies like CEZ. Again, spatial data collection becomes the very first important step for laying foundation for further analysis, enforcements, or incentives.\nSimilar to NYC, London’s city center is home to a lot more cultural amenities, especially Westminster and Camden. Yet despite the disparities, the boroughs on the outskirts have a decent number of amenities, ranging from 20 to 82, which can be considered relatively satisfactory.\n\n\nCode\nLondon_amenities.explore(column='Type', tiles='Carto DB Positron')\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nCode\nLondon_Boro_Counts = pd.DataFrame(London_amenities.groupby(['Borough']).size()).reset_index()\nLondon_Boro_Counts = London_Boro_Counts.rename(columns={London_Boro_Counts.columns[1]: 'Counts'})\n\n\n\n\nCode\nLondon_Boro = London_Boro.rename(columns={London_Boro.columns[1]: 'Borough'})\n\n\n\n\nCode\nLondon_Boro_Counts = London_Boro.merge(London_Boro_Counts, on='Borough')\nLondon_Boro_Counts.explore(\n    column='Counts', tiles='Carto DB Positron')\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/Final-1.html#income-by-borough",
    "href": "analysis/Final-1.html#income-by-borough",
    "title": "Part I - Arts & Culture in New York & London",
    "section": "2.3 Income by Borough",
    "text": "2.3 Income by Borough\nThe income data of boroughs of London is represented in hourly wage. The historic data has shown an increase in median hourly wage for all boroughs, though there has been a prominent difference in income across all boroughs. It can be spatially represented in the map below.\n\n\nCode\nEarnings = pd.read_csv('./Final_Data/1/London_Earnings_Simplified.csv')\n\n\n\n\nCode\nEarnings_geo = London_Boro.merge(Earnings, on='Borough')\n\n\n\n\nCode\nEarnings_Long = pd.melt(Earnings, id_vars=['Borough'], var_name='Year', value_name='Hourly Pay')\n\n\n\n\nCode\nfig = px.line(Earnings_Long, x='Year', y='Hourly Pay', color='Borough', symbol=\"Borough\")\n\nfig.update_layout(title_text=\"Hourly Wage in Boroughs (2002-2022)\", \n                  height=700,\n                 template='plotly_white')\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nEarnings_2022 = Earnings_Long.loc[Earnings_Long['Year'] == '2022']\nEarnings_Amenities = Earnings_2022.merge(London_Boro_Counts, on='Borough')\nEarnings_Amenities_Geo =  London_Boro_Counts.merge(Earnings_2022, on='Borough')\n\n\n\n\nCode\nEarnings_Amenities_Geo.explore(\n     column=\"Hourly Pay\",\n     tiles=\"Carto DB Positron\")\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/Final-1.html#income-and-access-to-cultural-infrastructure",
    "href": "analysis/Final-1.html#income-and-access-to-cultural-infrastructure",
    "title": "Part I - Arts & Culture in New York & London",
    "section": "2.4 Income and Access to Cultural Infrastructure",
    "text": "2.4 Income and Access to Cultural Infrastructure\nSimilar to NYC, there seems to be a weak correlation between income and number of cultural amenities, and most boroughs have a decent number of amenities, despite the gap with the top one, Westminster.\n\n\nCode\nfig = px.scatter(Earnings_Amenities, y=\"Counts\", x=\"Hourly Pay\", color=\"Borough\")\nfig.update_traces(marker_size=10)\nfig.update_layout(height=700, template='plotly_white', title=\"Count - Hourly Pay Relation in Boroughs, London\")\nfig.show()"
  },
  {
    "objectID": "analysis/Final-1.html#type-of-cultural-infrastructure-comparison",
    "href": "analysis/Final-1.html#type-of-cultural-infrastructure-comparison",
    "title": "Part I - Arts & Culture in New York & London",
    "section": "3.1 Type of Cultural Infrastructure Comparison",
    "text": "3.1 Type of Cultural Infrastructure Comparison\nThough this section is meant to provide comparisons between the two cities, it is important to acknowledge that the two cities are incomparable in many ways, from demographics to history to cultural roots. As this bar plot encapsulates, the existing data is largely different for two cities, with London having sophisticated data collection and more detailed breakdown. In London’s data, galleries are broken down into public and private commercial ones – while public galleries and museums are aggregated into the same category, commercial galleries stand alone in their own characteristic. Looking at data for NYC’s galleries, we see that it is a generic category of all galleries. Fundamentally, NYC’s data seems to be visitor oriented as visitors wouldn’t mind much about profiles of art galleries but rather what’s being displayed, versus London’s portal is set up more for city’s governing bodies, where public vs. private profiles largely impacts their income and tax status, funding, and many other aspects.\n\n\nCode\nLondon_Counts = pd.DataFrame(London_amenities.groupby(['Type']).size()).reset_index()\nLondon_Counts = London_Counts.rename(columns={London_Counts.columns[1]: 'Counts'})\nNYC_Counts = pd.DataFrame(NYC_amenities.groupby(['Type']).size()).reset_index()\nNYC_Counts = NYC_Counts.rename(columns={NYC_Counts.columns[1]: 'Counts'})\n\n\n\n\nCode\nfig = go.Figure()\n\n\nfig.add_trace(\n    go.Bar(x=NYC_Counts['Type'], \n           y=NYC_Counts['Counts'], \n           name='New York City',\n          marker=dict(color=\"#E81D2E\")))\n\n\nfig.add_trace(\n    go.Bar(x=London_Counts['Type'], \n           y=London_Counts['Counts'], \n           name='London',\n          marker=dict(color=\"Black\")))\n\n#add dropdown\n\nfig.update_layout(\n    updatemenus=[\n        dict(\n            active=0,\n            buttons=list([\n                dict(label=\"Both\",\n                    method=\"update\",\n                    args=[{\"visible\": [True, True]},\n                        {\"title\": \"Both\",\n                         \"annotations\": []}]),\n                dict(label=\"New York City\",\n                    method=\"update\",\n                    args=[{\"visible\": [True, False]},\n                        {\"title\": \"New York City\",\n                         \"annotations\": []}]),\n                dict(label=\"London\",\n                    method=\"update\",\n                    args=[{\"visible\": [False, True]},\n                        {\"title\": \"London\",\n                         \"annotations\": []}])\n            ]))])\n\nfig.update_layout(title_text=\"Cultural Amenities\", \n                  height=700,\n                 template='plotly_white')\nfig.show()"
  },
  {
    "objectID": "analysis/Final-1.html#two-cities-comparison-a-glance",
    "href": "analysis/Final-1.html#two-cities-comparison-a-glance",
    "title": "Part I - Arts & Culture in New York & London",
    "section": "3.2 Two Cities Comparison: A Glance",
    "text": "3.2 Two Cities Comparison: A Glance\nAdditionally, considering the big difference of size (areal size) and socio-economic status, and minor difference in population, it is only fair to look at total number of arts and cultural amenities after being normalized by population, area, or income. The normalized data for comparison is showcased in the following bar plot with a dropdown menu.\n\n\nCode\nLondon_Counts_short = London_Counts[London_Counts['Type'].isin(['Art Centers', 'Commercial Galleries', 'Museums', 'Theatre', 'Libraries'])]\n\n\n\n\nCode\nNYC_London_Compare = pd.DataFrame([['NYC', NYC_Counts['Counts'].sum()], ['London', London_Counts_short['Counts'].sum()]], columns=['City', 'Total Counts'])\nNYC_London_Compare['by Population (Per 1000 People)'] = NYC_London_Compare.apply(lambda row: row['Total Counts'] / 8335.897 if row['City'] == 'NYC' else row['Total Counts'] / 8796.628, axis=1)\nNYC_London_Compare['by Area/Size (Per Sq Miles)'] = NYC_London_Compare.apply(lambda row: row['Total Counts'] / 302.6 if row['City'] == 'NYC' else row['Total Counts'] / 607, axis=1)\nNYC_London_Compare['by Household Income ($)'] = NYC_London_Compare.apply(lambda row: row['Total Counts'] / 76607 if row['City'] == 'NYC' else row['Total Counts'] / 40642.44, axis=1)\n\n\n\n\nCode\nfig = go.Figure()\n\nfig.add_trace(\n    go.Bar(x=NYC_London_Compare['City'],\n           y=NYC_London_Compare['Total Counts'],\n           name='Total Counts',\n          marker=dict(color=\"#E81D2E\")))\n\nfig.add_trace(\n    go.Bar(x=NYC_London_Compare['City'], \n           y=NYC_London_Compare['by Population (Per 1000 People)'], \n           name='by Population (Per 1000 People)',\n          marker=dict(color=\"Black\")))\n    \nfig.add_trace(\n    go.Bar(x=NYC_London_Compare['City'], \n           y=NYC_London_Compare['by Area/Size (Per Sq Miles)'], \n           name='by Area/Size (Per Sq Miles)',\n          marker=dict(color=\"Grey\")))\n\nfig.add_trace(\n    go.Bar(x=NYC_London_Compare['City'], \n           y=NYC_London_Compare['by Household Income ($)'], \n           name='by Household Income ($)',\n          marker=dict(color=\"Blue\")))\n\n#add dropdown\n\nfig.update_layout(\n    updatemenus=[\n        dict(\n            active=0,\n            buttons=list([\n                dict(label=\"All\",\n                    method=\"update\",\n                    args=[{\"visible\": [True, True, True, True]},\n                        {\"title\": \"All\",\n                         \"annotations\": []}]),\n                dict(label=\"Total Counts\",\n                    method=\"update\",\n                    args=[{\"visible\": [True, False, False, False]},\n                        {\"title\": \"Number of Cultural Amenities\",\n                         \"annotations\": []}]),\n                dict(label=\"by Population (Per 1000 People)\",\n                    method=\"update\",\n                    args=[{\"visible\": [False, True, False, False]},\n                        {\"title\": \"Number of Cultural Amenities by Population (Per 1000 People)\",\n                         \"annotations\": []}]),\n                dict(label=\"by Area/Size (Per Sq Miles)\",\n                    method=\"update\",\n                    args=[{\"visible\": [False, False, True, False]},\n                        {\"title\": \"Number of Cultural Amenities by Area/Size (Per Sq Miles)\",\n                         \"annotations\": []}]),\n                dict(label=\"by Household Income ($)\",\n                    method=\"update\",\n                    args=[{\"visible\": [False, False, False, True]},\n                        {\"title\": \"Number of Cultural Amenities by Household Income ($)\",\n                         \"annotations\": []}])\n                \n            ]))])\n\nfig.update_layout(title=\"Cultural Amenities \", \n                  height=700,\n                 template='plotly_white')\nfig.show()"
  },
  {
    "objectID": "analysis/Final-2.html#nyc",
    "href": "analysis/Final-2.html#nyc",
    "title": "Part II - Street Networks and Accessibility",
    "section": "1.1 NYC",
    "text": "1.1 NYC\n\n\nCode\nNYC_art = pd.concat([NYC_art_galleries, NYC_museums])\n\n\n\n\nCode\nNYC_Subway = gpd.read_file(\"./Final_Data/2/MTA Subway Stations.geojson\")\n\n\n\n\nCode\n# Two minute Buffer\ntwo_min = NYC_Subway.to_crs(epsg=3857).buffer(150)\ntwo_min_buffer= gpd.GeoDataFrame(geometry=two_min).set_crs(epsg=3857)\n\ntwo_min_union= unary_union(two_min_buffer['geometry'])\nunion_2_NYC= gpd.GeoDataFrame(geometry=[two_min_union]).set_crs(epsg=3857).to_crs(epsg=4326)\n\n# Five minute Buffer\nfive_min = NYC_Subway.to_crs(epsg=3857).buffer(400)\nfive_min_buffer= gpd.GeoDataFrame(geometry=five_min).set_crs(epsg=3857)\n\nfive_min_union= unary_union(five_min_buffer['geometry'])\nunion_5_NYC= gpd.GeoDataFrame(geometry=[five_min_union]).set_crs(epsg=3857).to_crs(epsg=4326)\n\n# Ten minute Buffer\nten_min = NYC_Subway.to_crs(epsg=3857).buffer(800)\nten_min_buffer= gpd.GeoDataFrame(geometry=ten_min).set_crs(epsg=3857)\n\nten_min_union= unary_union(ten_min_buffer['geometry'])\nunion_10_NYC= gpd.GeoDataFrame(geometry=[ten_min_union]).set_crs(epsg=3857).to_crs(epsg=4326)\n\n\n\n\nCode\nNYC_art['Buffer_2_min'] = NYC_art['geometry'].apply(lambda point: 'Yes' if any(union_2_NYC['geometry'].contains(point)) else 'No' )\nNYC_art['Buffer_5_min'] = NYC_art['geometry'].apply(lambda point: 'Yes' if any(union_5_NYC['geometry'].contains(point)) else 'No' )\nNYC_art['Buffer_10_min'] = NYC_art['geometry'].apply(lambda point: 'Yes' if any(union_10_NYC['geometry'].contains(point)) else 'No' )\nNYC_art_melt = NYC_art.melt(id_vars=['Name', 'geometry', 'Address'], value_vars=['Buffer_2_min', 'Buffer_5_min', 'Buffer_10_min'], var_name='Buffer')\n\n\n\n\nCode\nbuffers = ['Buffer_2_min', 'Buffer_5_min', 'Buffer_10_min']\nbufferSelect = pn.widgets.Select(value='Buffer_2_min', options=buffers, name=\"Buffer\")\n\n\n# plot data on a map\n\ndef plot_galleries(data, selected_buffer):\n    m = data.explore(column='value', tiles='Carto DB Positron')\n    return m\n\n# Create Dashboard\n\ndef create_dashboard_NYC(selected_buffer):\n    galleries = NYC_art_melt.loc[NYC_art_melt['Buffer'] == selected_buffer]\n    m = plot_galleries(galleries, selected_buffer)\n    return pn.pane.plot.Folium(m, height=700) \n\n\ndashboard_NYC = pn.Column(\n    pn.Column(\"Subway Walking Distrance to Galleries, NYC\", bufferSelect),\n    pn.Spacer(height=30),\n    pn.bind(create_dashboard_NYC, selected_buffer=bufferSelect)\n)\n\n\n\n\n\nCode\ndashboard_NYC"
  },
  {
    "objectID": "analysis/Final-2.html#london",
    "href": "analysis/Final-2.html#london",
    "title": "Part II - Street Networks and Accessibility",
    "section": "1.2 London",
    "text": "1.2 London\n\n\nCode\nLondon_metro = gpd.read_file(\"./Final_Data/2/london-underground.geojson\")\nLondon_art = gpd.read_file(\"./Final_Data/2/London_Art.geojson\")\n\n\n\n\nCode\n# Five minute buffer\nlondon_five_min = London_metro.to_crs(epsg=3857).buffer(400)\nlondon_five_min_buffer = gpd.GeoDataFrame(geometry=london_five_min).set_crs(epsg=3857)\nlondon_five_min_union = unary_union(london_five_min_buffer['geometry'])\nunion_5_London = gpd.GeoDataFrame(geometry=[london_five_min_union]).set_crs(epsg=3857).to_crs(epsg=4326)\n\n# Ten minute buffer\nlondon_ten_min = London_metro.to_crs(epsg=3857).buffer(800)\nlondon_ten_min_buffer = gpd.GeoDataFrame(geometry=london_ten_min).set_crs(epsg=3857)\n\nlondon_ten_min_union = unary_union(london_ten_min_buffer['geometry'])\nunion_10_London = gpd.GeoDataFrame(geometry=[london_ten_min_union]).set_crs(epsg=3857).to_crs(epsg=4326)\n\n# Fifteen minute buffer\nlondon_fifteen_min = London_metro.to_crs(epsg=3857).buffer(1200)\nlondon_fifteen_min_buffer = gpd.GeoDataFrame(geometry=london_fifteen_min).set_crs(epsg=3857)\n\nlondon_fifteen_min_union = unary_union(london_fifteen_min_buffer['geometry'])\nunion_15_London = gpd.GeoDataFrame(geometry=[london_fifteen_min_union]).set_crs(epsg=3857).to_crs(epsg=4326)\n\n\n\n\nCode\nLondon_art['Buffer_5_min'] = London_art['geometry'].apply(lambda point: 'Yes' if any(union_5_London['geometry'].contains(point)) else 'No' )\nLondon_art['Buffer_10_min'] = London_art['geometry'].apply(lambda point: 'Yes' if any(union_10_London['geometry'].contains(point)) else 'No' )\nLondon_art['Buffer_15_min'] = London_art['geometry'].apply(lambda point: 'Yes' if any(union_15_London['geometry'].contains(point)) else 'No' )\nLondon_art_melt = London_art.melt(id_vars=['Name', 'geometry', 'Address'], value_vars=['Buffer_5_min', 'Buffer_10_min', 'Buffer_15_min'], var_name='Buffer')\n\n\n\n\nCode\nbuffers_london = ['Buffer_5_min', 'Buffer_10_min', 'Buffer_15_min']\nbufferSelect_london = pn.widgets.Select(value='Buffer_5_min', options=buffers_london, name=\"Buffer\")\n\n\n# plot data on a map\n\ndef plot_galleries(data, selected_buffer):\n    m = data.explore(column='value', tiles='Carto DB Positron')\n    return m\n\n# Create Dashboard\n\ndef create_dashboard_London(selected_buffer):\n    galleries = London_art_melt.loc[London_art_melt['Buffer'] == selected_buffer]\n    m = plot_galleries(galleries, selected_buffer)\n    return pn.pane.plot.Folium(m, height=700) \n\n\ndashboard_London = pn.Column(\n    pn.Column(\"Subway Walking Distrance to Galleries, London\", bufferSelect_london),\n    pn.Spacer(height=30),\n    pn.bind(create_dashboard_London, selected_buffer=bufferSelect_london)\n)\n\n\n\n\n\nCode\ndashboard_London"
  },
  {
    "objectID": "analysis/Final-3.html#department-breakdown",
    "href": "analysis/Final-3.html#department-breakdown",
    "title": "Part III - Museum Collections - MET, MoMA, British Museum",
    "section": "1.1 Department Breakdown",
    "text": "1.1 Department Breakdown\nThe first quick glance goes to this departmental breakdown. It is apparent that both museums have the largest collection in Drawings and Paintings, followed by photography. But the museums are taking different approaches when it comes to managing and breaking down – the MET is managing collection and departments by genre (a comprehensive way of distinguishing temporal, geographical, and thematic features of artworks), considering its large variety of artwork profiles; MoMA, on the other hand, takes the approach of medium, like many other modern art museums do.\n\n\nCode\nmet_dept = pd.DataFrame(met.groupby(['Department']).size()).reset_index()\nmet_dept = met_dept.rename(columns={met_dept.columns[1]: 'Counts'})\nmoma_dept = pd.DataFrame(moma_artwork.groupby(['Department']).size()).reset_index()\nmoma_dept = moma_dept.rename(columns={moma_dept.columns[1]: 'Counts'})\n\n\n\n\nCode\nfig = go.Figure()\n\n\nfig.add_trace(\n    go.Bar(x=met_dept['Department'], \n           y=met_dept['Counts'], \n           name='Met Dept',\n          marker=dict(color=\"#E81D2E\")))\n\n\nfig.add_trace(\n    go.Bar(x=moma_dept['Department'], \n           y=moma_dept['Counts'], \n           name='MoMA Dept',\n          marker=dict(color=\"Black\")))\n\n#add dropdown\n\nfig.update_layout(\n    updatemenus=[\n        dict(\n            active=0,\n            buttons=list([\n                dict(label=\"Met\",\n                    method=\"update\",\n                    args=[{\"visible\": [True, False]},\n                        {\"title\": \"Metropolitan Museum of Art Department Breakdown\",\n                         \"annotations\": []}]),\n                dict(label=\"MoMA\",\n                    method=\"update\",\n                    args=[{\"visible\": [False, True]},\n                        {\"title\": \"Modern Museum of Art Department Breakdown\",\n                         \"annotations\": []}])\n            ]))])\n\nfig.update_layout(title_text=\"Number of Objects Held by Departments at Museums\", \n                  height=700,\n                 template='plotly_white')\nfig.show()"
  },
  {
    "objectID": "analysis/Final-3.html#top-artists",
    "href": "analysis/Final-3.html#top-artists",
    "title": "Part III - Museum Collections - MET, MoMA, British Museum",
    "section": "1.2 Top Artists",
    "text": "1.2 Top Artists\nAfter departments, I am curious about who are the top 30 artists, who has the most artworks owned by the two museums. So, I have counted their artwork, ranked, and grouped their work by the departmental categories that was explored in section 1.1. The results are shown in the bar charts below. As expected, most top artists are predominantly producing photography work or paintings, which consist of the largest collections at both Museums.\n\n\nCode\nmet_top_artist = pd.DataFrame(met.groupby(['Artist Display Name']).size()).reset_index()\nmet_top_artist = met_top_artist.rename(columns={met_top_artist.columns[1]: 'Counts'})\nmet_top_artist_with_Co = met_top_artist[(met_top_artist['Artist Display Name'] != 'Unknown') & \n                                ~met_top_artist['Artist Display Name'].str.contains('Anonymous', case=False) &\n                                (met_top_artist['Artist Display Name'] != 'Unidentified artist')]\n\nmet_top_artist = met_top_artist[(met_top_artist['Artist Display Name'] != 'Unknown') & \n                                ~met_top_artist['Artist Display Name'].str.contains('Anonymous', case=False) &\n                                ~met_top_artist['Artist Display Name'].str.contains('company', case=False) &\n                                ~met_top_artist['Artist Display Name'].str.contains('Co.', case=False) &\n                                (met_top_artist['Artist Display Name'] != 'Unidentified artist')]\n \nmoma_top_artist = pd.DataFrame(moma_artwork.groupby(['Artist']).size()).reset_index()\nmoma_top_artist = moma_top_artist.rename(columns={moma_top_artist.columns[1]: 'Counts'})\nmoma_top_artist = moma_top_artist[(moma_top_artist['Artist'] != 'Unknown') & \n                                  (moma_top_artist['Artist'] != 'Anonymous') &\n                                  ~moma_top_artist['Artist'].str.contains('Unidentified', case=False)]\n\n\n\n\nCode\nmet_top_30 = met_top_artist.loc[met_top_artist['Counts'].nlargest(30).index]\nmoma_top_30 = moma_top_artist.loc[moma_top_artist['Counts'].nlargest(30).index]\nmet_30_artworks = met[met['Artist Display Name'].isin(met_top_30['Artist Display Name'])] \nmoma_30_artworks = moma_artwork[moma_artwork['Artist'].isin(moma_top_30['Artist'])] \nmoma_30_work_breakdown = moma_30_artworks.groupby(['Artist', 'Department']).size().reset_index()\nmoma_30_work_breakdown = moma_30_work_breakdown.rename(columns={moma_30_work_breakdown.columns[2]: 'Counts'})\n\nmet_30_work_breakdown = met_30_artworks.groupby(['Artist Display Name', 'Department']).size().reset_index()\nmet_30_work_breakdown = met_30_work_breakdown.rename(columns={met_30_work_breakdown.columns[2]: 'Counts'})\n\nmoma_30_work_pivot = moma_30_work_breakdown.pivot(index='Artist', columns='Department', values=\"Counts\").fillna(0)\n\nmet_30_work_pivot = met_30_work_breakdown.pivot(index='Artist Display Name', columns='Department', values=\"Counts\").fillna(0)\n\n\n\n\nCode\norder = moma_top_30['Artist'].values\nmoma_30_work_pivot.index = pd.CategoricalIndex(moma_30_work_pivot.index, categories=order, ordered=True)\nmoma_30_work_pivot = moma_30_work_pivot.sort_index()\n\norder = met_top_30['Artist Display Name'].values\nmet_30_work_pivot.index = pd.CategoricalIndex(met_30_work_pivot.index, categories=order, ordered=True)\nmet_30_work_pivot = met_30_work_pivot.sort_index()\n\n\n\n\nCode\nfig = go.Figure()\n\ncolors = ['#3A6C8C', '#0F3D3F','#B3D8EB','#728F4C','#CEE0C6','#242545','#EF819C','#F4B8D4']\n\nheadings = ['Architecture & Design', 'Architecture & Design - Image Archive', 'Drawings & Prints', 'Film', 'Fluxus Collection', 'Media and Performance', 'Painting & Sculpture', 'Photography']\n\n\n\nimport plotly.graph_objects as go\n\n\nx_data = np.transpose(moma_30_work_pivot.values)\ny_data = moma_30_work_pivot.index.values\n\n\nfor heading, xd, colors in zip(headings, x_data, colors):\n    fig.add_trace(go.Bar(\n            x=xd, \n            y=y_data,\n            name=heading,\n            orientation='h',\n            marker=dict(\n                color=colors,\n                line=dict(color='rgb(248, 248, 249)', width=1)\n            )\n        ))\n\nfig.update_layout(\n    height=800,\n    width=1500,\n    yaxis=dict(autorange=\"reversed\"),\n    barmode='stack',\n    margin=dict(l=120, r=10, t=140, b=80),\n    showlegend=True,\n    template='plotly_white',\n    autosize=True,\n    title='Top 30 Artists whom MoMA Holds Most Works of'\n)\n\n\n\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nfig = go.Figure()\n\ncolors = ['#3A6C8C','#dc596d','#B3D8EB','#949EC3', '#8B7099', '#242545','#EF819C','#F4B8D4','#728F4C','#CEE0C6', '#ffbb93', '#fa958f' ]\n\nheadings = met_30_work_pivot.columns.to_numpy()\n\n\n\nimport plotly.graph_objects as go\n\n\nx_data = np.transpose(met_30_work_pivot.values)\ny_data = met_30_work_pivot.index.values\n\n\nfor heading, xd, colors in zip(headings, x_data, colors):\n    fig.add_trace(go.Bar(\n            x=xd, \n            y=y_data,\n            name=heading,\n            orientation='h',\n            marker=dict(\n                color=colors,\n                line=dict(color='rgb(248, 248, 249)', width=1)\n            )\n        ))\n\nfig.update_layout(\n    height=800,\n    width=1500,\n    yaxis=dict(autorange=\"reversed\"),\n    barmode='stack',\n    margin=dict(l=120, r=10, t=140, b=80),\n    showlegend=True,\n    template='plotly_white',\n    autosize=True,\n    title='Top 30 Artists whom the Met Holds Most Works of'\n)\n\n\n\nfig.show()"
  },
  {
    "objectID": "analysis/Final-3.html#top-nationalities-of-artists",
    "href": "analysis/Final-3.html#top-nationalities-of-artists",
    "title": "Part III - Museum Collections - MET, MoMA, British Museum",
    "section": "1.3 Top Nationalities of Artists",
    "text": "1.3 Top Nationalities of Artists\nAlong the same line, I analyzed the top nationalities of artists whose work are hosted at the MET. The results show that the MET and MoMA both has the most artwork from American artists, though MoMA’s American artists’ collection is significantly larger than artworks of other nationalities. Additionally, we are seeing British, French, Japanese, Italian, German, and Dutch being leading group of artists. One disclaimer is that the analysis is conducted using different datasets – the MET one is generated from counting number of occurrence of one nationality from the list, while MoMA has a separate list of artists, which is used for this analysis. So, the MET analysis may double count artists if they have multiple work hosted, but MoMA’s count only for unique values. Despite the potential inaccuracy in the absolute value, this is useful to generate insights on leading nationalities of artists within either museum respectively.\n\n\nCode\nfiltered = met.dropna(subset=['Artist Nationality'])\nword_counts = filtered['Artist Nationality'].str.split('[\\s,|]', expand=True).stack().value_counts()\nword_counts = pd.DataFrame(word_counts)\n\n\n\n\nCode\nnationality = word_counts.iloc[0:12].reset_index()\nnationality = nationality.drop(index=[1,6])\nnationality = nationality.rename(columns={nationality.columns[0]: 'Nationalities',nationality.columns[1]: 'Counts' })\n\n\n\n\nCode\nfig = px.bar(x=nationality['Counts'].values, y=nationality['Nationalities'].values,\n            orientation='h')\n\nfig.update_layout(\n    height=800,\n    width=1500,\n    yaxis=dict(autorange=\"reversed\"),\n    barmode='stack',\n    margin=dict(l=120, r=10, t=140, b=80),\n    showlegend=True,\n    template='plotly_white',\n    autosize=True,\n    title='Top 10 Nationalities of Artists at the MET'\n)\n\n\n\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nmoma_nationality = pd.DataFrame(moma_artist.groupby(['Nationality']).size().reset_index())\nmoma_nationality = moma_nationality.rename(columns={moma_nationality.columns[1]: 'Counts' })\nmoma_nationality = moma_nationality.loc[moma_nationality['Counts'].nlargest(10).index]\n\n\n\n\nCode\nfig = px.bar(x=moma_nationality['Counts'].values, y=moma_nationality['Nationality'].values,\n            orientation='h')\n\nfig.update_layout(\n    height=800,\n    width=1500,\n    yaxis=dict(autorange=\"reversed\"),\n    barmode='stack',\n    margin=dict(l=120, r=10, t=140, b=80),\n    showlegend=True,\n    template='plotly_white',\n    autosize=True,\n    title='Top 10 Nationalities of Artists at MoMA'\n)\n\n\n\nfig.show()"
  },
  {
    "objectID": "analysis/Final-3.html#met---open-access-csv-json-file",
    "href": "analysis/Final-3.html#met---open-access-csv-json-file",
    "title": "Part III - Museum Collections - MET, MoMA, British Museum",
    "section": "2.1 MET - Open Access CSV / JSON file",
    "text": "2.1 MET - Open Access CSV / JSON file\nUtilizing the MET’s open data, I analyzed the top 30 tags / recurring themes of Chinese artwork. This is the kind of data analytics made possible by large scale digitization of in-house collection. It’s quite interesting to see how themes emerge as we put them into the database. The data can also be used for other types of analysis, shown in the first part already.\n\n\nCode\nmet_china_art = met.loc[(met['Culture'] == \"China\")]\n\n\n\n\nCode\nmet_china_art.head()\n\n\n\n\n\n\n\n\n\nObject Number\nIs Highlight\nIs Timeline Work\nIs Public Domain\nObject ID\nGallery Number\nDepartment\nAccessionYear\nObject Name\nTitle\nCulture\nPeriod\nDynasty\nReign\nPortfolio\nConstituent ID\nArtist Role\nArtist Prefix\nArtist Display Name\nArtist Display Bio\nArtist Suffix\nArtist Alpha Sort\nArtist Nationality\nArtist Begin Date\nArtist End Date\nArtist Gender\nArtist ULAN URL\nArtist Wikidata URL\nObject Date\nObject Begin Date\nObject End Date\nMedium\nDimensions\nCredit Line\nGeography Type\nCity\nState\nCounty\nCountry\nRegion\nSubregion\nLocale\nLocus\nExcavation\nRiver\nClassification\nRights and Reproduction\nLink Resource\nObject Wikidata URL\nMetadata Date\nRepository\nTags\nTags AAT URL\nTags Wikidata URL\n\n\n\n\n6933\n13.31.15\nFalse\nFalse\nTrue\n7411\n774\nThe American Wing\n1913.0\nShaving mug\nShaving Mug\nChina\nNaN\nNaN\nNaN\nNaN\n188\nMaker\n\nE. & W. Bennett Pottery\nAmerican, Baltimore, Maryland 1847–1857\n\nBennett, E. & W., Pottery\nAmerican\n1847\n1857\nNaN\nhttp://vocab.getty.edu/page/ulan/500524602\nhttps://www.wikidata.org/wiki/Q98446707\nca. 1853\n1850\n1853\nMottled brown earthenware\nH. 4 3/8 in. (11.1 cm)\nRogers Fund, 1913\nMade in\nBaltimore\nNaN\nNaN\nUnited States\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nhttp://www.metmuseum.org/art/collection/search...\nhttps://www.wikidata.org/wiki/Q116342297\nNaN\nMetropolitan Museum of Art, New York, NY\nMen\nhttp://vocab.getty.edu/page/aat/300025928\nhttps://www.wikidata.org/wiki/Q8441\n\n\n6979\n33.120.164\nFalse\nFalse\nTrue\n7457\n774\nThe American Wing\n1933.0\nBuckle\nShoe Buckle\nChina\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nca. 1800\n1797\n1800\nSilver\n2 3/8 x 1 3/4 in. (6 x 4.4 cm)\nBequest of Alphonso T. Clearwater, 1933\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nhttp://www.metmuseum.org/art/collection/search...\nhttps://www.wikidata.org/wiki/Q116341420\nNaN\nMetropolitan Museum of Art, New York, NY\nNaN\nNaN\nNaN\n\n\n30296\n96.14.1896\nFalse\nFalse\nTrue\n35967\nNaN\nAsian Art\n1896.0\nPanel\nNaN\nChina\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n18th century or earlier\n1650\n1799\nPaint; on leather\n9 1/4 x 5 3/8 in. (23.5 x 13.7 cm)\nGift of Mr. and Mrs. H. O. Havemeyer, 1896\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nLeatherwork\nNaN\nhttp://www.metmuseum.org/art/collection/search...\nNaN\nNaN\nMetropolitan Museum of Art, New York, NY\nMusical Instruments|Men|Elephants|Flowers\nhttp://vocab.getty.edu/page/aat/300041620|http...\nhttps://www.wikidata.org/wiki/Q34379|https://w...\n\n\n30297\n09.3\nFalse\nFalse\nTrue\n35968\nNaN\nAsian Art\n1909.0\nPictorial map\n清 佚名 台南地區荷蘭城堡|Forts Zeelandia and Provintia ...\nChina\nNaN\nNaN\nNaN\nNaN\n3750\nArtist\n\nUnidentified artist\nChinese, active 19th century\n\nUnidentified artist\n\n\n\nNaN\nNaN\nNaN\n19th century\n1800\n1899\nWall hanging; ink and color on deerskin\nImage: 59 1/4 × 80 3/4 in. (150.5 × 205.1 cm)\\...\nGift of J. Pierpont Morgan, 1909\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPaintings\nNaN\nhttp://www.metmuseum.org/art/collection/search...\nhttps://www.wikidata.org/wiki/Q79003782\nNaN\nMetropolitan Museum of Art, New York, NY\nMaps|Houses|Cities|Boats|Ships\nhttp://vocab.getty.edu/page/aat/300028094|http...\nhttps://www.wikidata.org/wiki/Q4006|https://ww...\n\n\n30298\n12.37.135\nFalse\nFalse\nFalse\n35969\nNaN\nAsian Art\n1912.0\nHanging scroll\nNaN\nChina\nQing dynasty (1644–1911)\nNaN\nNaN\nNaN\n1214\nArtist\n\nJin Zunnian\nChinese, active early 18th century\n\nJin Zunnian\nChinese\n1700\n1800\nNaN\nNaN\nNaN\ndated 1732\n1732\n1732\nHanging scroll; ink and color on silk\n67 x 38 in. (170.2 x 96.5 cm)\nRogers Fund, 1912\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPaintings\nNaN\nhttp://www.metmuseum.org/art/collection/search...\nNaN\nNaN\nMetropolitan Museum of Art, New York, NY\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nCode\nmet_china_art = met_china_art.dropna(subset=['Tags'])\n\n\n\n\nCode\ntags_counts = met_china_art['Tags'].str.split('[\\s,|]', expand=True).stack().value_counts()\ntags_counts = pd.DataFrame(tags_counts).reset_index()\ntags_counts = tags_counts.rename(columns={tags_counts.columns[1]: 'Counts' })\n\ntags_counts_met = tags_counts.loc[tags_counts['Counts'].nlargest(30).index]\n\n\n\n\nCode\nfig = px.bar(y=tags_counts_met['index'], x=tags_counts_met['Counts'])\nfig.update_layout(\n    height=800,\n    width=1500,\n    yaxis=dict(autorange=\"reversed\"),\n    barmode='stack',\n    margin=dict(l=120, r=10, t=140, b=80),\n    showlegend=True,\n    template='plotly_white',\n    autosize=True,\n    title='Top 30 Tags / Themes of Chinese artwork at the MET'\n)\n\n\nfig.show()"
  },
  {
    "objectID": "analysis/Final-3.html#british-museum---access-search-result-download",
    "href": "analysis/Final-3.html#british-museum---access-search-result-download",
    "title": "Part III - Museum Collections - MET, MoMA, British Museum",
    "section": "2.2 British Museum - Access Search Result Download",
    "text": "2.2 British Museum - Access Search Result Download\nA similar level of deep analysis into the content and themes of artwork can be conducted on the dataset downloaded from The British Museum. While the British Museum allows to download all results (cap at 20,000 items), some other museums like VAM (Victoria and Albert Museum only allows to download one page at a time (15 or 50 items), which is not efficient for large-scale analysis. However, they claim that they have an API to be utilized, that is not explore as a part of this project.\nIn addition to recurring themes, I also tried to do a quick glance of most used materials for Chinese artwork. Quick glance of such data might interest profane visitors who doesn’t have much background in Chinese history or art history in general.\n\n\nCode\nBM_Result = pd.read_csv(\"./Final_Data/3/British_Museum_Result.csv\")\n\n\n\n\nCode\nBM_Materials = BM_Result.dropna(subset=['Materials'])\nBM_Subjects = BM_Result.dropna(subset=['Subjects'])\n\n\n\n\nCode\nBM_Subjects_counts = BM_Subjects['Subjects'].str.split('[\\s,|;]', expand=True).stack().value_counts()\nBM_Subjects_counts = pd.DataFrame(BM_Subjects_counts).reset_index()\nBM_Subjects_counts = BM_Subjects_counts.rename(columns={BM_Subjects_counts.columns[0]: 'Subjects', BM_Subjects_counts.columns[1]: 'Counts' })\n\nBM_Subjects_counts = BM_Subjects_counts.loc[BM_Subjects_counts['Counts'].nlargest(34).index]\nBM_Subjects_counts = BM_Subjects_counts.drop(index=[0,5,16,17])\n\n\n\n\nCode\n\nfig = px.bar(y=BM_Subjects_counts['Subjects'], x=BM_Subjects_counts['Counts'])\nfig.update_layout(\n    height=800,\n    width=1500,\n    yaxis=dict(autorange=\"reversed\"),\n    barmode='stack',\n    margin=dict(l=120, r=10, t=140, b=80),\n    showlegend=True,\n    template='plotly_white',\n    autosize=True,\n    title='Top 30 Tags / Themes of Chinese artwork at the British Museum'\n)\n\n\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nBM_Materials_counts = BM_Materials['Materials'].str.split('[\\s,|;]', expand=True).stack().value_counts()\nBM_Materials_counts = pd.DataFrame(BM_Materials_counts).reset_index()\nBM_Materials_counts = BM_Materials_counts.rename(columns={BM_Materials_counts.columns[0]: 'Materials', BM_Materials_counts.columns[1]: 'Counts' })\n\nBM_Materials_counts = BM_Materials_counts.loc[BM_Materials_counts['Counts'].nlargest(31).index]\nBM_Materials_counts = BM_Materials_counts.drop(index=[1])\n\n\n\n\nCode\n\nfig = px.bar(y=BM_Materials_counts['Materials'], x=BM_Materials_counts['Counts'])\nfig.update_layout(\n    height=800,\n    width=1500,\n    yaxis=dict(autorange=\"reversed\"),\n    barmode='stack',\n    margin=dict(l=120, r=10, t=140, b=80),\n    showlegend=True,\n    template='plotly_white',\n    autosize=True,\n    title='Top 30 Materials of Chinese artwork on display at the British Museum'\n)\n\n\nfig.show()"
  },
  {
    "objectID": "analysis/Final-3.html#web-scrapping---philadelphia-museum-of-art-as-an-example",
    "href": "analysis/Final-3.html#web-scrapping---philadelphia-museum-of-art-as-an-example",
    "title": "Part III - Museum Collections - MET, MoMA, British Museum",
    "section": "2.3 Web Scrapping - Philadelphia Museum of Art as an example",
    "text": "2.3 Web Scrapping - Philadelphia Museum of Art as an example\nThe last approach is to scrap from the online galleries. The example here is Philadelphia Museum of Art’s Chinese art collection. In comparison to the earlier two, the information scrapped from the web is a lot less in detail. Particularly for art genres like Chinese art, of which many have unknown artist name or time or production, the information scrapped would be not useful, as we are not able to efficiently scrape details of many objects. Hence, the potential analyses are limited.\n\n\nCode\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nimport requests\nfrom time import sleep\n\n\n\n\nCode\ndriver = webdriver.Chrome()\nurl = \"https://philamuseum.org/search/collections?from=0&size=48&filters=%7B%22department%22%3A%5B%22East%20Asian%20Art%22%5D%2C%22place%22%3A%5B%22China%22%5D%7D\"\nresponse = driver.get(url)\nhtml_content = driver.page_source\n\n\nNoSuchWindowException: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=120.0.6099.71)\nStacktrace:\n0   chromedriver                        0x000000010a906c48 chromedriver + 4852808\n1   chromedriver                        0x000000010a8fe1b3 chromedriver + 4817331\n2   chromedriver                        0x000000010a4ca7bd chromedriver + 411581\n3   chromedriver                        0x000000010a49e2f8 chromedriver + 230136\n4   chromedriver                        0x000000010a54c41f chromedriver + 943135\n5   chromedriver                        0x000000010a563226 chromedriver + 1036838\n6   chromedriver                        0x000000010a5449a3 chromedriver + 911779\n7   chromedriver                        0x000000010a50c103 chromedriver + 680195\n8   chromedriver                        0x000000010a50d71e chromedriver + 685854\n9   chromedriver                        0x000000010a8c6792 chromedriver + 4589458\n10  chromedriver                        0x000000010a8cb99c chromedriver + 4610460\n11  chromedriver                        0x000000010a8abcb1 chromedriver + 4480177\n12  chromedriver                        0x000000010a8cc716 chromedriver + 4613910\n13  chromedriver                        0x000000010a89d23c chromedriver + 4420156\n14  chromedriver                        0x000000010a8ec798 chromedriver + 4745112\n15  chromedriver                        0x000000010a8ec94e chromedriver + 4745550\n16  chromedriver                        0x000000010a8fddf3 chromedriver + 4816371\n17  libsystem_pthread.dylib             0x00007ff802926259 _pthread_start + 125\n18  libsystem_pthread.dylib             0x00007ff802921c7b thread_start + 15\n\n\n\n\nCode\nsoup = BeautifulSoup(html_content, 'html.parser')\n\n\n\n\nCode\nselector = \".searchcard\"\n\ntables = soup.select(selector)\n\n\n\n\nCode\nresults = []\n\n\nmax_pages = 10\n\n# The base URL we will be using\nbase_url = \"https://philamuseum.org/search/collections?\"\n\n# loop over each page of search results\nfor page_num in range(1, max_pages + 1):\n    print(f\"Processing page {page_num}...\")\n\n    obj_num = (page_num-1)*48\n    \n    # Update the URL hash for this page number and make the combined URL\n    url_hash = f\"from={obj_num}&size=48&filters=%7B%22department%22%3A%5B%22East%20Asian%20Art%22%5D%2C%22place%22%3A%5B%22China%22%5D%7D\"\n    url = base_url + url_hash\n\n    # Go to the driver and wait for 5 seconds\n    driver.get(url)\n    sleep(5)\n\n    # YOUR CODE: get the list of all apartments\n    # This is the same code from Part 1.2 and 1.3\n    # It should be a list of 120 apartments\n    soup = soup\n    objects = tables\n    print(\"Number of Objects = \", len(objects))\n\n    # loop over each apartment in the list\n    page_results = []\n    for artwork in objects:\n\n        #artwork name\n        artwork_name = artwork.select_one(\".card-title\").text\n\n        #artist, Geoegraphy, Time \n        artist_geo_time = artwork.select_one(\".card-body\").text\n              \n        # Save the result\n        page_results.append([artwork_name, artist_geo_time])\n\n    # Create a dataframe and save\n    col_names = [\"artwork_name\", \"artist_geo_time\"]\n    df = pd.DataFrame(page_results, columns=col_names)\n    results.append(df)\n\n    print(\"sleeping for 10 seconds between calls\")\n    sleep(10)\n\n# Finally, concatenate all the results\nresults = pd.concat(results, axis=0).reset_index(drop=True)\n\n\nProcessing page 1...\nNumber of Objects =  48\nsleeping for 10 seconds between calls\nProcessing page 2...\nNumber of Objects =  48\nsleeping for 10 seconds between calls\nProcessing page 3...\nNumber of Objects =  48\nsleeping for 10 seconds between calls\nProcessing page 4...\nNumber of Objects =  48\nsleeping for 10 seconds between calls\nProcessing page 5...\nNumber of Objects =  48\nsleeping for 10 seconds between calls\nProcessing page 6...\nNumber of Objects =  48\nsleeping for 10 seconds between calls\nProcessing page 7...\nNumber of Objects =  48\nsleeping for 10 seconds between calls\nProcessing page 8...\nNumber of Objects =  48\nsleeping for 10 seconds between calls\nProcessing page 9...\nNumber of Objects =  48\nsleeping for 10 seconds between calls\nProcessing page 10...\nNumber of Objects =  48\nsleeping for 10 seconds between calls\n\n\n\n\nCode\nresults[['Artist', 'Geography', 'Time']] = pd.DataFrame(results['artist_geo_time'].str.split(',').tolist(), index=results.index)\n  \n\n\n\n\nCode\nresults.head(10)\n\n\n\n\n\n\n\n\n\nartwork_name\nartist_geo_time\nArtist\nGeography\nTime\n\n\n\n\n0\nReception Hall\nArtist/maker unknown, Chinese\nArtist/maker unknown\nChinese\nNone\n\n\n1\nPortrait of a Manchu Lady\nMangguli, Chinese (Manchu), 1672 - 1736\nMangguli\nChinese (Manchu)\n1672 - 1736\n\n\n2\nJar\nArtist/maker unknown, Chinese\nArtist/maker unknown\nChinese\nNone\n\n\n3\nCovered Cup\nArtist/maker unknown, Chinese\nArtist/maker unknown\nChinese\nNone\n\n\n4\nCup\nArtist/maker unknown, Chinese\nArtist/maker unknown\nChinese\nNone\n\n\n5\nTeapot\nArtist/maker unknown, Chinese\nArtist/maker unknown\nChinese\nNone\n\n\n6\nBowl\nArtist/maker unknown, Chinese\nArtist/maker unknown\nChinese\nNone\n\n\n7\nVase in the form of an Archaic Bronze Vessel\nArtist/maker unknown, Chinese\nArtist/maker unknown\nChinese\nNone\n\n\n8\nWall Vase (P'ing)\nArtist/maker unknown, Chinese\nArtist/maker unknown\nChinese\nNone\n\n\n9\nVase (P'ing)\nArtist/maker unknown, Chinese\nArtist/maker unknown\nChinese\nNone"
  }
]